{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 데이터 변환\n",
    "\n",
    "* Last updated 20191128THR1101 20181009_20170421_20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark ETL을 할 수 있다.\n",
    "* Spark를 사용하여 구조적 데이터를 분석할 수 있다.\n",
    "* Spark를 사용하여 텍스트 분석을 할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.3 데이터 타잎과 변환\n",
    "* S.3.1 vectors\n",
    "* S.3.2 sparse vectors\n",
    "* S.3.3 labeled point\n",
    "* S.3.4 libsvm format\n",
    "* S.4 통계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* S.5 DataFrame 변환 \n",
    "* S.5.1 텍스트 변환\n",
    "* S.5.2 Python을 사용한 단어 빈도 계산\n",
    "* S.5.3 Spark\n",
    "\n",
    "* S.5.4 StringIndexer\n",
    "* S.5.5 Tokenizer\n",
    "* S.5.6 RegTokenizer\n",
    "* S.5.7 Stopwords\n",
    "* S.5.8 CountVectorizer\n",
    "* S.5.9 TF-IDF\n",
    "* S.5.10 Word2Vec\n",
    "* S.5.11 NGram\n",
    "* S.5.12 연속데이터의 변환\n",
    "* 5.5.13 VectorAssembler\n",
    "* S.5.14 Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.3 문제 \n",
    "\n",
    "* 문제 S-1: 훈련데이터 만들기\n",
    "* 문제 S-2: Kolmogorov-Smirnov 검증\n",
    "* 문제 S-3: 평균, 표준편차와 같은 기본 통계 값을 구한다.\n",
    "* 문제 S-4: 연설문을 기계학습하기 위해 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 IPython Notebook에서 SparkSession 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys \n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "print spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 데이터 타잎과 변환\n",
    "\n",
    "Spark는 예측, 분류, 군집화, 추천 등 여러 기계학습 모델을 제공하고, 이를 위해 데이터를 추출, 변환하는 이른바 **ETL** (Extract, Transform, Load)이 필수적이다. 기계학습에는 무작정 데이터를 우겨 넣어서는 작동되지 않는다. 입력될 데이터를 일정한 형식을 가지도록 구성하는 것이 필요하다. 여기에 필요한 데이터 타잎 **```Vector```**, **```Labeled Point```**, **```Matrix```**를 설명한다. 특히 RDD는 label과 features를 가지고 있는 Labeled Point로 구성해야 한다.\n",
    "\n",
    "구분 | 설명\n",
    "----------|----------\n",
    "```Vector``` | ```numpy vector```와 같은 기능을 한다. **dense**와 **sparse** vector로 구분한다.\n",
    "```Labeled Point``` | 분류를 의미하는 클래스 또는 **label**과 속성 **features** 이 묶인 구조로서, supervised learning에 사용된다.\n",
    "```Matrix``` | ```numpy matrix```와 같은 특징을 가진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이러한 데이터 타잎은 Spark에서는 **```RDD mllib```** , **```DataFrame ml```** 패키지 별로 제공되므로, 식별하여 사용한다.\n",
    "```ml``` 패키지를 사용할 경우에는 자신의 ```pyspark.ml.linalg.Vector``` 등을 사용해야 한다. ```mllib```도 마찬가지이다.\n",
    "\n",
    "패키지 | 설명 | 데이터 타잎 예\n",
    "-------|-------|-----\n",
    "```mllib``` | RDD API | ```pyspark.mllib.linalg.Vector```, ```pyspark.mllib.linalg.Matrix```\n",
    "```ml``` | DataFrame API | ```pyspark.ml.linalg.Vector```, ```pyspark.ml.linalg.Matrix```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.1 vectors\n",
    "\n",
    "행렬 **Vector**는 **dense**와 **sparse**로 구분할 수 있다.\n",
    "numpy array를 사용해도 dense vector를 만들 수 있다. Spark 내부적으로 **numpy.array**를 사용하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dv = np.array([1.0, 2.1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark에서는 **```RDD mllib```** , **```DataFrame ml```**의 **Vectors**를 사용하여 dense vectors를 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0] <class 'pyspark.mllib.linalg.DenseVector'>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "dv1mllib=Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1mllib, type(dv1mllib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,2.1,3.0]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "dv1ml=Vectors.dense([1.0, 2.1, 3])\n",
    "print dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "dense vectors는 numpy array와 같은 특징을 가진다.\n",
    "인덱스로 값을 읽을 수 있다. 또한 반복문에서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 2.1 3.0\n"
     ]
    }
   ],
   "source": [
    "for e in dv1ml:\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "보통 벡터와 같이 **product**, **dot**, **norm**과 같은 벡터 연산을 할 수도 있다.\n",
    "결과 값은 numpy와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.41\n"
     ]
    }
   ],
   "source": [
    "print dv1ml.dot(dv1ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.41"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(dv,dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "더하기, 빼기, 곱하기, 나누기 연산은 항목별로 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0,4.41,9.0]\n"
     ]
    }
   ],
   "source": [
    "print dv1ml*dv1ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.3.2 Sparse vectors\n",
    "\n",
    "행렬에는 0 값이 많이 존재하기 때문에, 0값이 아닌 **NZ Nonzero**를 제거하여 저장하면 훨씬 효율적이다.\n",
    "**sparse**는 실제 **값이 없는 요소, '0'을 제거**하여 만든 vector이다.\n",
    "Spark에서 type field (1 바이트 길이)를 통해 식별한다 (0: sparse, 1: dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 1차원 Sparse Vectors\n",
    "\n",
    "예를 들어, 다음은 1차원 dense vector이다.\n",
    "```python\n",
    "[160,69,24]\n",
    "```\n",
    "\n",
    "sparse vector로 표현하면, 값이 없는 요소 Nonzero가 없으니 더 복잡해 보인다.\n",
    "3은 컬럼 갯수, 0,1,2는 값이 있는 컬럼, [160.0,69.0,24.0]는 실제 값을 의미한다.\n",
    "```python\n",
    "(3,[0,1,2],[160.0,69.0,24.0])\n",
    "```\n",
    "\n",
    "dense vector | sparse vector\n",
    "----------|----------\n",
    "**모든** 행열 값을 가지고 있다. | **인덱스 및 값**의 배열을 별도로 가진다.\n",
    "빈 값이 별로 없는 경우. | 빈 값 Nonzero가 많은 경우 사용. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparse vectors는 값 중에 0이 포함된 경우 이를 생략한다.\n",
    "```toArray()``` 함수를 사용하면 sparse에서 dense로 벡터를 변환할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  3.]\n"
     ]
    }
   ],
   "source": [
    "sv1 = Vectors.sparse(3, [1, 2], [1.0, 3.0])\n",
    "print sv1.toArray()  #convert sparse to array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sparse Vectors의 배열 방식 표현\n",
    "\n",
    "```python\n",
    "[1 0 2]\n",
    "[0 0 3]\n",
    "[4 5 6]\n",
    "\n",
    "행 | 0 | 0 | 1 | 2 | 2 | 2\n",
    "열 | 0 | 2 | 2 | 0 | 1 | 2\n",
    "값 | 1 | 2 | 3 | 4 | 5 | 6\n",
    "```\n",
    "\n",
    "행을 보면 0번째에 '1','2' 1번째에 '3', 2번째에 '4','5','6'이므로 **0,0,1,2,2,2**\n",
    "열을 보면 0번째에 '1', 2번째 '2','3', 0번째 '4', 1번째 '5', 2번째 '6'이므로 **0,2,2,0,1,2**\n",
    "\n",
    "**행, 열, 데이터를 한 쌍**으로 읽으면 된다.\n",
    "즉 행 0, 열 0의 위치에 1, 행 0, 열 2의 위치에 2. 이런 식으로 6개의 데이터가 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 2]\n",
      " [0 0 3]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "row = np.array([0, 0, 1, 2, 2, 2])\n",
    "col = np.array([0, 2, 2, 0, 1, 2])\n",
    "data = np.array([1, 2, 3, 4, 5, 6])\n",
    "mtx = sps.csc_matrix((data, (row, col)), shape=(3, 3))\n",
    "print mtx.todense()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Sparse Vectors의 CSR (Compressed Sparse Row)\n",
    "\n",
    "**```Compressed Sparse Column```** 또는 Yale Format이라고 한다. 다음 5개의 값으로 표현된다.\n",
    "* int 행 개수\n",
    "* int 열 갯수\n",
    "* int[] colPtrs IA:\n",
    "    * IA[0] = 0 처음은 0으로 놓고\n",
    "    * IA[i] = IA[i-1] + (i-1)번째 행의 NNZ (Num of Nonzeroes)\n",
    "    * 마지막은 NNZ (No of NonZeros)가 된다.\n",
    "* int[] rowIndices JA: 각 요소의 행(컬럼) 인덱스\n",
    "* double values\n",
    "\n",
    "```python\n",
    "[ 1.,  0.,  0.,  0.]\n",
    "[ 2.,  3.,  0.,  0.]\n",
    "[ 0.,  0.,  5.,  0.]\n",
    "[ 0.,  4.,  6.,  0.]\n",
    "[ 0.,  0.,  7.,  0.]\n",
    "[ 0.,  0.,  0.,  8.]\n",
    "```\n",
    "\n",
    "* 6은 행 갯수\n",
    "* 4능 열 갯수\n",
    "* [0, 2, 4, 7, 8]은 **0**:IA[0]=0으로 시작, **2**:IA[1]=IA[0]+2, **4**:IA[2]=IA[1]+2 **7**:IA[3]=IA[2]+3, **8**:IA[4]=IA[3]+1\n",
    "* 인덱스 [0, 1, 1, 3, 2, 3, 4, 5]은 컬럼순, 즉 위아래로 읽으면 1:**0**행, 2:**1**행, 3:**1**행, 4:**3**행, 5:**2**행, 6:**3**행, 7:**4**행, 8:**5**행\n",
    "* 실제 값 [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "dense Matrics는 행의 갯수, 열의 갯수, 실제 값을 넣어주면 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.],\n",
       "       [ 2.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  5.,  0.],\n",
       "       [ 0.,  4.,  6.,  0.],\n",
       "       [ 0.,  0.,  7.,  0.],\n",
       "       [ 0.,  0.,  0.,  8.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = Matrices.dense(6,4,[1, 2, 0, 0, 0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 5, 6, 7, 0, 0, 0, 0, 0, 0, 8])\n",
    "dm.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseMatrix(6, 4, [0, 2, 4, 7, 8], [0, 1, 1, 3, 2, 3, 4, 5], [1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0], False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.toSparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 4.0], [2.0, 5.0], [3.0, 6.0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "# Create a dense matrix ((1.0, 2.0), (3.0, 4.0), (5.0, 6.0))\n",
    "[ 1.,  4.], [ 2., 5.], [ 3.,  6.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 아래는 행3, 열2개 행열로 [9., 0.], [0., 8.], [0., 6.] 값이다.\n",
    "* 0,1,3: **0**:IA[0]=0으로 시작, **1**:IA[1]=IA[0]+1 (요소 9), **3**:IA[2]=IA[1]+2 (요소 6,8)\n",
    "* 0,2,1: 9:**0**행, 6:**2**행, 8:**1**행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sm = Matrices.sparse(3, 2, [0, 1, 3], [0, 2, 1], [9, 6, 8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 9.,  0.],\n",
      "             [ 0.,  8.],\n",
      "             [ 0.,  6.]])\n"
     ]
    }
   ],
   "source": [
    "d=sm.toDense()\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RowMatrix\n",
    "\n",
    "local matrix는 ```pyspark.mllib.linalg.Matrix, Matrices```를 사용한다.\n",
    "distributed matrix는 제공되는 패키지에서 보듯이 분산해서 사용할 수 있다.\n",
    "리스트를 묶어 여러 행으로 구성된 벡터를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = [[1.0,2.0,3.0],[1.1,2.1,3.1],[1.2,2.2,3.3]]\n",
    "my=spark.sparkContext.parallelize(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1.0, 2.0, 3.0], [1.1, 2.1, 3.1], [1.2, 2.2, 3.3]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RowMatrix는 ```pyspark.mllib.linalg.distributed```에서 제공되는 분산벡터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "rm=RowMatrix(my)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.mllib.linalg.distributed.RowMatrix'>\n"
     ]
    }
   ],
   "source": [
    "print type(rm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DenseVector([1.0, 2.0, 3.0]),\n",
       " DenseVector([1.1, 2.1, 3.1]),\n",
       " DenseVector([1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm.rows.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.3 Labeled Point\n",
    "\n",
    "#### label, features로 구성\n",
    "\n",
    "**분류** 및 **회귀분석**에 사용되는 데이터 타잎이다.\n",
    "**'label'**과 **'features'**로 구성된다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "label | supervised learning에서 '구분 값'으로 사용한다. 데이터타잎은 'Double'\n",
    "features | **sparse**, **dense** 모두 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "label 1.0, features [1.0, 2.0, 3.0]으로 LabeledPoint를 만들어 보자.\n",
    "\n",
    "구분 | 예제 | 설명\n",
    "-----|-----|-----\n",
    "label | 1.0 | 구분 값으로 Double 데이터 타잎\n",
    "features | [1.0, 2.0, 3.0] | **dense vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0,[1.0,2.0,3.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "print LabeledPoint(1.0, [1.0, 2.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992.0,(10,[0,1,2],[3.0,5.5,10.0]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "print LabeledPoint(1992, Vectors.sparse(10, {0: 3.0, 1:5.5, 2: 10.0}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "서로 다른 패키지의 데이터타잎 **```mllib LabeledPoint```**와 **```ml Vectors```**를 혼용하면, 형변환 오류가 발생한다.\n",
    "이러한 오류는 패키지를 혼용하지 않으면 된다.\n",
    "\n",
    "```python\n",
    "Cannot convert type <class 'pyspark.ml.linalg.DenseVector'> into Vector\n",
    "```\n",
    "\n",
    "**```dv1mllib```**은 앞서 **```mllib```**로부터 생성된 dense vector이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "LabeledPoint(1.0, dv1mllib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**```dv1ml```**은 앞서 **```ml```**로부터 생성된 dense vector이다.\n",
    "```mllib```에서 사용하려면, **```Vectors.fromML()```**를 사용해 ```ml```의 Vectors를 읽어서 ```mllib```로 변환하여 혼용을 막는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabeledPoint(1.0, [1.0,2.1,3.0])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "LabeledPoint(1.0, Vectors.fromML(dv1ml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame에서 Labeled Point\n",
    "\n",
    "* Python list에서 DataFrame 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [[1,[1.0,2.0,3.0]],[1,[1.1,2.1,3.1]],[0,[1.2,2.2,3.3]]]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Python list를 LabeledPoint로 생성하면, 'label'과 'features'의 명칭을 가지도록 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1,[1.0,2.0,3.0]),\n",
    "     LabeledPoint(1,[1.1,2.1,3.1]),\n",
    "     LabeledPoint(0,[1.2,2.2,3.3])]\n",
    "trainDf=spark.createDataFrame(p)\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mllib.linalg.Vectors를 사용하여 DataFrame을 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])\n",
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "schema를 사용해서 DataFrame을 생성해 보자.\n",
    "* 'label'은 **DoubleType**\n",
    "* 'features'는 **VectorType**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector, VectorUDT\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"label\", DoubleType(), True),\n",
    "    StructField(\"features\", VectorUDT(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf=_rdd.toDF(schema)\n",
    "trainDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sparse에서 dense vector로 변환\n",
    "\n",
    "방금 생성한 ```trainDf```는 sparse vector이다.\n",
    "사용자 함수udf User Defined Type를 사용하여 sparse vector를 dense vector로 변환해 보자.\n",
    "바로 변환할 수 있는 함수 **toDense() 함수를 지원하지 않으므로**, **sparse vector를 ```toArray()``` 함수를 사용해서 dense vector로 변환**한다.\n",
    "\n",
    "또 ```trainDf```는 RDD의 mllib에서 변환된 데이터이므로, mllib 라이브러리를 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "from pyspark.mllib.linalg import DenseVector,VectorUDT | mllib\n",
    "from pyspark.ml.linalg import DenseVector,VectorUDT | ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "다음 명령문을 요소별로 하나씩 설명한다.\n",
    "```python\n",
    "udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "```\n",
    "명령문 | 설명\n",
    "-----|-----\n",
    "```udf()``` | 사용자정의 함수\n",
    "```x.toArray()``` | sparse vector로 구성된 trainDf.features를 **```toArray()```**를 사용하여 array로 변환한다.\n",
    "```VectorUDT()``` | 타잎을 지정한다. 지정하지 않으면 ```StringType```을 기본으로, 자동으로 변환된다.\n",
    "```DenseVector()``` | array를 dense vector로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark.mllib.linalg import DenseVector, VectorUDT\n",
    "#myudf=udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "#myudf=udf(lambda x: Vectors.dense(x))\n",
    "myudf=udf(lambda x: DenseVector(x.toArray()), VectorUDT())\n",
    "_trainDf2=trainDf.withColumn('dvf',myudf(trainDf.features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- dvf: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+------------------+\n",
      "|label|            features|               dvf|\n",
      "+-----+--------------------+------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])| [0.0,1.0,0.0,5.5]|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|[-1.0,0.0,0.5,0.0]|\n",
      "+-----+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_trainDf2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: RDD 훈련데이터 만들기\n",
    "\n",
    "### 문제\n",
    "\n",
    "머신러닝은 사람이 경험을 통해 배우는 것과 비슷하게 **과거 데이터로부터 학습**을 한다.\n",
    "학습이란 어렵게 생각할 필요 없이, 과거 데이터에서 수학적이나 알고리즘을 활용하여 어떤 패턴을 찾아내는 것이다.\n",
    "spark에서 제공한 **데이터 파일 ```data/mllib/sample_svm_data.txt```을 읽어서 훈련데이터**를 만들어 보자.\n",
    "\n",
    "```python\n",
    "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "0 2.857738033247042 0 0 2.619965104088255 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "0 2.857738033247042 0 2.061393766919624 0 0 2.004684436494304 0 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
    "1 0 0 2.061393766919624 2.619965104088255 0 2.004684436494304 2.000347299268466 0 0 0 0 2.055002875864414 0 0 0 0\n",
    "1 2.857738033247042 0 2.061393766919624 2.619965104088255 0 2.004684436494304 0 0 0 0 0 2.055002875864414 0 0 0 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 해결\n",
    "\n",
    "데이터를 읽어 **RDD**를 생성하고, ```label```, ```features```를 구성하여 ```Labeled Point```로 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python으로 파일 읽기\n",
    "\n",
    "파일로부터 데이터를 읽기 위해, 파일명을 구성하고 ```try except``` 구문으로 입출력 오류를 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    _fp=os.path.join(os.environ[\"SPARK_HOME\"],\\\n",
    "        'data','mllib','sample_svm_data.txt')\n",
    "except:\n",
    "    print(\"An exception occurred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "파일로부터 데이터를 **```readlines()```** 함수로 모두 읽어 온다.\n",
    "첫 행을 읽으면 label, features로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_f=open(_fp,'r')\n",
    "_lines=_f.readlines()\n",
    "_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0 2.52078447201548 0 0 0 2.004684436494304 2.000347299268466 0 2.228387042742021 2.228387042742023 0 0 0 0 0 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print _lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Spark에서 RDD 생성\n",
    "\n",
    "원본 데이터 ```sample_svm_data.txt```는 공백으로 구분되어 있다.\n",
    "읽을 대상이 파일이므로, RDD를 사용한다. 각 행을 공백으로 분리하여 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_rdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "각 행으로 분리되므로 2차원 리스트가 생성이 된다. 첫째 행을 읽으려면 인덱스를 사용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 2.52078447201548,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 2.004684436494304,\n",
       " 2.000347299268466,\n",
       " 0.0,\n",
       " 2.228387042742021,\n",
       " 2.228387042742023,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.take(2)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LabeledPoint 생성\n",
    "\n",
    "위 데이터에서 보듯이 첫 열은 **label**로, 그 나머지는 **features**로 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "_trainRdd0=_rdd.map(lambda line:LabeledPoint(line[0], line[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd0.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "공백을 분리하고, 분리된 데이터를 labeled point로 구성하는 기능을 합쳐서 실행해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_trainRdd=spark.sparkContext.textFile(_fp)\\\n",
    "    .map(lambda line: [float(x) for x in line.split(' ')])\\\n",
    "    .map(lambda p:LabeledPoint(p[0], p[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 정리하면\n",
    "\n",
    "데이터를 변환하는 과정을 함수로 만들었다.\n",
    "```createLP(line)```는 행 데이터를 받아서 LabeledPoint로 생성하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(1.0, [0.0,2.52078447202,0.0,0.0,0.0,2.00468443649,2.00034729927,0.0,2.22838704274,2.22838704274,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def createLP(line):\n",
    "    p = [float(x) for x in line.split(' ')]\n",
    "    return LabeledPoint(p[0], p[1:])\n",
    "\n",
    "_rdd=spark.sparkContext.textFile(_fp)\n",
    "trainRdd = _rdd.map(createLP)\n",
    "\n",
    "trainRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.3.4 libsvm format\n",
    "\n",
    "* svm을 처리하기 위한 데이터 형식이다.\n",
    "* 0은 label, 나머지는 index:value 쌍으로 구성한다.\n",
    "```python\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "[label] [index1]:[value1] [index2]:[value2] ...\n",
    "```\n",
    "\n",
    "* 예\n",
    "```python\n",
    "0 128:51 129:159 130:253 131:159 132:50 155:48 156:238 157:252 158:252 159:252 160:237 182:54 183:227 184:253 185:252 186:239 187:233 ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fsvm=os.path.join(os.environ[\"SPARK_HOME\"],'data','mllib','sample_libsvm_data.txt')\n",
    "dfsvm = spark.read.format(\"libsvm\").load(fsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dfsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsvm.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(692, {127: 51.0, 128: 159.0, 129: 253.0, 130: 159.0, 131: 50.0, 154: 48.0, 155: 238.0, 156: 252.0, 157: 252.0, 158: 252.0, 159: 237.0, 181: 54.0, 182: 227.0, 183: 253.0, 184: 252.0, 185: 239.0, 186: 233.0, 187: 252.0, 188: 57.0, 189: 6.0, 207: 10.0, 208: 60.0, 209: 224.0, 210: 252.0, 211: 253.0, 212: 252.0, 213: 202.0, 214: 84.0, 215: 252.0, 216: 253.0, 217: 122.0, 235: 163.0, 236: 252.0, 237: 252.0, 238: 252.0, 239: 253.0, 240: 252.0, 241: 252.0, 242: 96.0, 243: 189.0, 244: 253.0, 245: 167.0, 262: 51.0, 263: 238.0, 264: 253.0, 265: 253.0, 266: 190.0, 267: 114.0, 268: 253.0, 269: 228.0, 270: 47.0, 271: 79.0, 272: 255.0, 273: 168.0, 289: 48.0, 290: 238.0, 291: 252.0, 292: 252.0, 293: 179.0, 294: 12.0, 295: 75.0, 296: 121.0, 297: 21.0, 300: 253.0, 301: 243.0, 302: 50.0, 316: 38.0, 317: 165.0, 318: 253.0, 319: 233.0, 320: 208.0, 321: 84.0, 328: 253.0, 329: 252.0, 330: 165.0, 343: 7.0, 344: 178.0, 345: 252.0, 346: 240.0, 347: 71.0, 348: 19.0, 349: 28.0, 356: 253.0, 357: 252.0, 358: 195.0, 371: 57.0, 372: 252.0, 373: 252.0, 374: 63.0, 384: 253.0, 385: 252.0, 386: 195.0, 399: 198.0, 400: 253.0, 401: 190.0, 412: 255.0, 413: 253.0, 414: 196.0, 426: 76.0, 427: 246.0, 428: 252.0, 429: 112.0, 440: 253.0, 441: 252.0, 442: 148.0, 454: 85.0, 455: 252.0, 456: 230.0, 457: 25.0, 466: 7.0, 467: 135.0, 468: 253.0, 469: 186.0, 470: 12.0, 482: 85.0, 483: 252.0, 484: 223.0, 493: 7.0, 494: 131.0, 495: 252.0, 496: 225.0, 497: 71.0, 510: 85.0, 511: 252.0, 512: 145.0, 520: 48.0, 521: 165.0, 522: 252.0, 523: 173.0, 538: 86.0, 539: 253.0, 540: 225.0, 547: 114.0, 548: 238.0, 549: 253.0, 550: 162.0, 566: 85.0, 567: 252.0, 568: 249.0, 569: 146.0, 570: 48.0, 571: 29.0, 572: 85.0, 573: 178.0, 574: 225.0, 575: 253.0, 576: 223.0, 577: 167.0, 578: 56.0, 594: 85.0, 595: 252.0, 596: 252.0, 597: 252.0, 598: 229.0, 599: 215.0, 600: 252.0, 601: 252.0, 602: 252.0, 603: 196.0, 604: 130.0, 622: 28.0, 623: 199.0, 624: 252.0, 625: 252.0, 626: 253.0, 627: 252.0, 628: 252.0, 629: 233.0, 630: 145.0, 651: 25.0, 652: 128.0, 653: 252.0, 654: 253.0, 655: 252.0, 656: 141.0, 657: 37.0}))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfsvm.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame 변환\n",
    "\n",
    "DataFrame으로 만들어진 데이터를 변환해보자.\n",
    "이러한 작업이 필요한 이유는 **기계학습에 넘겨줄 입력데이터를 형식에 맞추어야** 하기 때문이다.\n",
    "데이터는 형식에 맞게 변환되고, 군집화, 회귀분석, 분류, 추천 모델 등에 입력으로 사용된다\n",
    "물론 데이터는 '일련의 수' 또는 '텍스트'로 구성된다.\n",
    "이런 데이터로부터 특징을 추출하여 **feature vectors**를 구성한다.\n",
    "지도학습을 하는 경우에는 **class 또는 label** 값이 필요하다.\n",
    "\n",
    "구분 | 컬럼 구성\n",
    "-----|-----\n",
    "DataFrame | 'label' (DoubleType), 'features' (sparse or dense vectors)\n",
    "RDD | LabeledPoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**RDD**를 만들고 나서도 데이터를 변환하기 위해 map-reduce와 같은 **transformation**, **action**을 사용하는 것과 같이,\n",
    "**DataFrame**도 역시 **Transformer**, **Estimator**를 사용할 수 있다.\n",
    "Estimator는 DataFrame에 적용되는 알고리즘을 말하는 것으로, Transformer를 생성해낸다.\n",
    "Transformer는 DataFrame을 다른 DataFrame으로 변환하는 알고리즘을 말한다.\n",
    "\n",
    "변환 기능 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "Estimator | DataFrame에 적용해서 Transformer를 생성한다.| **```Estimator.fit()```**\n",
    "Transformer | DataFrame을 적용해서 다른 DataFrame으로 생성한다. | **```Transformer.transform()```**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.1 텍스트 변환\n",
    "\n",
    "#### Bag of Words 모델\n",
    "\n",
    "텍스트를 단어의 집합, **'bag of words'**으로 구성된다고 보며, 단어의 **순서**는 의미를 가지지 않는다.\n",
    "\n",
    "이런 영화리뷰가 있다고 하자.\n",
    "\"...그 영화는 매우 강렬했다. 그냥 좋았다. 영화관에서 보는 동안 긴장을 늦출 수 없었다. 갑돌이가 분장한 악당의 케릭터가 만들어지는 과정은 흥미롭지 않을 수가 없었다. 무비의 이야기 전개는 빠르고, 무엇이 진실이고 거짓인지 판단할 수 없었다. 누가 이런 영화를 좋아 하지 않을 수가 있겠는가 이모티콘...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단계\n",
    "\n",
    "단계 1: 단어로 분할 Tokenization\n",
    "그, 영화는, 매우, 강렬했다, 그냥, 좋았다, 영화관에서, 보는, 동안, 긴장을, 늦출, 수, 없었다, 갑돌이가, 분장한, 악당의, 케릭터가, 만들어지는, 과정은, 흥미롭지, 않을, 수가, 없었다, 무비의, 이야기, 전개는, 빠르고, 무엇이, 진실이고, 거짓인지, 판단할, 수, 없었다, 누가, 이런, 영화를, 좋아, 하지, 않을, 수가, 있겠는가, 이모티콘\n",
    "\n",
    "단계 2: 정리\n",
    "- 불필요, 오류 정리\n",
    "\n",
    "단계 3: 불용어 stopwords 제거\n",
    "- 그, 수, 수가, 수, 이런, 하지, 수가 등\n",
    "\n",
    "단계 4: 어간 추출 stemming\n",
    "영화는, 영화의는 다른 단어지만 조사를 제거하면 동일한 단어\n",
    "- 좋았다, 좋아 단어들은 어근을 판별하면 동일한 단어이다.\n",
    "- 영화, 무비의 단어는 이음동의"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "단계 5: 계량화\n",
    "- word vector로 만든다.\n",
    "- 있다-없다, 단어빈도, TF-IDF 사용할 수 있다.<br>dense, sparse 모두 가능하다.\n",
    "```[1,1,1,1,1,0,0],[0,1,0,1,1,1,1]```\n",
    "\n",
    "```python\n",
    "[0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 1 0 1 1 0 0 0]\n",
    "[0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
    "[1 0 0 1 0 0 1 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 1 1]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
    "[0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.2 Python을 사용한 단어 빈도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let it be lyrics\n",
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문서, 문장, 단어의 계층을 먼저 이해해야 한다.\n",
    "문서는 문장으로 구성되어 있고, 문장은 단어로 구성되어 있다.\n",
    "따라서 첫째 반복문은 문서의 각 문장에 대해, 단어로 분리하고 있다.\n",
    "그 다음 반복문은 각 단어에 대해 빈도를 계산한다.\n",
    "각 단어가 키가 되는데, **키가 존재하면 빈도를 증가하고, 존재하지 않으면 새로운 키를 생성**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 단어 빈도는 dictionary d에 저장하였다.\n",
    "dictionary는 키, 빈도의 쌍으로 저장되어 있어서 ```iteritems()```으로 읽어낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right 1\n",
      "be 7\n",
      "is 1\n",
      "When 1\n",
      "it 7\n",
      "in 3\n",
      "Mary 1\n",
      "Speaking 2\n",
      "standing 1\n",
      "darkness 1\n",
      "find 1\n",
      "wisdom, 3\n",
      "to 1\n",
      "Let 4\n",
      "And 1\n",
      "I 1\n",
      "let 3\n",
      "She 1\n",
      "words 3\n",
      "Mother 1\n",
      "front 1\n",
      "trouble 1\n",
      "me 2\n",
      "myself 1\n",
      "hour 1\n",
      "of 6\n",
      "times 1\n",
      "Whisper 1\n",
      "my 1\n",
      "comes 1\n"
     ]
    }
   ],
   "source": [
    "# Python 2 - 3 compatible code\n",
    "# for k,v in d.items():\n",
    "for k,v in d.iteritems():\n",
    "    print k,v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.3 Spark\n",
    "\n",
    "텍스트를 2차원 배열로 만들어, DataFrame을 생성한다. schema는 만들어 주지 않아도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d,['sent'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```truncate=True```는 줄여서, ```False```는 출력을 줄이지 않고 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|        우리 Let it be|\n",
      "|         나 Let it be|\n",
      "|         너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.4 Tokenizer\n",
    "\n",
    "Tokenizer는 문장을 단어로 분리한다.\n",
    "분리하는 기준은 whitespace로 공백, TAB, CR, New Line 등이 해당된다.\n",
    "입력은 \"sent\"로 출력은 \"words\"로 한다.\n",
    "\n",
    "구분 | 설명 | 예\n",
    "----------|----------|----------|\n",
    "corpus | 말뭉치 | \"why she had to go\", \"where she have to go\"\n",
    "document | 레코드 | \"why she had to go\"\n",
    "vocabularay | 중복없는 단어 집합 | \"why\",\"she\",\"had\",\"to\",\"go\",\"where\",\"have\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```transform()```은 앞서 만든 ```tokenizer```모델에 DataFrame을 변환하여 다른 DataFrame을 생성한다.\n",
    "그 결과는 문자열 배열로 구성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```for```문으로 출력해보자. ```Row()``` 객체로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(sent=u'When I find myself in times of trouble', words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'])\n",
      "Row(sent=u'Mother Mary comes to me', words=[u'mother', u'mary', u'comes', u'to', u'me'])\n",
      "Row(sent=u'Speaking words of wisdom, let it be', words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'])\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\", \"words\").take(3):\n",
    "    print r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.5 RegTokenizer\n",
    "\n",
    "Tokenizer는 white space로 분리하지만, RegexTokenizer는 단어를 분리하기 위해 **정규표현식**을 적용할 수 있다.\n",
    "정규표현식을 사용하여 분리하거나 특정 패턴을 추출할 수 있다.\n",
    "공백으로 분리할 경우 간단히 정규표현식 ```\\s``` 패턴을 적용할 수 있다.\n",
    "한글에는 ```\\w``` 패턴이 적용되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|\n",
      "|         나 Let it be|    [나, let, it, be]|\n",
      "|         너 Let it be|    [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.6 Stopwords\n",
    "\n",
    "텍스트를 분리하고 나면, 별 의미가 없거나 쓸모가 없는 단어들이 존재한다.\n",
    "예를 들어 이, 그, 저와 같은 **한 단어** 또는 있다 등과 같은 **일부 동사**, 그래서, 그러나 등과 같은 **접속사** 등이 후보가 될 수 있다.\n",
    "이런 불필요한 단어들을 불용어 Stopwords라고 하며, 입력데이터에서 제거하도록 한다.\n",
    "영어의 경우 불용어가 식별되어 제공되고 있다\n",
    "http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 현재 stop words에 자신의 것을 추가해서, 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_489b8b8ff0eb64c1cbf7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)\n",
    "\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they them their theirs themselves what which who whom this that these those am is are was were be been being have has had having do does did doing a an the and but if or because as until while of at by for with about against between into through during before after above below to from up down in out on off over under again further then once here there when where why how all any both each few more most other some such no nor not only own same so than too very s t can will just don should now i'll you'll he'll she'll we'll they'll i'd you'd he'd she'd we'd they'd i'm you're he's she's it's we're they're i've we've you've they've isn't aren't wasn't weren't haven't hasn't hadn't don't doesn't didn't won't wouldn't shan't shouldn't mustn't can't couldn't cannot could here's how's let's ought that's there's what's when's where's who's why's would 나 너 우리\n"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print e,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 한글의 stop words '너','우리'가 제거되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.7 CountVectorizer\n",
    "\n",
    "```CountVectorizer```는 텍스트를 입력해서, word vector를 출력한다.\n",
    "우선 Tokenizer를 사용해서 단어로 분리하고난 후 사용한다.\n",
    "\n",
    "* minDF\n",
    "    * 소수점은 비율, 사용된 문서 수/전체 문서 수\n",
    "        * 정수는 사용된 문서 수, 단어가 몇 개의 문서에 사용되어야 하는지\n",
    "\n",
    "* 입력: a collection of text documents\n",
    "* 출력: word vector (sparse) vocabulary x TF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3번째 문장 \"Speaking words of wisdom, let it be\"의 word vector를 구성해 본다.\n",
    "id 값은 모든 문장에서 단어를 추출하고 나서야 부여된다.\n",
    "\n",
    "단어 (3행 \"Speaking words of wisdom, let it be\") | id | 빈도 | \n",
    "-----|-----|-----\n",
    "Speaking | 7 | 1\n",
    "words | 13 | 1\n",
    "of | stopword | 0\n",
    "wisdom | 12 | 1\n",
    "let | 3 | 1\n",
    "it | stopword | 0\n",
    "be | stopword | 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문서와 단어별로 표를 만들 수 있다.\n",
    "Let it be는 4번 반복이 되고 있다.\n",
    "'be': 1 'it': 9 'let': 10를 확인해보자.\n",
    "\n",
    "위 **word vector**를 표로 나타내면 아래와 같다.\n",
    "행은 문장, 열은 id이다.\n",
    "**3행은 doc2**이다. 해당하는 **단어 id의 빈도**를 적었다. 다른 행과 열은 이해를 돕기 위해 비워 놓았다.\n",
    "\n",
    "```doc``` \\ 단어 id  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |10 |11 |12 |13 |...\n",
    "------|---|---|---|---|---|---|---|---|---|---|---|---|---|---\n",
    "```doc 0``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 1``` |   |   |   |   |   |   |   |   |   |   |   |   |   |...\n",
    "```doc 2``` |   |   | 1 |   |   |   | 1 |   |   |   |   | 1 | 1 |...\n",
    "...   |   |   |   |   |   |   |   |   |   |   |   |   |   |..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sklearn CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 1 1 0 0 0]\n",
      " [1 0 0 0 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 1 1]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Spark CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```CountVectorizerModel```은 ```fit()```하고 나면 얻어진다. 다음에 사용하는 ```HashingTF```는 ```fit()```하지 않는다는 점에서 차이가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print type(cv),type(cvModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cvDf = cvModel.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 가운데 일부 컬럼만을 선택하여 출력할 수 있다.\n",
    "```(16,[5,6,8],[1.0,1.0,1.0])```\n",
    "16은 전체 단어의 개수, 그리고 다음 5,6,8은 값이 있는 컬럼 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|And in my hour of...|    [hour, darkness]|(16,[7,9],[1.0,1.0])|\n",
      "|She is standing r...|[standing, right,...|(16,[4,12,15],[1....|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "|        우리 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         나 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|         너 Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|           Let it be|               [let]|      (16,[0],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|(16,[0,1,2,11],[1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.select('sent','nostops','cv').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```CountVectorizer```에서 사용된 단어 목록을 출력할 수 있다. 아래 단어의 수를 세어보면 위 sparse vector의 컬럼 개수와 동일하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'let',\n",
       " u'wisdom,',\n",
       " u'words',\n",
       " u'speaking',\n",
       " u'right',\n",
       " u'trouble',\n",
       " u'find',\n",
       " u'hour',\n",
       " u'times',\n",
       " u'darkness',\n",
       " u'mother',\n",
       " u'whisper',\n",
       " u'front',\n",
       " u'mary',\n",
       " u'comes',\n",
       " u'standing']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvModel.vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.8 TF-IDF\n",
    "\n",
    "```TfidfTransformer```는 **TF-IDF(Term Frequency-Inverse Document Frequency)**를 계산한다.\n",
    "이를 위해서는 우선 Tokenizer를 사용하여 문장을 단어로 분리해 놓아야 한다.\n",
    "\n",
    "HashingTF를 사용하여 'word vector'를 계산한다.\n",
    "HashingTF은 hash함수에 따라 단어의 고유번호를 생성하며, hash고유번호의 충돌 가능성을 줄이기 위해, 단어 수를 제한할 수 있다.\n",
    "그리고 IDF를 계산하고, TF-IDF를 계산한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### S.5.8.1 TF-IDF 계산\n",
    "\n",
    "'Let it be'가사 세 번째 줄 **'wisdom' 단어**의 TF-IDF를 계산해보자.\n",
    "**TF**는 단어빈도수, 즉 문서에 단어가 나타난 빈도수를 의미한다.\n",
    "단어빈도는 경우에 따라서는 문제가 될 수 있다. 예를 들어, 'a', 'the', 'of'와 같은 단어는 빈도는 높지만 별로 유용하지 못하다.\n",
    "이 경우 IDF는 유용하다. **IDF**는 자주 나타나는 단어에 대한 가중치를 줄이고, 드물게 나타나는 단어에 가중치를 높이는 방식으로 계산된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "t는 단어, 문서는 d, D는 corpus,\n",
    "\n",
    "항목 | 설명 | 예제\n",
    "-----|-----|-----\n",
    "tf(d,f) | 단어 t가 문서 d에서 나타나는 단어의 빈도 수, term frequency | $f_{t,d}$ / (number of words in d) = 1/4 = 0.25<br>(3번째 문서에 stopwords를 제외하면 4개의 단어, wisdom은 1회 나타난다.)\n",
    "df | document frequency 단어가 나타난 문서 수 | 3 (wisdom이 포함된 문서는 3)\n",
    "N | number of documents 전체 문서의 수 | 11 (전체의 문서는 11개)\n",
    "idf | inverse document frequency 단어가 나타난 문서의 비율을 거꾸로 | ln(N+1 / df+1) + 1 = log(12/4) + 1 = 1.09861 + 1<br>0으로 나뉘는 것을 방지하기 위해 **smoothing**, 즉 1을 더한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "프로그래밍에서는 메모리를 적게 사용하도록 설계되어 있다. ```1/4```와 같이 정수 타잎으로 연산하면, 정수를 사용하여 연산하고 그 결과도 정수를 출력하게 된다. 이를 변환하기 위해 ```1.```의 경우에서와 같이 소수를 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09861228867\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### S.5.8.2 sklearn을 사용한 TF-IDF\n",
    "\n",
    "우선 'sklearn'의 TF-IDF를 계산해보자.\n",
    "**```CountVectorizer```**는 텍스트를 단어의 빈도로 변환해주어, 문서 x 단어 표를 출력할 수 있다.\n",
    "**```CountVectorizer()```**의 인자로\n",
    "analyzer (\"word\", \"character ngram\" 등 선택),\n",
    "tokenizer (단어의 tokenizer를 지정),\n",
    "stop_words (불용어 처리 기준),\n",
    "max_features (최대 속성 개수) 등을 지정할 수 있다.\n",
    "그 다음으로, TF-IDF를 계산할 수 있다. 이 때 (문서id, 단어id) 별로 결과가 출력된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```TfidfVectorizer```를 사용해서 계산하면 그 결과를 아래와 같이 볼 수 있다.\n",
    "```python\n",
    "(2,12) 2.09861228867\n",
    "```\n",
    "\n",
    "결과에서\n",
    "**'2'**는 3번째 문서번호, **'12'**는 'wisdom' 단어번호\n",
    "TF-IDF는 ```2.09861228867```이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, min_df=1, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.79175946923\n",
      "  (0, 9)\t2.79175946923\n",
      "  (1, 0)\t2.79175946923\n",
      "  (1, 4)\t2.79175946923\n",
      "  (1, 5)\t2.79175946923\n",
      "  (2, 3)\t1.40546510811\n",
      "  (2, 12)\t2.09861228867\n",
      "  (2, 13)\t2.09861228867\n",
      "  (2, 7)\t2.38629436112\n",
      "  (3, 1)\t2.79175946923\n",
      "  (3, 2)\t2.79175946923\n",
      "  (4, 6)\t2.79175946923\n",
      "  (4, 8)\t2.79175946923\n",
      "  (5, 3)\t1.40546510811\n",
      "  (5, 12)\t2.09861228867\n",
      "  (5, 13)\t2.09861228867\n",
      "  (5, 7)\t2.38629436112\n",
      "  (6, 3)\t1.40546510811\n",
      "  (7, 3)\t1.40546510811\n",
      "  (8, 3)\t1.40546510811\n",
      "  (9, 3)\t1.40546510811\n",
      "  (10, 11)\t2.79175946923\n",
      "  (10, 3)\t1.40546510811\n",
      "  (10, 12)\t2.09861228867\n",
      "  (10, 13)\t2.09861228867\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.fit_transform(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'standing': 8, u'right': 6, u'darkness': 1, u'hour': 2, u'whisper': 11, u'times': 9, u'let': 3, u'speaking': 7, u'words': 13, u'mother': 5, u'trouble': 10, u'wisdom': 12, u'mary': 4, u'comes': 0}\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.79175947  2.79175947  2.79175947  1.40546511  2.79175947  2.79175947\n",
      "  2.79175947  2.38629436  2.79175947  2.79175947  2.79175947  2.79175947\n",
      "  2.09861229  2.09861229]\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.idf_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### S.5.8.3 Spark를 사용한 TF-IDF\n",
    "\n",
    "HashingTF는 단어집합을 워드벡터 word vector로 변환하는데, 해시함수를 사용해서 단어에 해당하는 일련번호를 결정한다.\n",
    "HashingTF에서의 **```numFeatures```는 $2^n$**으로 결정하게 된다.\n",
    "단어 갯수가 900이면, $2^{10}=1024$이므로 1024로 설정하면 된다.\n",
    "기본은 $2^{18}=262,144$이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\", numFeatures=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```HashingTF```는 ```fit()```하지 않고 ```transform()``` 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(50,[10,24,43],[1.0,1.0,1.0])```\n",
    "16은 해시 개수 (앞서 CountVectorizer의 경우에서와 같이 전체 단어의 개수가 아니다), 그리고 다음 [10,24,43]은 값이 있는 **해시 컬럼** 번호, 1.0,1.0,1.0은 그 값을 말한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|                hash|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|(50,[10,24,43],[1...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|(50,[1,21,24],[1....|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(50,[9,12,14,41],...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|(50,[23,27],[1.0,...|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|(50,[24,43,46],[1...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|(50,[9,12,14,41],...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|               [let]|     (50,[14],[1.0])|\n",
      "|         나 Let it be|    [나, let, it, be]|               [let]|     (50,[14],[1.0])|\n",
      "|         너 Let it be|    [너, let, it, be]|               [let]|     (50,[14],[1.0])|\n",
      "|           Let it be|       [let, it, be]|               [let]|     (50,[14],[1.0])|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|(50,[9,14,15,41],...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idfModel = idf.fit(hashDf)\n",
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(nostops=[u'find', u'times', u'trouble'], hash=SparseVector(50, {10: 1.0, 24: 1.0, 43: 1.0}))\n",
      "Row(nostops=[u'mother', u'mary', u'comes'], hash=SparseVector(50, {1: 1.0, 21: 1.0, 24: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'hour', u'darkness'], hash=SparseVector(50, {23: 1.0, 27: 1.0}))\n",
      "Row(nostops=[u'standing', u'right', u'front'], hash=SparseVector(50, {24: 1.0, 43: 1.0, 46: 1.0}))\n",
      "Row(nostops=[u'speaking', u'words', u'wisdom,', u'let'], hash=SparseVector(50, {9: 1.0, 12: 1.0, 14: 1.0, 41: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n",
      "Row(nostops=[u'let'], hash=SparseVector(50, {14: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"nostops\",\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.10 Word2Vec\n",
    "\n",
    "* see wikipedia https://en.wikipedia.org/wiki/Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(w2v=DenseVector([-0.0367, 0.0097, 0.0479]))\n",
      "Row(w2v=DenseVector([-0.0482, 0.0223, 0.0095]))\n",
      "Row(w2v=DenseVector([0.052, -0.001, -0.0019]))\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(vectorSize=3,minCount=0,inputCol=\"words\",outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)\n",
    "for e in w2vDf.select(\"w2v\").take(3):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.11 NGram\n",
    "\n",
    "* unigram은 한 단어로, bigram은 두 단어로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|              ngrams|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|[and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|[she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking words, ...|\n",
      "|        우리 Let it be|   [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|         나 Let it be|    [나, let, it, be]|[나 let, let it, i...|\n",
      "|         너 Let it be|    [너, let, it, be]|[너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|     [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper words, w...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n",
      "Row(words=[u'when', u'i', u'find', u'myself', u'in', u'times', u'of', u'trouble'], ngrams=[u'when i', u'i find', u'find myself', u'myself in', u'in times', u'times of', u'of trouble'])\n",
      "Row(words=[u'mother', u'mary', u'comes', u'to', u'me'], ngrams=[u'mother mary', u'mary comes', u'comes to', u'to me'])\n",
      "Row(words=[u'speaking', u'words', u'of', u'wisdom,', u'let', u'it', u'be'], ngrams=[u'speaking words', u'words of', u'of wisdom,', u'wisdom, let', u'let it', u'it be'])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n",
    "ngramDf = ngram.transform(tokDf)\n",
    "ngramDf.show()\n",
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.4 StringIndexer\n",
    "\n",
    "문자열 컬럼을 인덱스 컬럼으로 변환한다. **빈도가 제일 높은 순서**로 ```0.0```부터 인덱스 값이 주어진다. 인덱스는 double 형을 가지게 된다.\n",
    "없는 레이블에 대해서는 예외가 발생할 수 있으므로 (default), ```setHandleInvalid(\"skip\")``` 함수로 'skip', 'keep', 'error' 등 처리\n",
    "\n",
    "구분 | 설명 | 예\n",
    "-----|-----|-----\n",
    "nominal | 명목 또는 구분 값 cateogry  | 사자, 호랑이, 사람\n",
    "ordinal | 명목값과 다른 점은 순서가 있다. | 키 low, med, high\n",
    "interval | 일정한 간격이 있다. | 150-165, 165-180, 180-195\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")\n",
    "model=labelIndexer.fit(myDf)\n",
    "siDf=model.transform(myDf)\n",
    "siDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.12 연속데이터의 변환\n",
    "\n",
    "몸무게(inches), 키(pounds) 데이터를 분석해보자.\n",
    "이 데이터는 정량, 연속 데이터이다. \n",
    "출처는 https://people.sc.fsu.edu/~jburkardt/data/csv/hw_200.csv\n",
    "\n",
    "```python\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))\n",
    "\n",
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+\n",
      "|  id|weight|height|weight2|\n",
      "+----+------+------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|\n",
      "| 4.0| 68.22|142.34|    1.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|\n",
      "| 7.0|  69.8|141.49|    1.0|\n",
      "| 8.0| 70.01|136.46|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|\n",
      "+----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Binarizer\n",
    "binarizer = Binarizer(threshold=68.0, inputCol=\"weight\", outputCol=\"weight2\")\n",
    "binDf = binarizer.transform(myDf)\n",
    "binDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+-------+-------+\n",
      "|  id|weight|height|weight2|height3|\n",
      "+----+------+------+-------+-------+\n",
      "| 1.0| 65.78|112.99|    0.0|    0.0|\n",
      "| 2.0| 71.52|136.49|    1.0|    1.0|\n",
      "| 3.0|  69.4|153.03|    1.0|    2.0|\n",
      "| 4.0| 68.22|142.34|    1.0|    2.0|\n",
      "| 5.0| 67.79| 144.3|    0.0|    2.0|\n",
      "| 6.0|  68.7| 123.3|    1.0|    0.0|\n",
      "| 7.0|  69.8|141.49|    1.0|    2.0|\n",
      "| 8.0| 70.01|136.46|    1.0|    1.0|\n",
      "| 9.0|  67.9|112.37|    0.0|    0.0|\n",
      "|10.0| 66.78|120.67|    0.0|    0.0|\n",
      "+----+------+------+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "\n",
    "discretizer = QuantileDiscretizer(numBuckets=3, inputCol=\"height\", outputCol=\"height3\")\n",
    "qdDf = discretizer.fit(binDf).transform(binDf)\n",
    "qdDf.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 5.5.13 VectorAssembler\n",
    "\n",
    "* 열을 묶어서 Vector열로 만든다.\n",
    "* string은 묶을 수 없다.\n",
    "* pyspark.ml.linalg.Vectors를 사용한다. (주의: pyspark.mllib.linalg.Vectors를 사용하지 않는다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: double (nullable = true)\n",
      " |-- weight: double (nullable = true)\n",
      " |-- height: double (nullable = true)\n",
      " |-- weight2: double (nullable = true)\n",
      " |-- height3: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+---+------+------+-------+-------+---------+\n",
      "| id|weight|height|weight2|height3| features|\n",
      "+---+------+------+-------+-------+---------+\n",
      "|1.0| 65.78|112.99|    0.0|    0.0|(2,[],[])|\n",
      "|2.0| 71.52|136.49|    1.0|    1.0|[1.0,1.0]|\n",
      "|3.0|  69.4|153.03|    1.0|    2.0|[1.0,2.0]|\n",
      "|4.0| 68.22|142.34|    1.0|    2.0|[1.0,2.0]|\n",
      "|5.0| 67.79| 144.3|    0.0|    2.0|[0.0,2.0]|\n",
      "+---+------+------+-------+-------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "va = VectorAssembler(inputCols=[\"weight2\",\"height3\"],outputCol=\"features\")\n",
    "vaDf = va.transform(qdDf)\n",
    "vaDf.printSchema()\n",
    "vaDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### S.5.14 Pipeline\n",
    "\n",
    "**Pipeline**은 여러 Estimator를 묶은 Estimator를 반환한다. 단계적으로 Estimator를 적용하기 위해 사용한다.\n",
    "\n",
    "* Pipeline은 여러 작업을 묶어, 순서대로 단계적으로 처리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    (0L, \"a b c d e spark\", 1.0),\n",
    "    (1L, \"b d\", 0.0),\n",
    "    (2L, \"spark f g h\", 1.0),\n",
    "    (3L, \"hadoop mapreduce\", 0.0),\n",
    "    (4L, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n",
    "\n",
    "# Fit the pipeline to training documents.\n",
    "model = pipeline.fit(df)\n",
    "myDf = model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 연설문을 기계학습하기 위해 변환\n",
    "\n",
    "한 글자 단어를 제외하고, 단어의 TF-IDF를 계산해서 features로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#spark.createDataFrame(os.path.join(\"data\", \"20191021_policeAddress.txt\"))\n",
    "#df=spark.read.text(os.path.join(\"data\", \"20191021_policeAddress.txt\"))\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "police=spark.read\\\n",
    "    .options(header=\"true\", delimiter=\" \", inferSchema=\"true\")\\\n",
    "    .schema(\n",
    "        StructType([\n",
    "            StructField(\"sent\",StringType()),\n",
    "            ])\n",
    "    )\\\n",
    "    .text(os.path.join(\"data\", \"20191021_policeAddress.txt\")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문장이 잘리지 않고 온전하게 출력되려면 'False'로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sent                                                                                                                                 |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.                                                                                              |\n",
      "|                                                                                                                                     |\n",
      "|국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.                              |\n",
      "|                                                                                                                                     |\n",
      "|오늘 홍조근정훈장을 받으신 중앙경찰학교장 이은정 치안감님, 근정포장을 받으신 광주남부경찰서 김동현 경감님을 비롯한 수상자 여러분께 각별한 축하와 감사를 드립니다. 또한 경찰 영웅으로 추서되신 차일혁, 최중락님께 국민의 사랑을 전해드립니다.|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "police.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(police)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|sent                                                                                                                                 |words                                                                                                                                                            |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.                                                                                              |[존경하는, 국민, 여러분,, 경찰관, 여러분,, 일흔네, 돌, ‘경찰의, 날’입니다.]                                                                                                                |\n",
      "|                                                                                                                                     |[]                                                                                                                                                               |\n",
      "|국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.                              |[국민의, 안전을, 위해, 밤낮없이, 애쓰시는, 전국의, 15만, 경찰관, 여러분께, 먼저, 감사를, 드립니다., 전몰·순직, 경찰관들의, 고귀한, 희생에, 경의를, 표합니다., 유가족, 여러분께, 위로의, 마음을, 전합니다.]                                  |\n",
      "|                                                                                                                                     |[]                                                                                                                                                               |\n",
      "|오늘 홍조근정훈장을 받으신 중앙경찰학교장 이은정 치안감님, 근정포장을 받으신 광주남부경찰서 김동현 경감님을 비롯한 수상자 여러분께 각별한 축하와 감사를 드립니다. 또한 경찰 영웅으로 추서되신 차일혁, 최중락님께 국민의 사랑을 전해드립니다.|[오늘, 홍조근정훈장을, 받으신, 중앙경찰학교장, 이은정, 치안감님,, 근정포장을, 받으신, 광주남부경찰서, 김동현, 경감님을, 비롯한, 수상자, 여러분께, 각별한, 축하와, 감사를, 드립니다., 또한, 경찰, 영웅으로, 추서되신, 차일혁,, 최중락님께, 국민의, 사랑을, 전해드립니다.]|\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.\n",
      " \n",
      "국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.\n"
     ]
    }
   ],
   "source": [
    "for r in tokDf.select(\"sent\").take(3):\n",
    "    print r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4e989379a80b9a486107"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop.setStopWords([u\"돌\",u\"너\", u\"우리\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_4e989379a80b9a486107"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|               words|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|존경하는 국민 여러분, 경찰관 ...|[존경하는, 국민, 여러분,, ...|[존경하는, 국민, 여러분,, ...|\n",
      "|                    |                  []|                  []|\n",
      "|국민의 안전을 위해 밤낮없이 애...|[국민의, 안전을, 위해, 밤낮...|[국민의, 안전을, 위해, 밤낮...|\n",
      "|                    |                  []|                  []|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|[오늘, 홍조근정훈장을, 받으신...|[오늘, 홍조근정훈장을, 받으신...|\n",
      "|                    |                  []|                  []|\n",
      "|       사랑하는 경찰관 여러분,|   [사랑하는, 경찰관, 여러분,]|   [사랑하는, 경찰관, 여러분,]|\n",
      "|                    |                  []|                  []|\n",
      "|여러분의 헌신적 노력으로 우리의...|[여러분의, 헌신적, 노력으로,...|[여러분의, 헌신적, 노력으로,...|\n",
      "|                    |                  []|                  []|\n",
      "|치안의 개선은 국민의 체감으로 ...|[치안의, 개선은, 국민의, 체...|[치안의, 개선은, 국민의, 체...|\n",
      "|                    |                  []|                  []|\n",
      "|한국을 찾는 외국 관광객들도 우...|[한국을, 찾는, 외국, 관광객...|[한국을, 찾는, 외국, 관광객...|\n",
      "|                    |                  []|                  []|\n",
      "|올해는 ‘경찰의 날’에 맞춰 국...|[올해는, ‘경찰의, 날’에, ...|[올해는, ‘경찰의, 날’에, ...|\n",
      "|                    |                  []|                  []|\n",
      "|      자랑스러운 경찰관 여러분,|  [자랑스러운, 경찰관, 여러분,]|  [자랑스러운, 경찰관, 여러분,]|\n",
      "|                    |                  []|                  []|\n",
      "|경찰헌장은 “나라와 겨레를 위하...|[경찰헌장은, “나라와, 겨레를...|[경찰헌장은, “나라와, 겨레를...|\n",
      "|                    |                  []|                  []|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'find', u'times', u'trouble']\n",
      "[u'mother', u'mary', u'comes']\n",
      "[u'speaking', u'words', u'wisdom,', u'let']\n"
     ]
    }
   ],
   "source": [
    "for r in stopDf.select(\"nostops\").take(3):\n",
    "    for e in r:\n",
    "        print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 문제 텍스트\n",
    "\n",
    "http://help.sentiment140.com/for-students/\n",
    "\n",
    "* 트윗 정서 (0=부정, 2=중립, 4=긍정)\n",
    "* 트윗 ID\n",
    "* 트윗 일자\n",
    "* 조회 (없으면 NO_QUERY)\n",
    "* 사용자\n",
    "* 트윗 텍스트\n",
    "\n",
    "1) 전체 데이터 갯수, 각 '부정' '긍정' '중립' 별 데이터 갯수\n",
    "2) 트윗 정서 컬럼을 'label'로 변경\n",
    "5) stopwords 제거하고, 출력\n",
    "6) pipeline으로 tf-idf 계산하고 'features' 컬럼으로 변경\n",
    "\n",
    "https://github.com/tthustla/twitter_sentiment_analysis_part2/blob/master/Capstone_part3-Copy1.ipynb\n",
    "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jsl/Code/git/bb/jsl/pyds'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet = spark.read\\\n",
    "    .format('com.databricks.spark.csv')\\\n",
    "    .options(header='false', inferschema='true')\\\n",
    "    .load(os.path.join(\"data\",\"stanford\", \"testdata.manual.2009.06.14.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0', '_c1', '_c2', '_c3', '_c4', '_c5']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "498"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_c1': 0, '_c0': 0, '_c3': 0, '_c2': 0, '_c5': 0, '_c4': 0}\n"
     ]
    }
   ],
   "source": [
    "def countNull(df,var):\n",
    "    return df.where(df[var].isNull()).count()\n",
    "\n",
    "missing = {c: countNull(tweet,c) for c in ['_c0', '_c1', '_c2', '_c3', '_c4', '_c5']}\n",
    "\n",
    "print missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "|_c0|_c1|                 _c2|    _c3|           _c4|                 _c5|\n",
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "|  4|  3|Mon May 11 03:17:...|kindle2|        tpryan|@stellargirl I lo...|\n",
      "|  4|  4|Mon May 11 03:18:...|kindle2|        vcu451|Reading my kindle...|\n",
      "|  4|  5|Mon May 11 03:18:...|kindle2|        chadfu|Ok, first assesme...|\n",
      "|  4|  6|Mon May 11 03:19:...|kindle2|         SIX15|@kenburbary You'l...|\n",
      "|  4|  7|Mon May 11 03:21:...|kindle2|      yamarama|@mikefish  Fair e...|\n",
      "|  4|  8|Mon May 11 03:22:...|kindle2|  GeorgeVHulme|@richardebaker no...|\n",
      "|  0|  9|Mon May 11 03:22:...|    aig|       Seth937|Fuck this economy...|\n",
      "|  4| 10|Mon May 11 03:26:...| jquery|     dcostalis|Jquery is my new ...|\n",
      "|  4| 11|Mon May 11 03:27:...|twitter|       PJ_King|       Loves twitter|\n",
      "|  4| 12|Mon May 11 03:29:...|  obama|   mandanicole|how can you not l...|\n",
      "|  2| 13|Mon May 11 03:32:...|  obama|          jpeb|Check this video ...|\n",
      "|  0| 14|Mon May 11 03:32:...|  obama|   kylesellers|@Karoli I firmly ...|\n",
      "|  4| 15|Mon May 11 03:33:...|  obama|   theviewfans|House Corresponde...|\n",
      "|  4| 16|Mon May 11 05:05:...|   nike|        MumsFP|Watchin Espn..Jus...|\n",
      "|  0| 17|Mon May 11 05:06:...|   nike|   vincentx24x|dear nike, stop w...|\n",
      "|  4| 18|Mon May 11 05:20:...| lebron|  cameronwylie|#lebron best athl...|\n",
      "|  0| 19|Mon May 11 05:20:...| lebron|       luv8242|I was talking to ...|\n",
      "|  4| 20|Mon May 11 05:21:...| lebron|    mtgillikin|i love lebron. ht...|\n",
      "|  0| 21|Mon May 11 05:21:...| lebron|ursecretdezire|@ludajuice Lebron...|\n",
      "|  4| 22|Mon May 11 05:21:...| lebron|     Native_01|@Pmillzz lebron I...|\n",
      "+---+---+--------------------+-------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|_c5                                                                                                                                         |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|@stellargirl I loooooooovvvvvveee my Kindle2. Not that the DX is cool, but the 2 is fantastic in its own right.                             |\n",
      "|Reading my kindle2...  Love it... Lee childs is good read.                                                                                  |\n",
      "|Ok, first assesment of the #kindle2 ...it fucking rocks!!!                                                                                  |\n",
      "|@kenburbary You'll love your Kindle2. I've had mine for a few months and never looked back. The new big one is huge! No need for remorse! :)|\n",
      "|@mikefish  Fair enough. But i have the Kindle2 and I think it's perfect  :)                                                                 |\n",
      "|@richardebaker no. it is too big. I'm quite happy with the Kindle2.                                                                         |\n",
      "|Fuck this economy. I hate aig and their non loan given asses.                                                                               |\n",
      "|Jquery is my new best friend.                                                                                                               |\n",
      "|Loves twitter                                                                                                                               |\n",
      "|how can you not love Obama? he makes jokes about himself.                                                                                   |\n",
      "|Check this video out -- President Obama at the White House Correspondents' Dinner http://bit.ly/IMXUM                                       |\n",
      "|@Karoli I firmly believe that Obama/Pelosi have ZERO desire to be civil.  It's a charade and a slogan, but they want to destroy conservatism|\n",
      "|House Correspondents dinner was last night whoopi, barbara &amp; sherri went, Obama got a standing ovation                                  |\n",
      "|Watchin Espn..Jus seen this new Nike Commerical with a Puppet Lebron..sh*t was hilarious...LMAO!!!                                          |\n",
      "|dear nike, stop with the flywire. that shit is a waste of science. and ugly. love, @vincentx24x                                             |\n",
      "|#lebron best athlete of our generation, if not all time (basketball related) I don't want to get into inter-sport debates about   __1/2     |\n",
      "|I was talking to this guy last night and he was telling me that he is a die hard Spurs fan.  He also told me that he hates LeBron James.    |\n",
      "|i love lebron. http://bit.ly/PdHur                                                                                                          |\n",
      "|@ludajuice Lebron is a Beast, but I'm still cheering 4 the A..til the end.                                                                  |\n",
      "|@Pmillzz lebron IS THE BOSS                                                                                                                 |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet.select('_c5').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식\n",
    "\n",
    "\n",
    "* I've -> I have\n",
    "* loooooveee -> love\n",
    "* 단어 뒤에 !!! 이런 거 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date_day(a):\n",
    "    x, y = re.split('^([\\d]+-[\\d]+-[\\d]+)', a)[1:]\n",
    "    return [x, y[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20-13-2012', 'monday']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_date_day('20-13-2012-monday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_date_udf = udf(get_date_day, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ">>> df = sc.parallelize([('20-13-2012-monday',), ('20-14-2012-tues',), ('20-13-2012-wed',)]).toDF(['A'])\n",
    ">>> df.show()\n",
    "+-----------------+\n",
    "|                A|\n",
    "+-----------------+\n",
    "|20-13-2012-monday|\n",
    "|  20-14-2012-tues|\n",
    "|   20-13-2012-wed|\n",
    "+-----------------+\n",
    "\n",
    ">>> df = df.withColumn(\"A12\", get_date_udf('A'))\n",
    ">>> df.show(truncate=False)\n",
    "+-----------------+--------------------+\n",
    "|A                |A12                 |\n",
    "+-----------------+--------------------+\n",
    "|20-13-2012-monday|[20-13-2012, monday]|\n",
    "|20-14-2012-tues  |[20-14-2012, tues]  |\n",
    "|20-13-2012-wed   |[20-13-2012, wed]   |\n",
    "+-----------------+--------------------+\n",
    "\n",
    ">>> df = df.withColumn(\"A1\", udf(lambda x:x[0])('A12')).withColumn(\"A2\", udf(lambda x:x[1])('A12'))\n",
    ">>> df = df.drop('A12')\n",
    ">>> df.show(truncate=False)\n",
    "+-----------------+----------+------+\n",
    "|A                |A1        |A2    |\n",
    "+-----------------+----------+------+\n",
    "|20-13-2012-monday|20-13-2012|monday|\n",
    "|20-14-2012-tues  |20-14-2012|tues  |\n",
    "|20-13-2012-wed   |20-13-2012|wed   |\n",
    "+-----------------+----------+------+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweet=tweet.withColumnRenamed(\"_c0\", \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tweet.filter((tweet.label != 2) & (df.bar != 'b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet=tweet.filter(tweet.label != 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    4|  182|\n",
      "|    0|  177|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweet.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(train, val, test) = tweet.randomSplit([0.6, 0.2, 0.2], seed = 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"_c5\", outputCol=\"words\")\n",
    "#tokDf = tokenizer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_401e9ef90eea58d7de36"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stopwords=list()\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")\n",
    "stop.setStopWords([u\"돌\",u\"너\", u\"우리\"])\n",
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)\n",
    "#stopDf=stop.transform(tokDf)\n",
    "#stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "hashtf = HashingTF(numFeatures=2**16, inputCol=\"words\", outputCol='tf')\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\", minDocFreq=5) #minDocFreq: remove sparse terms\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")\n",
    "pipeline = Pipeline(stages=[tokenizer, stop, hashtf, idf])\n",
    "\n",
    "pipelineFit = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+--------------------+------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|label|_c1|                 _c2|   _c3|           _c4|                 _c5|               words|             nostops|                  tf|            features|\n",
      "+-----+---+--------------------+------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|    0|  9|Mon May 11 03:22:...|   aig|       Seth937|Fuck this economy...|[fuck, this, econ...|[fuck, this, econ...|(65536,[7173,1607...|(65536,[7173,1607...|\n",
      "|    0| 14|Mon May 11 03:32:...| obama|   kylesellers|@Karoli I firmly ...|[@karoli, i, firm...|[@karoli, i, firm...|(65536,[8436,9287...|(65536,[8436,9287...|\n",
      "|    0| 17|Mon May 11 05:06:...|  nike|   vincentx24x|dear nike, stop w...|[dear, nike,, sto...|[dear, nike,, sto...|(65536,[2830,5660...|(65536,[2830,5660...|\n",
      "|    0| 19|Mon May 11 05:20:...|lebron|       luv8242|I was talking to ...|[i, was, talking,...|[i, was, talking,...|(65536,[556,2437,...|(65536,[556,2437,...|\n",
      "|    0| 21|Mon May 11 05:21:...|lebron|ursecretdezire|@ludajuice Lebron...|[@ludajuice, lebr...|[@ludajuice, lebr...|(65536,[15889,160...|(65536,[15889,160...|\n",
      "+-----+---+--------------------+------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDf = pipelineFit.transform(train)\n",
    "valDf = pipelineFit.transform(val)\n",
    "trainDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(maxIter=100)\n",
    "lrModel = lr.fit(trainDf)\n",
    "predictions = lrModel.transform(valDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|label|prediction|\n",
      "+-----+----------+\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    0|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "|    4|       4.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.select('label','prediction').show(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4507042253521127"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(val.count())\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
