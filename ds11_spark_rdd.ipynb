{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark RDD\n",
    "\n",
    "* Last updated 20181112MON1130 20171218 20170515 20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* 파일, JSON 등에서 RDD를 생성할 수 있다.\n",
    "* map-reduce 등 RDD API를 사용하여 데이터를 변환하고, 분석할 수 있다.\n",
    "* 텍스트데이터를 단어 빈도, word vector로 변환할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "\n",
    "* S.2 Data를 사용하는 API \n",
    "* S.3 SparkSession\n",
    "* S.3.1 Spark 2.0 vs 1.6 \n",
    "* S.3.2 SparkContext\n",
    "* S.3.3 IPython Notebook에서 SparkSession 생성하기\n",
    "* S.4 RDD 소개\n",
    "* S.5 RDD 생성\n",
    "* S.6 RDD API \n",
    "* S.6.1 비슷한 Python 함수\n",
    "* S.6.2 RDD 사용하기\n",
    "* 5.6.3 Pair RDD\n",
    "* S.7 spark-submit\n",
    "\n",
    "### S.1.3 문제\n",
    "\n",
    "* 문제 S-1: Hello Spark - 환경설정을 읽어 클라이언트를 생성하기.\n",
    "* 문제 S-2: RDD를 사용하여 word count를 계산하고 그래프 그리기.\n",
    "* 문제 S-3: 성적 합계 및 평균.\n",
    "* 문제 S-4: 서울시 지하철호선별 승차인원 평균 구하기.\n",
    "* 문제 S-3: RDD를 사용하여 word vector를 생성하기."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 클라이언트 생성\n",
    "\n",
    "### S.3.1 SparkSession\n",
    "\n",
    "#### Spark 2.x\n",
    "\n",
    "**Spark 2.0부터는 SparkSession을 시작점**으로 개별적인 Context를 모두 **통합**해서 사용한다.\n",
    "SparkSession은 빅데이터를 처리하기 위해 구성된 **Spark 클러스터에 대한 클라이언트**와 같은 역할을 하며, 이를 통해 모든 기능을 사용한다.\n",
    "따라서 클러스터 매니저로부터 에서 CPU, 메모리, 하드웨어 등 시스템 자원을 배분받아서 필요한 작업을 수행하게 된다.\n",
    "\n",
    "#### 이전 Spark 1.x\n",
    "참고로 **Spark 2.0 이전에는 SparkContext를 먼저 만들고** 이를 통해 다른 Context를 사용했다. 아래에서 보듯이 Spark 1.x에서는 **SparkContext를 직접 생성**하고, **이를 통해 SQLContext, HiveContext도 생성**하여 사용했다. 이런 방식은 2.x에서도 호환되므로 그대로 사용할 수도 있지만, **SparkSession으로 통합**되었다.\n",
    "\n",
    "```python\n",
    "# 버전 1.x\n",
    "import pyspark\n",
    "conf=pyspark.SparkConf()\n",
    "conf = pyspark.SparkConf().setAppName(\"myAppName\")\n",
    "sc = pyspark.SparkContext(conf=conf)  #SparkContext를 직접 생성한다.\n",
    "sqlContext = SQLContext(sc)           #SparkContext를 넣어서 SQLContext를 생성\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Context는 아래와 같이 Spark 서버를 사용하기 위한 클라이언트라고 생각하자.\n",
    "\n",
    "Context 구분 | 설명 | 2.0의 사용 예 | 1.x의 사용 예\n",
    "----------|----------|----------|----------\n",
    "SparkContext | RDD를 사용하는 Context | spark.SparkContext | SparkContext()\n",
    "StreamingContext | 향후 제공 | |\n",
    "SQLContext | Spark SQL, DataFrame | spark.sql | SQLContext(SparkContext)\n",
    "HiveContext | HiveQL, DataFrame | spark.sql | HiveContext(SparkContext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.3.2 IPython Notebook에서 SparkSession 생성하기\n",
    "\n",
    "* Spark에서 pyspark shell을 제공하고 있다.\n",
    "* 이 강의에서는 ipython notebook에서 pyspark를 사용하기로 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: Hello Spark, 환경설정을 읽어 클라이언트 생성하기.\n",
    "\n",
    "Spark를 별도로 설치하지 않고 Databricks를 사용할 수도 있다 (https://community.cloud.databricks.com/) 이 사이트에서는 SparkSession을 미리 생성해 주어서 굳이 SparkSession을 생성하지 않아도 된다. 또는 PySpark를 사용해도 마찬가지 이다. 필요한 SparkSession을 이미 만들어 주어서 번거롭게 만들어 사용하지 않아도 된다.\n",
    "여기서는 Notebook에서 사용하기 위해 SparkSession을 생성하기로 한다.\n",
    "\n",
    "### 경로 및 라이브러리 추가\n",
    "\n",
    "Spark가 설치된 디렉토리를 SPARK_HOME으로 설정하고, 또한 실행에 필요한 라이브러리를 추가한다.\n",
    "보통 라이브러리 경로는 디렉토리를 지정하게 되지만, **zip**파일도 경로로 추가할 수 있다.\n",
    "\n",
    "설정 항목 | 설명\n",
    "----------|----------\n",
    "SPARK_HOME | Spark가 설치된 경로이다.\n",
    "PYTHONPATH | sys.path.insert()를 사용하여 'PYTHONPATH'를 수정한다. pyspark.zip, py4j-0.10.x-src.zip를 추가 (버전은 수정해야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "#home=os.path.expanduser(\"~\") # HOME이 설정되어 있지 않으면 expanduser('~')를 사용한다.\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Python 라이브러리 경로가 방금 추가한 두 개의 zip파일이 잡혀있는 것을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/pyspark.zip\n",
      "/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/python/lib/py4j-0.10.1-src.zip\n",
      "\n",
      "/usr/lib/python2.7\n",
      "/usr/lib/python2.7/plat-x86_64-linux-gnu\n",
      "/usr/lib/python2.7/lib-tk\n",
      "/usr/lib/python2.7/lib-old\n",
      "/usr/lib/python2.7/lib-dynload\n",
      "/home/jsl/.local/lib/python2.7/site-packages\n",
      "/usr/local/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages\n",
      "/usr/lib/python2.7/dist-packages/PILcompat\n",
      "/usr/lib/python2.7/dist-packages/gtk-2.0\n",
      "/usr/lib/python2.7/dist-packages/ubuntu-sso-client\n",
      "/usr/local/lib/python2.7/dist-packages/IPython/extensions\n",
      "/home/jsl/.ipython\n"
     ]
    }
   ],
   "source": [
    "for i in sys.path:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SparkSession 생성\n",
    "\n",
    "SparkSession을 생성해 보자. SparkSesion은 sql 모듈로 **'pyspark.sql.SparkSession'**을 클라이언트로 사용한다.\n",
    "SparkSession은 **builder.getOrCreate()** 함수를 호출하여, 기존의 session 또는 새로 생성하여 사용한다. 함수 **getOrCreate()** 함수는 **singleton 패턴**으로 한 번에 하나의 세션만이 존재하도록 한다.\n",
    "SparkSession을 종료하려면 stop() 함수를 호출한다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()\n",
    "spark.stop()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### spark-warehouse\n",
    "\n",
    "spark-warehouse는 내부적으로 사용되는 디렉토리로서, Derby 데이터베이스 관련 파일을 저장하게 된다.\n",
    "이 디렉토리는 Spark를 실행하면 작업 디렉토리 아래 자동으로 생성된다.\n",
    "또는 ```/conf/hive-site.xml``` 파일에 spark.sql.warehouse.dir를 설정해도 된다.\n",
    "또는 실행하면서 추가할 수도 있다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"C:/Users/jsl/myTemp\")\\\n",
    "    .getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "spark-warehouse 디렉토리는 conf.get() 함수로 알아볼 수 있다. 다음에서 보듯이, 현재 작업디렉토리 밑에 spark-warehous가 만들어졌다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'file:${system:user.dir}/spark-warehouse'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.get('spark.sql.warehouse.dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "몇 가지 설정을 확인해 보자.\n",
    "설치디렉토리 아래 ```conf/```에 있는 spark-defaults.conf와 같은 설정파일에 자신에게 필요한 설정을 해 놓으면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n",
      "pyspark-shell\n",
      "local[*]\n",
      "117.16.44.45\n"
     ]
    }
   ],
   "source": [
    "print spark.version\n",
    "print spark.conf.get('spark.app.name')\n",
    "print spark.conf.get('spark.master')\n",
    "print spark.conf.get('spark.driver.host')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Spark를 실행하면서 라이브러리를 설정해야 할 필요도 생겨나게 된다. 다음은 mongo, graphframes, csv 등의 라이브러리가 설정되어 있는 내용을 보여주고 있다. 라이브러리는 https://spark-packages.org 를 방문해서 찾아서 사용하면 된다. 실제로 설치하지 않고, 명명규칙에 따라, maven에서 하는 것과 같이, 콜론으로 구분해, 패키지명과 라이브러리를 버전정보와 같이 적어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphframes:graphframes:0.4.0-spark2.0-s_2.11,org.mongodb.spark:mongo-spark-connector_2.10:2.0.0,com.databricks:spark-csv_2.11:1.5.0\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 데이터 구조\n",
    "\n",
    "Spark에서는 RDD, Dataframe, DataSet와 같은 세 가지 데이터구조를 제공하고 있다. **RDD**는 Spark 1.0부터 사용되었고, 이를 기반으로 다른 데이터구조가 만들어졌다. 다음으로 **Dataframe**은 버전 1.3에서 제공되어 많이 쓰이고 있다. 마지막으로 **DataSet**은 그 후 1.6부터 제공하고 있다. 이 가운데 **RDD API에 대한 지원은 축소**되고 있다.\n",
    "\n",
    "데이터구조 | 도입된 spark version | 설명\n",
    "---------|---------|---------\n",
    "**RDD** | Spark 1.0 | **비구조적**, schema free, low-level\n",
    "**Dataframe** | 1.3 | **구조적**, schema를 가진다. Dataset[Row]와 같은 의미로, 타잎을 강제하지 않는다.\n",
    "**Dataset** | 1.6 | 자바의 Generic과 같이 Dataset[T]으로 '타잎'을 강제하는 형식이다. Scala and Java에서 사용한다. Python loosely-typed이므로 사용하지 않는다.\n",
    "\n",
    "\n",
    "* RDD는 데이터가 **비구조적**인 경우 사용하기 적합하다. 모델schema를 정하지 않고 사용할 수 있다. 반면에 DataFrame과 DataSet은 데이터가 schema와 데이터타잎을 가진 **구조적**인 경우 사용한다.\n",
    "* Spark의 RDD, DataFrame 모두 immutable이라 일단 생성되고 나면 원본을 수정할 수 없다.\n",
    "* Spark의 데이터는 모두 **lazy**, 실제 transformation을 action까지 연기한다. **변환할 때마다 실제 변환이 일어나면 그 결과가 메모리에 저장되는 비효율성**을 막기 위해, **action이 실행되는 경우, 계산이 이루어지고, 실제 메모리를 사용**한다. RDD의 경우, action이 실행될 때마다 재계산이 이루어지는 것을 막기 위해 persist (or cache)함수를 사용할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 RDD 소개\n",
    "\n",
    "RDD (Resilient Distributed Dataset)는 그 줄임말에서 알 수 있듯이 **데이터를 저장하고 있는 DataSet**이며, 여러 컴퓨터에 **분산**해서 사용해서 사용할 수 있다는 점이 특징이다.\n",
    "* Resilient - 작업이 실패하지 않도록 fault tolerent, 즉 어느 한 노드에서 작업이 실패하면 다른 노드에서 실행\n",
    "* Distributed - 클라스터로 구성된 여러 노드에 분산해서 처리\n",
    "* Dataset - 데이터 구조\n",
    "\n",
    "RDD는 Python List, 파일, hdfs 등 다양한 자료에서 생성할 수 있고, 생성된 자료는 수정할 수 없는 read-only이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 RDD 생성\n",
    "\n",
    "RDD는 **sparkContext**로부터 만들어 진다. 1) 외부 파일 또는 2) 배열과 같은 자료구조에서 읽어서 생성한다.\n",
    "\n",
    "생성 방법 | 설명 | 함수\n",
    "----------|----------|----------\n",
    "외부에서 읽기 | 파일, HDFS, HBase 등 | ```textFile(\"mydir/\")```<br>```textFile(\"mydir/*.txt\")```<br>```textFile(\"mydir/*.gz\")```<br>```Hadoop InputFormat```\n",
    "내부에서 읽기 | Pytho list에서 생성 | parallelize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### List에서 RDD 생성하기\n",
    "\n",
    "Python list에서 RDD를 생성할 수 있다.\n",
    "sparkcontext.parallelize()함수를 사용하여 list로부터 RDD를 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일에서 RDD 생성하기\n",
    "\n",
    "파일에서 직접 RDD를 생성해 본다.\n",
    "현재 작업 디렉토리 아래에 **'data/' 디렉토리**를 만들고 아래 파일을 생성한다.\n",
    "파일 내용은 wikipedia에서 Apache spark를 검색한 후 첫 문단을 복사해서 가져 왔다.\n",
    "일부러 3째줄은 한글, 4째 줄은 같은 단어를 반복해서 추가했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Wikipedia'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing data/ds_bigdata_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_bigdata_wiki.txt\n",
    "활용사례 및 의의[편집]\n",
    "정치 및 사회[편집]\n",
    "2008년 미국 대통령 선거[편집]\n",
    "2008년 미국 대통령 선거에서 버락 오바마 미국 대통령 후보는 다양한 형태의 유권자 데이터베이스를 확보하여 이를 분석, 활용한 '유권자 맞춤형 선거 전략'을 전개했다. 당시 오바마 캠프는 인종, 종교, 나이, 가구형태, 소비수준과 같은 기본 인적 사항으로 유권자를 분류하는 것을 넘어서서 과거 투표 여부, 구독하는 잡지, 마시는 음료 등 유권자 성향까지 전화나 개별 방문을 또는 소셜 미디어를 통해 유권자 정보를 수집하였다. 수집된 데이터는 오바마 캠프 본부로 전송되어 유권자 데이터베이스를 온라인으로 통합관리하는 ‘보트빌더(VoteBuilder.com)’시스템의 도움으로 유권자 성향 분석, 미결정 유권자 선별 , 유권자에 대한 예측을 해나갔다. 이를 바탕으로‘유권자 지도’를 작성한 뒤 ‘유권자 맞춤형 선거 전략’을 전개하는 등 오바마 캠프는 비용 대비 효과적인 선거를 치를 수 있었다.\n",
    "\n",
    "대한민국 제19대 총선[편집]\n",
    "중앙선거관리위원회는 대한민국 제19대 총선부터 소셜 네트워크 등 인터넷 상의 선거 운동을 상시 허용하였다.[15] 이에 소셜 미디어 상에서 선거 관련 데이터는 증폭되었으며, 2010년 대한민국 제5회 지방 선거 및 2011년 대한민국 재보궐선거에서 소셜 네트워크 서비스의 중요성을 확인한 정당들 또한 SNS 역량 지수를 공천 심사에 반영하는 등[16] 소셜 네트워크 활용에 주목했다. 이 가운데 여론 조사 기관들은 기존 여론조사 방식으로 예측한 2010년 제5회 지방 선거 및 2011년 재보궐선거의 여론조사 결과와 실제 투표 결과와의 큰 차이를 보완하고자 빅 데이터 기술을 활용한 SNS 여론 분석을 시행했다. 그러나 SNS 이용자의 대다수가 수도권 20~30대에 쏠려 있기에[17], 빅 데이터를 이용한 대한민국 제19대 총선에 대한 SNS 분석은 수도권으로 한정되어 일치하는 한계를 드러내기도 하였다.\n",
    "\n",
    "경제 및 경영[편집]\n",
    "아마존닷컴의 추천 상품 표시 / 구글 및 페이스북의 맞춤형 광고[편집]\n",
    "아마존닷컴은 모든 고객들의 구매 내역을 데이터베이스에 기록하고, 이 기록을 분석해 소비자의 소비 취향과 관심사를 파악한다.[18] 이런 빅 데이터의 활용을 통해 아마존은 고객별로 '추천 상품(레코멘데이션)'을 표시한다. 고객 한사람 한사람의 취미나 독서 경향을 찾아 그와 일치한다고 생각되는 상품을 메일, 홈 페이지상에서 중점적으로 고객 한사람 한사람에게 자동적으로 제시하는 것이다.[19] 아마존닷컴의 추천 상품 표시와 같은 방식으로 구글 및 페이스북도 이용자의 검색 조건, 나아가 사진과 동영상 같은 비정형 데이터 사용을 즉각 처리하여 이용자에게 맞춤형 광고를 제공하는 등 빅데이터의 활용을 증대시키고 있다.\n",
    "\n",
    "문화[편집]\n",
    "MLB (메이저 리그 베이스볼)의 머니볼 이론 및 데이터 야구[편집]\n",
    "머니볼 이론이란 경기 데이터를 철저하게 분석해 오직 데이터를 기반으로 적재적소에 선수들을 배치해 승률을 높인다는 게임 이론이다.[20] 이는 미국 메이저 리그 베이스볼 오클랜드 어슬레틱스의 구단장 빌리 빈이 리그 전체 25위에 해당하는 낮은 구단 지원금 속에서도 최소비용으로 최대효과를 거둔 상황에서 유래되었다. 빌리 빈은 하치해 최하위에 그치던 팀을 4년 연속 포스트시즌에 진출시키고 메이저 리그 최초로 20연승이라는 신기록을 세우도록 탈바꿈 시켰다. 미국 월스트리트 저널은 미국 경제에 큰 영향을 끼치는 파워 엘리트 30인에 워렌 버핏, 앨런 그린스펀과 함께 빌리 빈을 선정[21] 하는 등 머니볼 이론은 경영, 금융 분야에서도 주목받았다. 최근 들어서 과학기술 및 카메라 기술의 발달로 더욱 정교한 데이터의 수집이 가능해졌으며 투구의 궤적 및 투수의 그립, 타구 방향, 야수의 움직임까지 잡아낼 수 있게 되었다. 이처럼 기존의 정형 데이터뿐만 아닌 비정형 데이터의 수집과 분석, 활용을 통해 최근 야구경기에서 빅 데이터의 중요성은 더욱 커지고 있다.\n",
    "\n",
    "선수의 인기만을 쫓는 것이 아니라 팀별 승률이나 선수의 성적을 나타내는 수치와 야구를 관전한다면 그 재미는 배가된다. '출루율'은 타율로 인정되지 않는 볼넷을 포함하여 타자가 성공적으로 베이스를 밟은 횟수의 비율, '장타율'은 타수마다 밟은 총 베이스를 계산해서 타격력이 얼마나 강한지를 나타내는 비율이다.\n",
    "\n",
    "출루율과 장타율 못지 않게 '타수'는 한두 경기에서 낸 성적이 아닌, 수천 번의 타석에 들어 좋은 성적을 만들어낸 선수를 선별하기 위한 기초 통계자료이다. 이처럼 한 선수의 타율에서 팀의 역대 시리즈 전적까지 모든 것을 숫자로 표현할 수 있다고 해서 야구를 '통계의 스포츠'라고 부르기도 한다. 야구뿐만 아니라 생활 곳곳에서 활용되는 통계는 복잡한 상황과 설명을 간단한 숫자로 바꿔주는 매우 강력한 도구이다.[22]\n",
    "\n",
    "'프로파일링'과 '빅데이터' 기법을 활용한 프로그램 MBC <프로파일링>[편집]\n",
    "방송에는 19세 소년의 살인 심리를 파헤친 '용인살인사건의 재구성', 강남 3구 초등학교 85곳의 학업성취도평가 성적과 주변 아파트 매매가의 상관관계를 빅데이터(디지털 환경에서 발생한 방대한 규모의 데이터)를 통해 분석한 '강남, 부자일수록 공부를 잘할까'[23]\n",
    "\n",
    "2014년 FIFA 월드컵 독일 우승과 '빅데이터'[편집]\n",
    "브라질에서 개최된 2014년 FIFA 월드컵에서 독일은 준결승에서 개최국인 브라질을 7:1로 꺾고, 결승에서 아르헨티나와 연장전까지 가는 접전 끝에 1:0으로 승리를 거두었다. 무패행진으로 우승을 차지한 독일 국가대표팀의 우승의 배경에는 '빅데이터'가 있었다.\n",
    "\n",
    "독일 국가대표팀은 SAP와 협업하여 훈련과 실전 경기에 'SAP 매치 인사이트'를 도입했다. SAP 매치 인사이트란 선수들에게 부착된 센서를 통해 운동량, 순간속도, 심박수, 슈팅동작 등 방대한 비정형 데이터를 수집, 분석한 결과를 감독과 코치의 태블릿PC로 전송하여 그들이 데이터를 기반으로 전술을 짜도록 도와주는 솔루션이다. 기존에 감독의 경험이나 주관적 판단으로 결정되는 전략과는 달리, SAP 매치 인사이트를 통해 이루어지는 분석은 선수들에 대한 분석 뿐만 아니라 상대팀 전력, 강점, 약점 등 종합적인 분석을 통해 좀 더 과학적인 전략을 수립할 수 있다. 정보 수집에 쓰이는 센서 1개가 1분에 만들어내는 데이터는 총 12000여개로 독일 국가대표팀은 선수당 4개(골키퍼는 양 손목을 포함해 6개)의 센서를 부착했고, 90분 경기동안 한 선수당 약 432만개, 팀 전체로 약 4968만개의 데이터를 수집했다고 한다.월드컵8강 獨 전차군단 비밀병기는 '빅데이터'\n",
    "\n",
    "과학기술 및 활용[편집]\n",
    "통계학[편집]\n",
    "데이터 마이닝이란 기존 데이터베이스 관리도구의 데이터 수집, 저장, 관리, 분석의 역량을 넘어서는 대량의 정형 또는 비정형 데이터 집합 및 이러한 데이터로부터 가치를 추출하고 결과를 분석하는 기술로, 수집되는 ‘빅 데이터’를 보완하고 마케팅, 시청률조사, 경영 등으로부터 체계화돼 분류, 예측, 연관분석 등의 데이터 마이닝을 거쳐 통계학적으로 결과를 도출해 내고 있다.[24][25]\n",
    "\n",
    "대한민국에서는 2000년부터 정보통신부의 산하단체로 사단법인 한국BI데이터마이닝학회가 설립되어 데이터 마이닝에 관한 학술과 기술을 발전, 보급, 응용하고 있다. ‎또한 국내ㆍ외 통계분야에서 서서히 빅 데이터 활용에 대한 관심과 필요성이 커지고 있는 가운데 국가통계 업무를 계획하고 방대한 통계자료를 처리하는 국가기관인 통계청이 빅 데이터를 연구하고 활용방안을 모색하기 위한 '빅 데이터 연구회'를 발족하였다.[26] 하지만 업계에 따르면, 미국과 영국, 일본 등 선진국들은 이미 빅 데이터를 다각적으로 분석해 조직의 전략방향을 제시하는 데이터과학자 양성에 사활을 걸고 있다. 그러나 한국은 정부와 일부 기업이 데이터과학자 양성을 위한 프로그램을 진행 중에 있어 아직 걸음마 단계인 것으로 알려져 있다.[27]\n",
    "\n",
    "생물정보학[편집]\n",
    "최근 생물학에서 DNA, RNA, 단백질 서열 및 유전자들의 발현과 조절에 대한 데이터의 양이 급격히 증가했고 이에 따라 이 빅 데이터를 활용한 생명의 이해에 관한 논의가 진행되고 있다.\n",
    "\n",
    "보건의료[편집]\n",
    "국민건강보험공단은 가입자의 자격·보험료, 진료·투약내용, 건강검진 결과 및 생활습관 정보 등 2조1천억건, 92테라바이트의 빅데이터를 보유하고 있고, 한편, 건강보험심사평가원은 진료내역, 투약내용(의약품 안심서비스), 의약품 유통 등의 2조2천억건, 89테라바이트의 빅데이터를 보유하고 있으며, 경제협력개발기구(OECD)는 한국의 건강보험 빅데이터 순위가 2위라고 발표했었다. 건보공단과 심평원은 빅데이터를 민간에 널리 알리고 더 많이 개방하고 있다. (연합뉴스 2016.6.14 인터넷뉴스 참조)\n",
    "\n",
    "빅 데이터를 활용하면 미국 의료부문은 연간 3,300 억 달러(미 정부 의료 예산의 약 8%에 해당하는 규모)의 직간접적인 비용 절감 효과를 보일 것으로 전망된다.[28] 특히 임상분야에서는 의료기관 별 진료방법, 효능, 비용 데이터를 분석하여 보다 효과적인 진료방법을 파악하고 환자 데이터를 온라인 플랫폼화하여 의료협회 간 데이터 공유로 치료 효과를 제고하며 공중보건 영역에선 전국의 의료 데이터를 연계하여 전염병 발생과 같은 긴박한 순간에 빠른 의사결정을 가능케 할 전망이다.[29]\n",
    "\n",
    "한편, 의료 분야에서 빅 데이터가 효과를 발휘하기 위해서는 대량의 의료정보 수집이 필수적이기 때문에, 개인정보의 보호와 빅 데이터 활용이라는 두 가지 가치가 상충하게 되된다. 따라서, 의료 분야에서 빅 데이터의 활용과 보급을 위해서는 이러한 문제에 대한 가이드라인 마련이 필요한 상태이다.[30]\n",
    "\n",
    "기업 경영[편집]\n",
    "대규모의 다양한 데이터를 활용한 '빅데이터 경영'이 주목받으면서 데이터 품질을 높이고 방대한 데이터의 처리를 돕는 데이터 통합(Data Integration)의 중요성이 부각되고 있다.\n",
    "\n",
    "데이터 통합(DI)은 데이터의 추출, 변환, 적재를 위한 ETL 솔루션이 핵심인데 ETL 솔루션을 활용하면 일일이 수많은 데이터를 기업 데이터 포맷으로 코딩하지 않아도 되고 데이터 품질을 제고할 수 있기 때문에 DI는 빅데이터 환경에 꼭 필요한 데이터 솔루션으로 평가받고 있는 단계까지 진입되었다.\n",
    "\n",
    "한편 비즈니스 인텔리전스(Business Intelligence, BI)보다 진일보한 빅데이터 분석 방법이 비즈니스 애널리틱스(Business analytics, BA)인데 고급분석 범주에 있는 BA는 기본적으로 BI를 포함하면서도 미래 예측 기능과 통계분석, 확률 분석 등을 포함해 최적의 데이터 기반 의사결정을 가능케 하는 것으로 평가받고 있기도 하다.[31]\n",
    "\n",
    "마케팅[편집]\n",
    "인터넷으로 시작해서 인터넷으로 마감하는 생활, 스마트폰을 이용해 정보를 검색하고 쇼핑도하고 SNS를 이용해서 실시간으로 글을 남기는 등의 다양하게 인터넷을 이용하는 동안 남는 흔적같은 모인 데이터들을 분석하면 개인의 생활 패턴, 소비성향 등을 예측할 수 있고 기업들은 이런 데이터를 통해서 소비자가 원하는 것들을 미리 예측할 수 있다. 빅 데이터가 마케팅 자료로 활용되는 사례이다.[31]\n",
    "\n",
    "기상정보[편집]\n",
    "한반도 전역의 기상관측정보를 활용해 일기예보와 각종 기상특보 등 국가 기상서비스를 제공하고 있는 기상청은 정밀한 기상예측을 위한 분석 과정에서 발생하는 데이터 폭증에 대응하고자 빅데이터 저장시스템의 도입을 추진하였다.\n",
    "\n",
    "대다수 스토리지 기업들의 솔루션을 검토한 끝에 한국 IBM의 고성능 대용량 파일공유시스템(General Parallel File System, 이하 GPFS)을 적용한 스토리지 시스템을 선택하였다고 밝혔다.\n",
    "\n",
    "한국IBM이 기상청에 제공한 GPFS 기반의 빅데이터 저장시스템은 IBM 시스템 스토리지 제품군, 시스템 x서버 제품군과 고속 네트워킹 랙스위치(RackSwitch) 등이 통합돼 있는 시스템이다.[31]\n",
    "\n",
    "보안관리[편집]\n",
    "보안관리는 빅데이터 환경을 이용해 성장과 기술 발전을 동시에 이루는 분야로 분리한다. 클라우드 및 모바일 환경으로 접어들면서 물리/가상화 IT 시스템의 복잡성이 더욱 높아지고 있어 유무선 네트워크, 프라이빗/퍼블릭 클라우드, 모바일 애플리케이션과 기기관리 등 IT 시스템 전반에서 대대적인 변화가 예상되고 있어 막대한 양의 보안관리가 중요한 요소로 현실화되고 있다.[32]\n",
    "\n",
    "구글 번역[편집]\n",
    "구글에서 제공하는 자동 번역 서비스인 구글 번역은 빅 데이터를 활용한다. 지난 40년 간 컴퓨터 회사 IBM의 자동 번역 프로그램 개발은 컴퓨터가 명사, 형용사, 동사 등 단어와 어문의 문법적 구조를 인식하여 번역하는 방식으로 이뤄졌다. 이와 달리 2006년 구글은 수억 건의 문장과 번역문을 데이터베이스화하여 번역시 유사한 문장과 어구를 기존에 축적된 데이터를 바탕으로 추론해 나가는 통계적 기법을 개발하였다. 캐나다 의회의 수백만 건의 문서를 활용하여 영어-불어 자동번역 시스템개발을 시도한 IBM의 자동 번역 프로그램은 실패한 반면 구글은 수억 건의 자료를 활용하여 전 세계 58개 언어 간의 자동번역 프로그램 개발에 성공하였다. 이러한 사례로 미루어 볼 때, 데이터 양의 측면에서의 엄청난 차이가 두 기업의 자동 번역 프로그램의 번역의 질과 정확도를 결정했으며, 나아가 프로젝트의 성패를 좌우했다고 볼 수 있다.[31]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRdd3=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_bigdata_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "활용사례 및 의의[편집]\n",
      "정치 및 사회[편집]\n"
     ]
    }
   ],
   "source": [
    "for i in myRdd3.take(2):\n",
    "    print i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RDD와 Spark Dataframe를 만드는 함수는 서로 다르다\n",
    "\n",
    "다음에 배우겠지만, file에서 읽는 방식이 RDD와 Dataframe이 서로 다르다.\n",
    "RDD는 sparkContext.textFile(), Dataframe은 read.text()을 사용한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "SparkSession.sparkContext.textFile() | **'SparkContext'를 사용하므로 RDD를 생성**한다.\n",
    "SparkSession.read.text() | **DataFrame을 생성**한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(value=u'Wikipedia')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf=spark.read.text(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "print myDf.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print type(myDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### csv에서 RDD 생성하기\n",
    "\n",
    "csv 파일을 읽어서 RDD를 생성해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 파일에서 읽어 생성하면, 전체가 하나의 리스트가 된다.\n",
    "아래에서 보듯이 파일의 각 라인이 묶여서 리스트의 한 요소로 만들어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'35, 2', u'40, 27', u'12, 38', u'15, 31', u'21, 1']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))\n",
    "myRdd4.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2차원 리스트로 만드려면, map()함수를 사용한다.\n",
    "```python\n",
    "모든 줄을 반복:\n",
    "    한 줄line을 읽는다.\n",
    "    줄line을 컴마(,)로 분리한다.\n",
    "    줄line을 리스트로 만든다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'35', u' 2'],\n",
       " [u'40', u' 27'],\n",
       " [u'12', u' 38'],\n",
       " [u'15', u' 31'],\n",
       " [u'21', u' 1']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd5 = myRdd4.map(lambda line: line.split(','))\n",
    "myRdd5.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 RDD API\n",
    "\n",
    "앞서 RDD를 생성하여 보았다.\n",
    "이제는 RDD라는 데이터구조에서 데이터를 읽고 변환하고 분석하여 보자.\n",
    "RDD는 **데이터 변환Transformations**, **연산Actions**으로 구분할 수 있다.\n",
    "다음에 배우게 될 **Dataframe**의 **Transformer**, **Estimator**와 비교될 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 변환 **transformations**\n",
    "\n",
    "**lazy연산**을 한다. 즉, 실제 변환은 action이 수행되는 시점까지 늦추어져서 이루어진다.\n",
    "변환 결과는 RDD 또는 seq(RDD)로 만들어진다.\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "map(fn) | 요소별로 fn을 적용해서 결과 RDD 돌려줌 | .map(lambda x: x.split(' ')\n",
    "filter(fn) | 요소별로 선별하여 fn을 적용해서 결과 RDD 돌려줌 | .filter(lambda x: \"Spark\" in x)\n",
    "flatMap(fn) | 요소별로 fn을 적용하고, flat해서 결과 RDD 돌려줌 | .flatMap(lambda x: x.split(' '))\n",
    "groupByKey() | key를 그룹해서 iterator를 돌려줌. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### **actions**\n",
    "\n",
    "* RDD를 값으로 변환한다. 예, Python list\n",
    "\n",
    "함수 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "reduce(fn) | 요소별로 fn을 사용해서 줄여서 결과 list를 돌려줌 |\n",
    "collect() | 모든 요소를 결과 list로 돌려줌 |\n",
    "count() | 요소의 갯수를 결과 list로 돌려줌 |\n",
    "countByKey() | key별 함수 |\n",
    "foreach(fn) | 각 데이터 항목에 함수fn을 적용 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  S.6.1 우선 Python 함수로 해보기\n",
    "\n",
    "#### map()함수를 사용하지 않고 해보기\n",
    "\n",
    "map()을 사용하지 않고 섭씨를 화씨로 변환하는 **c2f**함수를 만들어 보자.\n",
    "* 데이터를 하나씩 읽어 **for문으로 처리하고, list로 만들어 반환**한다.\n",
    "* 곧 이어 다음에 **map()함수를 사용하면 for문이 없어지게** 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    f=list()\n",
    "    for i in c:\n",
    "        _f=(float(9)/5)*i + 32\n",
    "        f.append(_f)\n",
    "    return f\n",
    "\n",
    "print c2f(celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### map() 함수를 사용해 보기\n",
    "\n",
    "Python은 map(), reduce(), filter() 함수를 이미 가지고 있다.\n",
    "Python map()을 사용해 보자. **for문이 없어진다**는 점에 유의한다.\n",
    "**map() 함수의 인자는 2개** 이다.\n",
    "* (1) 첫째 인자는 **처리 함수**이고, **함수의 return은 반드시 있어야** 한다,\n",
    "* (2) 두 번째 인자는 **처리하려는 데이터**이다.\n",
    "\n",
    "함수 | 설명 | 예\n",
    "-------|-------|-------\n",
    "map() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **map(fn,data)**\n",
    "filter() | 각 데이터 요소에 함수의 결과 True를 선택해서 반환 | **filter(fn, data)**\n",
    "reduce() | 각 데이터 요소에 함수를 적용해서 list를 반환 | **reduce(fn, data)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102.56, 97.7, 99.14, 100.03999999999999]\n"
     ]
    }
   ],
   "source": [
    "celsius = [39.2, 36.5, 37.3, 37.8]\n",
    "def c2f(c):\n",
    "    return (float(9)/5)*c + 32\n",
    "\n",
    "f=map(c2f, celsius)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**lambda함수**를 사용하면 줄 수가 많이 줄게 된다. lambda는 무명 함수이므로 함수 선언이 별도 필요 없다. 처리 결과는 **'return'을 사용하지 않아도 반환**된다.\n",
    "\n",
    "* lambda는 함수를 정의하는 명령어로서, 이름이 없다는 특징이 있다. 매우 직관적으로 사용할 수 있게 된다.\n",
    "* map-reduce 함수에서 자주 사용되므로 잘 이해해야 한다.\n",
    "* 단순히 인자에 2를 곱하는 lamdba함수 'y'를 정의하면 다음과 같다. 같은 기능을 lambda를 사용하지 않고, 기존 함수 방식으로 만들면 프로그램이 조금 더 늘어나게 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    return x*2\n",
    "y=f(1)\n",
    "print y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "y=lambda x:x*2\n",
    "print y(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[102.56, 97.7, 99.14, 100.03999999999999]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(lambda c:(float(9)/5)*c + 32, celsius)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "문자열에 map()을 사용해 보자\n",
    "* 문자열 \"Hello World\"을 사용하면, 각 철자를 map()함수의 인자로 처리해서 split()한다.\n",
    "* list [\"Hello World\"]를 사용하면, 각 단어를 map()함수의 인자로 처리해서 split()한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['H'], ['e'], ['l'], ['l'], ['o'], [], ['W'], ['o'], ['r'], ['l'], ['d']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Hello World\"\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hello', 'World']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = [\"Hello World\"]\n",
    "map(lambda x:x.split(),sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### filter()\n",
    "\n",
    "filter()는 데이터를 선별한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 3, 5, 13, 21, 55]\n"
     ]
    }
   ],
   "source": [
    "fib = [0,1,1,2,3,5,8,13,21,34,55]\n",
    "result = filter(lambda x: x % 2, fib)\n",
    "print result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### reduce()\n",
    "\n",
    "reduce() 역시 **함수와 데이터 2개의 인자**를 받는다.\n",
    "데이터에 대해 함수를 반복적으로 적용하여 결과 값을 만들게 된다, 즉 [ func(func(s1, s2),s3), ... , sn ]와 같이 수행한다. 아래 예는 1부터 101까지 **두 수 x,y 인자를 반복해서 더한다**는 것이다.\n",
    "\n",
    "단계 | x | y | 함수적용\n",
    "-----|-----|-----|-----\n",
    "1 | 1 | 2 | func(1,2)\n",
    "2 | func(1,2) | 3 | func(func(1,2),3)\n",
    "3 | func(func(1,2),3) | 4 | func(func(func(1,2),3),4)\n",
    "...|...|...|...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x+y, range(1,101))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6.2 RDD 사용하기\n",
    "\n",
    "* transformation, action을 사용한다.\n",
    "* lambda함수를 또는 사용자함수를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### map\n",
    "\n",
    "map()을 사용해서 **제곱**해 보자. map()은 변환함수라서, 아직 실제 값은 안 만들어져 있고 반환 값은 RDD이다.\n",
    "결과를 보려면 collect()를 사용해서 출력해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[1] at RDD at PythonRDD.scala:48\n"
     ]
    }
   ],
   "source": [
    "nRdd = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "squared = nRdd.map(lambda x: x * x)\n",
    "print squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "print squared.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### reduce\n",
    "\n",
    "앞서 RDD를 사용하지 않고, Python reduce()한 예와 비교해 보자.\n",
    "reduce()는 lamdba함수를 사용해서 **입력 데이터 2개씩을 서로 더해서 x+y** 결과 값을 만들어 낸다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5050"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd100 = spark.sparkContext.parallelize(range(1,101))\n",
    "myRdd100.reduce(lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단순 통계 기능\n",
    "\n",
    "텍스트데이터와 달리 정량데이터로부터 sum, min, max, 표준편차 등 서술통계를 계산해 낼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 1 4 1.11803398875 1.25\n"
     ]
    }
   ],
   "source": [
    "print nRdd.sum(), nRdd.min(), nRdd.max(), nRdd.stdev(), nRdd.variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### filter()\n",
    "\n",
    "조건에 맞는 문장만 분리해 보자.\n",
    "위 파일에서 \"Spark\" 단어가 포함된 문장이 4개인지 확인해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "myRdd_spark=myRdd2.filter(lambda line: \"Spark\" in line)\n",
    "print myRdd_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "한글을 filter하려면 앞에 u를 붙여준다. u는 유니코드를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n"
     ]
    }
   ],
   "source": [
    "myRdd_unicode = myRdd2.filter(lambda line: u\"스파크\" in line)\n",
    "print myRdd_unicode.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "filter()를 사용해서 **stopwords** 제거하기\n",
    "문장 안에 stopwords를 포함한 경우는 제거되지 않는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['is','am','are','the','for','a']\n",
    "myRdd_stop = myRdd2.filter(lambda x: x not in stopwords)\n",
    "myRdd_stop.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### foreach()\n",
    "\n",
    "foreach()는 action이지만, 다른 action과 달리 반환 값이 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f(x): print(x)\n",
    "spark.sparkContext.parallelize([1, 2, 3, 4, 5]).foreach(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### map()함수로 단어 분리\n",
    "\n",
    "lambda 함수를 사용해 공백으로 단어로 분리해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words=myRdd2.map(lambda x:x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* lambda아닌 사용자함수로 map()을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mySplit(x):\n",
    "    return x.split(\" \")\n",
    "\n",
    "words=myRdd2.map(mySplit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'Wikipedia'],\n",
       " [u'Apache',\n",
       "  u'Spark',\n",
       "  u'is',\n",
       "  u'an',\n",
       "  u'open',\n",
       "  u'source',\n",
       "  u'cluster',\n",
       "  u'computing',\n",
       "  u'framework.'],\n",
       " [u'\\uc544\\ud30c\\uce58',\n",
       "  u'\\uc2a4\\ud30c\\ud06c\\ub294',\n",
       "  u'\\uc624\\ud508',\n",
       "  u'\\uc18c\\uc2a4',\n",
       "  u'\\ud074\\ub7ec\\uc2a4\\ud130',\n",
       "  u'\\ucef4\\ud4e8\\ud305',\n",
       "  u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.']]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wikipedia \n",
      "-----\n",
      "Apache Spark is an open source cluster computing framework. \n",
      "-----\n",
      "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다. \n",
      "-----\n",
      "Apache Spark Apache Spark Apache Spark Apache Spark \n",
      "-----\n",
      "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크 \n",
      "-----\n",
      "Originally developed at the University of California, Berkeley's AMPLab, \n",
      "-----\n",
      "the Spark codebase was later donated to the Apache Software Foundation, \n",
      "-----\n",
      "which has maintained it since. \n",
      "-----\n",
      "Spark provides an interface for programming entire clusters with \n",
      "-----\n",
      "implicit data parallelism and fault-tolerance. \n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for line in words.collect():\n",
    "    for word in line:\n",
    "        print word,\n",
    "    print \"\\n-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 각 문장의 철자 갯수를 센다.\n",
    "    * 첫 문장 'Wikipedia'는 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 59, 32, 51, 72, 71, 30, 64, 46]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.map(lambda s:len(s)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 문장 처리하기\n",
    "* 단어를 교체하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myList=[\"this is\",\"a line\"]\n",
    "_rdd=spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'is'], ['a', 'line']]\n"
     ]
    }
   ],
   "source": [
    "wordsRdd=_rdd.map(lambda x:x.split())\n",
    "print wordsRdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is', 'AA line']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repRdd=_rdd.map(lambda x:x.replace(\"a\",\"AA\"))\n",
    "repRdd.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 첫 글자를 대문자로 만들어서 출력해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'s'.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['THIS', 'A']\n"
     ]
    }
   ],
   "source": [
    "upperRDD =wordsRdd.map(lambda x: x[0].upper())\n",
    "print upperRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['THIS', 'IS'], ['A', 'LINE']]\n"
     ]
    }
   ],
   "source": [
    "upper2RDD =wordsRdd.map(lambda x: [i.upper() for i in x])\n",
    "print upper2RDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### pipeline\n",
    "\n",
    "파이프라인은 transformation(예: map()), action(예: collect()) 함수를 연이어 적용하는 방식을 말한다.\n",
    "파이프라인이 아니라면 함수를 하나씩 끝나고, 결과를 받은 후 다음 함수를 단계별로 적용하게 된다.\n",
    "보다 효율적인 처리를 위해 함수들을 파이프라인같이 붙여서 중간결과를 별도로 산출하지 않고 연이어 처리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "upper2list=wordsRdd.map(lambda x: [i.upper() for i in x]).collect()\n",
    "print type(upper2list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "wordsLength = wordsRdd\\\n",
    "    .map(len)\\\n",
    "    .collect()\n",
    "print wordsLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일에 쓰기\n",
    "\n",
    "Spark에서 list는 RDD로 만들어 로컬 파일에 쓰게 된다.\n",
    "단 RDD는 **디렉토리**가 만들어지고 그 안에 내용이 쓰이게 된다. 파일로 쓰이려면 rdd.coalesce()를 사용해야 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.parallelize(upper2list).saveAsTextFile(\"data/ds_spark_wiki_out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"['A', 'LINE']\", u\"['THIS', 'IS']\"]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd=spark.sparkContext.textFile(\"data/ds_spark_wiki_out\")\n",
    "_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### groupBy\n",
    "\n",
    "transformation 함수이다. Pair, unpaired RDD에 모두 사용할 수 있지만, 주로 **unpaired RDD**에 많이 쓰인다.\n",
    "키를 선택하여 사용할 수 있는 점이 장점이다.\n",
    "groupByKey()와 비교해서 상대적으로 빠르지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞 2글자를 key로 사용해서, groupBy()를 사용할 수 있다.\n",
    "결과는 key-value로 생성되고, value는 iterator로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Wikipedia',\n",
       " u'Apache Spark is an open source cluster computing framework.',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c\\ub294 \\uc624\\ud508 \\uc18c\\uc2a4 \\ud074\\ub7ec\\uc2a4\\ud130 \\ucef4\\ud4e8\\ud305 \\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.',\n",
       " u'Apache Spark Apache Spark Apache Spark Apache Spark',\n",
       " u'\\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c \\uc544\\ud30c\\uce58 \\uc2a4\\ud30c\\ud06c',\n",
       " u\"Originally developed at the University of California, Berkeley's AMPLab,\",\n",
       " u'the Spark codebase was later donated to the Apache Software Foundation,',\n",
       " u'which has maintained it since.',\n",
       " u'Spark provides an interface for programming entire clusters with',\n",
       " u'implicit data parallelism and fault-tolerance.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 앞 2글자로 grouping해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "아파 아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
      "아파 아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
      "-----\n",
      "im implicit data parallelism and fault-tolerance.\n",
      "-----\n",
      "th the Spark codebase was later donated to the Apache Software Foundation,\n",
      "-----\n",
      "Wi Wikipedia\n",
      "-----\n",
      "Ap Apache Spark is an open source cluster computing framework.\n",
      "Ap Apache Spark Apache Spark Apache Spark Apache Spark\n",
      "-----\n",
      "Sp Spark provides an interface for programming entire clusters with\n",
      "-----\n",
      "Or Originally developed at the University of California, Berkeley's AMPLab,\n",
      "-----\n",
      "wh which has maintained it since.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#myRdd_group=myRdd2.flatMap(lambda x:x.split()).groupBy(lambda x:w[0:2])\n",
    "myRdd_group=myRdd2.groupBy(lambda x:x[0:2])\n",
    "\n",
    "for (k,v) in myRdd_group.collect():\n",
    "    for eachValue in v:\n",
    "        print k, eachValue\n",
    "    print \"-----\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "곧 groupByKey()에서 사용할 데이터에 groupBy()를 적용해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', <pyspark.resultiterable.ResultIterable at 0x7f5594479cd0>),\n",
       " ('key1', <pyspark.resultiterable.ResultIterable at 0x7f5594479f10>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', [('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1), ('key2', 1)]),\n",
       " ('key1',\n",
       "  [('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1),\n",
       "   ('key1', 1)])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupBy(lambda x:x[0]).mapValues(list).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5.6.3 Pair RDD\n",
    "\n",
    "Pair RDD는 **key,value 쌍**으로 구성된 RDD를 말한다.\n",
    "이러한 RDD는 키에 대해 연산을 하는 byKey() 또는 값에 대해 byValue() 함수를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "byKey | 동일한 키에 대해 연산<br>- 단계 1: key-value를 계산한다. 각 key의 빈도를 계산  '(key,1)'<br>- 단계 2: byKey를 적용한다. 동일한 key의 value를 더해준다.\n",
    "byValue | 예, mapValues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "다음과 같이 groupByKey(), reduceBykey(), aggregateByKey(), mapValues()함수를 사용할 수 있다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "groupByKey() | 같은 key를 grouping, 부분partition에서 먼저 reduce하지 않고, 전체로 계산한다.\n",
    "reduceByKey() | 같은 key의 value를 합계, 부분partition에서 먼저 reduce하고, 전체로 계산한다. grouping + aggregation. 즉 reduceByKey = groupByKey().reduce()\n",
    "aggregateByKey() | reduceByKey()와 유사하지만 결과를 다른 형식으로 반환. For example (1,2),(1,4) as input and (1,\"six\") as output\n",
    "mapValues() | PairRDD는 key,value가 있기 마련이다. value에 대해 적용하는 함수이다. 즉 key가 아니라 value에 적용하는 함수이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pari RDD 데이터 예제\n",
    "\n",
    "Partition1, 2, 3으로 데이터가 분할되어 있다고 가정하자.\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,1)<br> | (key1,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> | (key2,1)<br> | (key1,1)<br>\n",
    "(key1,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              | (key2,1)<br>\n",
    "(key2,1)<br> |              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Pair RDD 생성\n",
    "\n",
    "테스트 데이터를 구성하고, parallelize()함수를 사용하여 RDD를 만든다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_testList=[(\"key1\",1),(\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key2\",1),\n",
    "           (\"key1\",1),(\"key1\",1),(\"key2\",1),(\"key2\",1)]\n",
    "_testRdd=spark.sparkContext.parallelize(_testList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Pair RDD는 key-value로 구성된다. key만 출력해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['key1',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key1',\n",
       " 'key1',\n",
       " 'key2',\n",
       " 'key2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.keys().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### groupByKey, reduceByKey, mapValues\n",
    "\n",
    "* reduceByKey: 동일한 key에 대해 value를 합계한다. 그 결과는 (key1,6) (key2,5)\n",
    "\n",
    "P1 | P2 | P3\n",
    "-----|-----|-----\n",
    "(key1,3)<br>(key2,2) | (key1,1)<br>(key2,1) | (key1,2)<br>(key2,2)\n",
    "\n",
    "* groupByKey: (key1,[1,1,1,1,1,1]) (key2,[1,1,1,1,1])\n",
    "\n",
    "key1 | key2\n",
    "-----|-----\n",
    "(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1)<br>(key1,1) | (key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)<br>(key2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', 5), ('key1', 6)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "groupByKey()함수의 결과는 **```ResultIterable```**이다. **```mapValues()```**함수에 **list() 함수**를 적어주고 collect()하면 그 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', <pyspark.resultiterable.ResultIterable at 0x7f65001b6850>),\n",
       " ('key1', <pyspark.resultiterable.ResultIterable at 0x7f65001b68d0>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', [1, 1, 1, 1, 1]), ('key1', [1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.groupByKey().mapValues(list).collect() # list is a function, that is, list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이해를 돕기 위해 **```mapValues()```**에 lambda함수를 사용해 1씩 더해본다.\n",
    "키는 변동이 없고, 값만 1씩 증가하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key1', 2),\n",
       " ('key1', 2),\n",
       " ('key2', 2),\n",
       " ('key2', 2)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 단어빈도 예제\n",
    "\n",
    "앞서 위키데이터를 사용해서, 단어를 세어보자.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | flatMap()은 RDD를 전체로 flatten해서 공백으로 분리하고, 단어빈도를 계산한다. map()은 요소별로 적용된다.\n",
    "2 | 요소에 대해 단어빈도 (x,1)를 만듦\n",
    "3 | groupByKey()는 key를 묶어준다. 따라서 **```ResultIterable```** iterator를 반환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', <pyspark.resultiterable.ResultIterable at 0x7f50e1b91990>),\n",
       " (u'\\uc18c\\uc2a4', <pyspark.resultiterable.ResultIterable at 0x7f5108406c10>),\n",
       " (u'is', <pyspark.resultiterable.ResultIterable at 0x7f5108406e90>)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* groupBy()를 한 후 mapValues(sum)을 하면 key별 합계를 구할 수 있다.\n",
    "* mapValues()는 기능을 value에 적용하는 함수\n",
    "    * 내장함수 sum()은 value의 합계를 내는 sum(value)을 의미\n",
    "    * 사용자 함수를 정의해서 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 1),\n",
       " (u'\\uc18c\\uc2a4', 1),\n",
       " (u'is', 1),\n",
       " (u'Wikipedia', 1),\n",
       " (u'AMPLab,', 1),\n",
       " (u'maintained', 1),\n",
       " (u'donated', 1),\n",
       " (u'\\ucef4\\ud4e8\\ud305', 1),\n",
       " (u'open', 1),\n",
       " (u'since.', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'AMPLab,', 1),\n",
       " (u'Apache', 6),\n",
       " (u\"Berkeley's\", 1),\n",
       " (u'California,', 1),\n",
       " (u'Foundation,', 1),\n",
       " (u'Originally', 1),\n",
       " (u'Software', 1),\n",
       " (u'Spark', 7),\n",
       " (u'University', 1),\n",
       " (u'Wikipedia', 1)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def f(x): return len(x)\n",
    "myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(f)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* for문을 사용해서 출력을 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .groupByKey()\\\n",
    "    .mapValues(sum)\\\n",
    "    .sortByKey(True)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'AMPLab,', 1)\n",
      "(u'Apache', 6)\n",
      "(u\"Berkeley's\", 1)\n",
      "(u'California,', 1)\n",
      "(u'Foundation,', 1)\n",
      "(u'Originally', 1)\n",
      "(u'Software', 1)\n",
      "(u'Spark', 7)\n",
      "(u'University', 1)\n",
      "(u'Wikipedia', 1)\n"
     ]
    }
   ],
   "source": [
    "for e in wc:\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: RDD를 사용하여 word count를 계산하고 그래프 그리기.\n",
    "\n",
    "* reduceByKey\n",
    "    * 같은 key에 대해 그 value를 reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Python으로 파일을 생성하고 단어 빈도를 세어 보기\n",
    "\n",
    "우선 map-reduce를 사용하지 않고 단어의 갯수를 세어본다.\n",
    "문서의 단어를 세는 알고리즘은 다음과 같다.\n",
    "단어별 갯수를 저장하기 위해서는 **dictionary**를 사용한다.\n",
    "\n",
    "```python\n",
    "키-갯수 dictionary d 초기화\n",
    "모든 단어를 반복:\n",
    "    단어w를 하나 읽음\n",
    "    단어w가 처음이면 d에 키를 생성하고 1개로\n",
    "    d에 있는 단어w이면 있는 키의 갯수를 하나 증가시킴\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "f=open(os.path.join(\"data\", \"ds_spark_wiki.txt\"))\n",
    "d = dict()\n",
    "for sent in f.readlines():\n",
    "    for w in sent.split():\n",
    "        if w not in d:\n",
    "            d[w]=1\n",
    "        else:\n",
    "            d[w]=d[w]+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "dictionary는 key-value 쌍으로 구성되어 있다.\n",
    "저장된 데이터는 반복문을 사용하여 **d.iteritems()**으로 하나씩 key, value를 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "an 2\n",
      "아파치 5\n",
      "Apache 6\n",
      "Spark 7\n",
      "스파크 4\n",
      "the 3\n"
     ]
    }
   ],
   "source": [
    "d1 = dict()\n",
    "for key, value in d.iteritems():\n",
    "    if value>1:\n",
    "        d1[key]=value\n",
    "        print key, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD를 사용해서 단어 빈도\n",
    "\n",
    "읽은 데이터는 map-reduce로 단어의 수를 세어 본다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "flatMap | 파일 전체를 (flat해서) map. flat하지 않으면 줄바꿈으로 구분함.\n",
    "map | 단어별로 **(x,1)**로 구성함\n",
    "reduceByKey | 동일한 단어(키)의 value, 즉 갯수를 서로 합하게 됨.\n",
    "map() | 앞 함수의 출력 (x,1)의 순서를 바꿈. 즉, 갯수x[1]를 앞으로 단어x[1]을 뒤로 자리 바꿈.\n",
    "sortByKey(false) | 내림차순 정렬\n",
    "take(10) | 10개를 선택함.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wc2=myRdd2\\\n",
    "    .flatMap(lambda x:x.split())\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .reduceByKey(lambda x,y:x+y)\\\n",
    "    .map(lambda x:(x[1],x[0]))\\\n",
    "    .sortByKey(False)\\\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n",
      "7 Spark\n",
      "6 Apache\n",
      "5 아파치\n",
      "4 스파크\n",
      "3 the\n",
      "2 an\n",
      "1 and\n",
      "1 소스\n",
      "1 is\n",
      "1 Wikipedia\n"
     ]
    }
   ],
   "source": [
    "print type(wc2)\n",
    "for i in wc2:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "flatMap()을 하지 않고, **단순하게 map()**을 하여 갯수를 세면 **문장이 키**로 세어진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "wc2=myRdd2\\\n",
    "    .map(lambda x:(x,1))\\\n",
    "    .take(10)\n",
    "for i in wc2:\n",
    "    print i[0],i[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* barh() horizontal bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAFdCAYAAAD2ROx1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSvv/OBPGdJHKtS4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6a608dd9d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "count = map(lambda x: x[0], wc2)\n",
    "word = map(lambda x: x[1], wc2)\n",
    "plt.barh(range(len(count)), count, color = 'grey')\n",
    "plt.yticks(range(len(count)), word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### combineByKey\n",
    "\n",
    "앞서 사용했던 데이터로 combineByKey()를 사용해 보자. 키별로 합계 및 갯수 ```(key, (sum, count))```를 계산한다.\n",
    "\n",
    "구분 | combiner | merge values | merge combiner\n",
    "-----|-----|-----|-----\n",
    "설명 | 각 키에 대해 **```(value,1)```** 튜플 만든다 | 값을 더해나감 (sum,count)<br>즉, **```sum+value```**,**```count+1```** | combiner를 더함 key,(sum,count)\n",
    "예제 | key1,(1,1) | 다음 신규 데이터 (key1,1)을 추가해서 key1,(1+1,1+1) | key1,(5,5)\n",
    "결과 | key1,(1,1)<br>key1,(1,1)<br>key1,(1,1)<br>key1,(1,1)<br>key1,(1,1)<br>key1,(1,1)<br>key2,(1,1)<br>key2,(1,1)<br>key2,(1,1)<br>key2,(1,1)<br>key2,(1,1) | (5,5)<br>(6,6) | key1,(5,5)<br>key2,(6,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('key2', (5, 5)), ('key1', (6, 6))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),\n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1])) \\\n",
    "        .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 평균 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_testCbkRdd=_testRdd.combineByKey(lambda value: (value,1),\n",
    "                     lambda x,value: (x[0]+value, x[1]+1),                      \n",
    "                     lambda x,y: (x[0]+y[0], x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key1': 1.0, 'key2': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey = _testCbkRdd.map(lambda (key,(sum,count)):(key,float(sum)/count))\n",
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### combineByKey 예제\n",
    "\n",
    "* combineByKey(x, y, z) - groupByKey()는 값 합계를 미리 계산하지 때문에 비효율적이다.\n",
    "* partition별로 무작위 배분되어 실행되나, 아래는 하나의 partition을 가정하고 예제를 설명한다.\n",
    "    * partition A: ('kim',86),('lim',87),('kim',91),('lim',79)\n",
    "    * partition B: ('kim',75),('lim',78),('lim',92),('lee',99)\n",
    "\n",
    "구분 | 함수명 | 설명\n",
    "-----|-----|-----\n",
    "x | Combiner 함수 | 값을 combine (V) -> C 예: (value,1)\n",
    "y | Merge value 함수 | 값을 merge (C, V) -> C 예: (sum,count)\n",
    "z | Merge combiners 함수 | combiner를 merge (C, C) -> C) 예: (K, C)\n",
    "\n",
    "데이터 | 적용 함수 | 결과\n",
    "-----|-----|-----\n",
    "('kim',86) | combiner | accum[kim],(86,1)\n",
    "('lim',87) | combiner | accum[lim],(87,1)\n",
    "('kim',75) | merge value | (accum[kim],75) -> accum[kim],(86+75,1+1) = (161,2)\n",
    "('kim',91) | merge value | (accum[kim],91) -> (161+91,2+1) = (252,3)\n",
    "('lim',78) | merge value | (accum[lim],78) -> (87+78,1+1) = (165,2)\n",
    "('lim',92) | merge value | (accum[lim],92) -> (165+92,2+1) = (257,3)\n",
    "('lim',79) | merge value | (accum[lim],79) -> (257+79,3+1) = (336,4)\n",
    "('lee',99) | combiner | accum[lee],(99,1)\n",
    "최종적으로 | merge combiners | [('lim', (336, 4)), ('lee', (99, 1)), ('kim', (252, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('lim', (336, 4)), ('lee', (99, 1)), ('kim', (252, 3))]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marks = spark.sparkContext.parallelize([('kim',86),('lim',87),('kim',75),\n",
    "                                      ('kim',91),('lim',78),('lim',92),\n",
    "                                      ('lim',79),('lee',99)])\n",
    "marksByKey = marks.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "marksByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('M', (718.0, 4)), ('F', (326.0, 2))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heights = spark.sparkContext.parallelize([\n",
    "        ('M',182.),('F',164.),('M',180.),('M',185.),('M',171.),('F',162.)\n",
    "    ])\n",
    "heightsByKey = heights.combineByKey(lambda value: (value,1),\n",
    "                             lambda x,value: (x[0]+value, x[1]+1),\n",
    "                             lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "heightsByKey.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'M': 179.5, 'F': 163.0}\n"
     ]
    }
   ],
   "source": [
    "avgByKey = heightsByKey.map(lambda (label,(valSum,count)):\n",
    "                                (label,valSum/count))\n",
    "\n",
    "print avgByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 성적 합계 및 평균.\n",
    "\n",
    "이름 | 과목 | 점수\n",
    "-----|-----|-----\n",
    "김하나 | English | 100\n",
    "김하나 | Math | 80\n",
    "임하나 | English | 70\n",
    "임하나 | Math | 100\n",
    "김갑돌 | English | 82.3\n",
    "김갑돌 | Math | 98.5\n",
    "\n",
    "* 문제 3-1: 이름으로 합계\n",
    "\n",
    "```python\n",
    "'임하나' 170.0\n",
    "'김하나' 180.0\n",
    "'김갑돌' 180.8\n",
    "```\n",
    "* 문제 3-2: 과목으로 합계\n",
    "\n",
    "```python\n",
    "'English' 252.3\n",
    "'Math' 278.5\n",
    "```\n",
    "\n",
    "* 문제 3-3: 이름으로 합계과 개수\n",
    "\n",
    "```python\n",
    "'임하나' (170.0, 2)\n",
    "'김하나' (180.0, 2)\n",
    "'김갑돌' (180.8, 2)\n",
    "```\n",
    "\n",
    "* 문제 3-4: 이름으로평균\n",
    "\n",
    "```python\n",
    "'임하나' 85.0\n",
    "'김하나' 90.0\n",
    "'김갑돌' 90.4\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "marks=[\n",
    "    \"'김하나','English', 100\",\n",
    "    \"'김하나','Math', 80\",\n",
    "    \"'임하나','English', 70\",\n",
    "    \"'임하나','Math', 100\",\n",
    "    \"'김갑돌','English', 82.3\",\n",
    "    \"'김갑돌','Math', 98.5\"\n",
    "]\n",
    "_marksRdd=spark.sparkContext.parallelize(marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' 170.0\n",
      "'김하나' 180.0\n",
      "'김갑돌' 180.8\n"
     ]
    }
   ],
   "source": [
    "# marks by name\n",
    "_marksbyname=_marksRdd.map(lambda x:x.split(',')).map(lambda x: (x[0],float(x[2])))\n",
    "for i in _marksbyname.reduceByKey(lambda x,y:x+y).collect():\n",
    "  print i[0],i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'English' 252.3\n",
      "'Math' 278.5\n"
     ]
    }
   ],
   "source": [
    "# marks by subject\n",
    "_marksbysubject=_marksRdd.map(lambda x:x.split(',')).map(lambda x: (x[1],float(x[2])))\n",
    "for i in _marksbysubject.reduceByKey(lambda x,y:x+y).collect():\n",
    "  print i[0],i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# sum, counts by name\n",
    "sum_counts = _marksbyname.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' (170.0, 2)\n",
      "'김하나' (180.0, 2)\n",
      "'김갑돌' (180.8, 2)\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print each,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'임하나' 85.0\n",
      "'김하나' 90.0\n",
      "'김갑돌' 90.4\n"
     ]
    }
   ],
   "source": [
    "# average\n",
    "averageByKey = sum_counts.map(lambda (key,(sum,count)):(key,float(sum)/count))\n",
    "for i in averageByKey.collect():\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 서울시 지하철호선별 승차인원 평균 구하기.\n",
    "\n",
    "### 문제\n",
    "\n",
    "정량데이터는 보통 집단화하여 빈도, 평균, 합계 등 서술통계를 계산한다.\n",
    "서울시 지하철호선별 역별 승하차 인원 정보를 가져와 평균을 구해보자.\n",
    "* 파일 명 CARD_SUBWAY_MONTH_201501.csv를 다운로드 받아서 일부만 테스트용 데이터로 사용한다.\n",
    "* 오픈API 샘플URL http://openapi.seoul.go.kr:8088/(인증키)/xml/CardSubwayStatsNew/1/5/20151101\n",
    "\n",
    "### 해결\n",
    "\n",
    "PairRDD를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 2015년 1월 csv파일의 일부를 가져와서 리스트를 생성한다.\n",
    "* 데이터 헤더: \"사용일자\",\"노선명\",\"역ID\",\"역명\",승차총승객수,하차총승객수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_sub=[\"20150101,2호선,0236,영등포구청,6199,6219\",\n",
    "\"20150101,2호선,0237,당산,7982,8946\",\n",
    "\"20150101,2호선,0238,합정,17406,15241\",\n",
    "\"20150101,3호선,0309,지축,515,538\",\n",
    "\"20150101,3호선,0310,구파발,6879,6260\",\n",
    "\"20150101,3호선,0311,연신내,20031,19470\",\n",
    "\"20150101,3호선,0312,불광,9519,11029\",\n",
    "\"20150101,4호선,0425,회현,7465,7574\",\n",
    "\"20150101,4호선,0426,서울역,3943,10823\",\n",
    "\"20150101,경부선,1002,남영,4340,4535\",\n",
    "\"20150101,경부선,1003,용산,28980,27684\",\n",
    "\"20150101,경부선,1004,노량진,23021,23862\",\n",
    "\"20150101,경부선,1005,대방,6360,6476\",\n",
    "\"20150101,경부선,1006,영등포,37247,36102\",\n",
    "\"20150101,경원선,1008,이촌,1940,1507\",\n",
    "\"20150101,경원선,1009,서빙고,911,1000\",\n",
    "\"20150101,경원선,1010,한남,1885,1863\",\n",
    "\"20150101,경원선,1011,옥수,43,37\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_subRdd = spark.sparkContext.parallelize(_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101,2호선,0236,영등포구청,6199,6219\n"
     ]
    }
   ],
   "source": [
    "for i in _subRdd.take(1):\n",
    "    print i,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* map()을 사용해서 컴마를 제외할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20150101 2호선 0236 영등포구청 6199 6219\n",
      "20150101 2호선 0237 당산 7982 8946\n",
      "20150101 2호선 0238 합정 17406 15241\n",
      "20150101 3호선 0309 지축 515 538\n",
      "20150101 3호선 0310 구파발 6879 6260\n",
      "20150101 3호선 0311 연신내 20031 19470\n",
      "20150101 3호선 0312 불광 9519 11029\n",
      "20150101 4호선 0425 회현 7465 7574\n",
      "20150101 4호선 0426 서울역 3943 10823\n",
      "20150101 경부선 1002 남영 4340 4535\n",
      "20150101 경부선 1003 용산 28980 27684\n",
      "20150101 경부선 1004 노량진 23021 23862\n",
      "20150101 경부선 1005 대방 6360 6476\n",
      "20150101 경부선 1006 영등포 37247 36102\n",
      "20150101 경원선 1008 이촌 1940 1507\n",
      "20150101 경원선 1009 서빙고 911 1000\n",
      "20150101 경원선 1010 한남 1885 1863\n",
      "20150101 경원선 1011 옥수 43 37\n"
     ]
    }
   ],
   "source": [
    "for i in _subRdd.map(lambda x:x.split(',')).collect():\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 호선별 데이터 개수를 출력해 본다.\n",
    "* map()의 3번째 인덱스는 앞에서부터 4번째 철자를 출력한다.\n",
    "* 컴마로 분리해서 인덱스를 사용해야 제대로 데이터를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:int(x[3])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6199,\n",
       " 7982,\n",
       " 17406,\n",
       " 515,\n",
       " 6879,\n",
       " 20031,\n",
       " 9519,\n",
       " 7465,\n",
       " 3943,\n",
       " 4340,\n",
       " 28980,\n",
       " 23021,\n",
       " 6360,\n",
       " 37247,\n",
       " 1940,\n",
       " 911,\n",
       " 1885,\n",
       " 43]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:int(x[4])).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 총 승차인원의 합계를 reduce()로 합산한다.\n",
    "* reduce()함수는 인자를 2개 받아서 합계를 구한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184666"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_subRdd.map(lambda x:x.split(',')).map(lambda x:(int(x[4]))).reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 호선별 승차인원 평균을 계산한다.\n",
    "* Pair RDD를 사용하므로, 사전에 데이터를 (호선, 승차인원) 형식으로 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_subLineByPassengers=_subRdd.map(lambda x:x.split(',')).map(lambda x: (x[1],int(x[4])))\n",
    "sum_counts = _subLineByPassengers.combineByKey(\n",
    "    (lambda x: (x, 1)), # the initial value, with value x and count 1\n",
    "    (lambda acc, value: (acc[0]+value, acc[1]+1)), # how to combine a pair value with the accumulator: sum value, and increment count\n",
    "    (lambda acc1, acc2: (acc1[0]+acc2[0], acc1[1]+acc2[1])) # combine accumulators\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 (11408, 2)\n",
      "2호선 (31587, 3)\n",
      "경원선 (4779, 4)\n",
      "3호선 (36944, 4)\n",
      "경부선 (99948, 5)\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    for each in i:\n",
    "        print each,\n",
    "    print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 11408 2\n",
      "2호선 31587 3\n",
      "경원선 4779 4\n",
      "3호선 36944 4\n",
      "경부선 99948 5\n"
     ]
    }
   ],
   "source": [
    "for i in sum_counts.collect():\n",
    "    print i[0],i[1][0],i[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 위와 달리 map()을 사용하여 평균을 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "averageByKey = sum_counts.map(lambda (key,(sum,count)):(key,float(sum)/count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2\\xed\\x98\\xb8\\xec\\x84\\xa0': 10529.0,\n",
       " '3\\xed\\x98\\xb8\\xec\\x84\\xa0': 9236.0,\n",
       " '4\\xed\\x98\\xb8\\xec\\x84\\xa0': 5704.0,\n",
       " '\\xea\\xb2\\xbd\\xeb\\xb6\\x80\\xec\\x84\\xa0': 19989.6,\n",
       " '\\xea\\xb2\\xbd\\xec\\x9b\\x90\\xec\\x84\\xa0': 1194.75}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "averageByKey.collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4호선 5704.0\n",
      "2호선 10529.0\n",
      "경원선 1194.75\n",
      "3호선 9236.0\n",
      "경부선 19989.6\n"
     ]
    }
   ],
   "source": [
    "for i in averageByKey.collect():\n",
    "    for j in i:\n",
    "        print j,\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 spark-submit\n",
    "\n",
    "* sys.path 설정은 하지 않아도 된다.\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```\n",
    "log4j.rootCategory=ERROR, console\n",
    "```\n",
    "\n",
    "* Python 파일의 encoding 선언\n",
    "    * 기본 설정은 7-bit ASCII\n",
    "```\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_hello.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_hello.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print spark.version\n",
    "    spark.conf.set(\"spark.logConf\",\"false\")\n",
    "    rdd=spark.sparkContext.parallelize(range(1000), 10)\n",
    "    print \"mean=\",rdd.mean()\n",
    "    nums = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "    squared = nums.map(lambda x: x * x).collect()\n",
    "    for num in squared:\n",
    "        print \"%i \" % (num)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 244ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "---------BEGIN-----------\n",
      "---------RESULT-----------\n",
      "2.0.0\n",
      "mean= 499.5\n",
      "1 \n",
      "4 \n",
      "9 \n",
      "16 \n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_hello.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_spark_rdd_reduceBykey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_rdd_reduceBykey.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "import os\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    myRdd=spark.sparkContext\\\n",
    "        .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "    res=myRdd\\\n",
    "        .flatMap(lambda x:x.split())\\\n",
    "        .map(lambda x:(x,1))\\\n",
    "        .reduceByKey(lambda x,y:x+y)\\\n",
    "        .map(lambda x:(x[1],x[0]))\\\n",
    "        .sortByKey(False)\\\n",
    "        .take(10)\n",
    "    for i in res:\n",
    "        print i\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jsl/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jsl/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "graphframes#graphframes added as a dependency\n",
      "org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency\n",
      "com.databricks#spark-csv_2.11 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0\n",
      "\tconfs: [default]\n",
      "\tfound graphframes#graphframes;0.4.0-spark2.0-s_2.11 in spark-packages\n",
      "\tfound com.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 in central\n",
      "\tfound com.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.11.0 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.7 in central\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.10;2.0.0 in central\n",
      "\tfound org.mongodb#mongo-java-driver;3.2.2 in central\n",
      "\tfound com.databricks#spark-csv_2.11;1.5.0 in central\n",
      "\tfound org.apache.commons#commons-csv;1.1 in central\n",
      "\tfound com.univocity#univocity-parsers;1.5.1 in central\n",
      ":: resolution report :: resolve 246ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-csv_2.11;1.5.0 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-api_2.11;2.1.2 from central in [default]\n",
      "\tcom.typesafe.scala-logging#scala-logging-slf4j_2.11;2.1.2 from central in [default]\n",
      "\tcom.univocity#univocity-parsers;1.5.1 from central in [default]\n",
      "\tgraphframes#graphframes;0.4.0-spark2.0-s_2.11 from spark-packages in [default]\n",
      "\torg.apache.commons#commons-csv;1.1 from central in [default]\n",
      "\torg.mongodb#mongo-java-driver;3.2.2 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.10;2.0.0 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.7 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   10  |   0   |   0   |   0   ||   10  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 10 already retrieved (0kB/6ms)\n",
      "---------RESULT-----------\n",
      "(7, u'Spark')\n",
      "(6, u'Apache')\n",
      "(3, u'the')\n",
      "(2, u'an')\n",
      "(1, u'and')\n",
      "(1, u'\\uc18c\\uc2a4')\n",
      "(1, u'is')\n",
      "(1, u'Wikipedia')\n",
      "(1, u'AMPLab,')\n",
      "(1, u'maintained')\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_rdd_reduceBykey.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: RDD를 사용하여 word vector를 생성하기.\n",
    "\n",
    "### 문제\n",
    "사람은 문서를 읽으면 그 의미를 알 수 있다. 주제가 무엇이고 심지어는 숨겨진 행간도 이해할 수 있다.\n",
    "그러나 컴퓨터는 문서를 읽고 의미를 이해하는 것이 쉽지 않다.\n",
    "대신 그 안에 어떤 단어가 쓰였고, 많이 쓰인 단어가 무엇인지 **word vector**를 만들어 분석하게 된다.\n",
    "word vector는 단어빈도를 나타낸다.\n",
    "\n",
    "### 해결\n",
    "* RDD API를 사용해서 단어를 셀 수 있다 (map, reduce 등).\n",
    "* mllib 패키지를 사용하여 데이터를 변환할 수 있다.\n",
    "    * TF-IDF, Word2Vec 등을 사용할 수 있다.\n",
    "    * mllib에 없는 변환기능은 ml을 사용한다 (ml은 DataFrame API 패키지.)\n",
    "        * Tokenizer, Stopwords, NGram 등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 단어별 빈도\n",
    "\n",
    "파일 전체를 읽어서, 단어의 수를 세어 본다.\n",
    "파일은 이미 만들어 놓은 wiki를 사용한다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | sparkContext.textFile()로 파일을 읽어 RDD로 만든다.\n",
    "2 | flatMap()을 으로 공백으로 분리하여 RDD를 생성한다. 이러한 변환은 실제 연산까지 일어나지 않는다, 즉 lazy변환이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/ds_spark_wiki.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/ds_spark_wiki.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "lines=spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\n",
    "wc=lines\\\n",
    "    .flatMap(lambda x: x.split(' '))\n",
    "print type(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "공백으로 분리된 RDD를 collect()함수로 취합한다.\n",
    "단어의 갯수는 count() 함수로 알 수 있다.\n",
    "**collect() 결과는 list**로 만들어 진다.\n",
    "모든 항목을 출력하려면 list의 인덱스 [:]를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 갯수:  72\n",
      "전체 단어:  [u'Wikipedia', u'Apache', u'Spark', u'is', u'an', u'open', u'source', u'cluster', u'computing', u'framework.', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c\\ub294', u'\\uc624\\ud508', u'\\uc18c\\uc2a4', u'\\ud074\\ub7ec\\uc2a4\\ud130', u'\\ucef4\\ud4e8\\ud305', u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4.', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'Apache', u'Spark', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c', u'\\uc544\\ud30c\\uce58', u'\\uc2a4\\ud30c\\ud06c', u'Originally', u'developed', u'at', u'the', u'University', u'of', u'California,', u\"Berkeley's\", u'AMPLab,', u'the', u'Spark', u'codebase', u'was', u'later', u'donated', u'to', u'the', u'Apache', u'Software', u'Foundation,', u'which', u'has', u'maintained', u'it', u'since.', u'Spark', u'provides', u'an', u'interface', u'for', u'programming', u'entire', u'clusters', u'with', u'implicit', u'data', u'parallelism', u'and', u'fault-tolerance.']\n"
     ]
    }
   ],
   "source": [
    "print \"단어 갯수: \", wc.count()\n",
    "print \"전체 단어: \", wc.collect()[0:72]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위에서 한글이 unicode로 출력되는 것은 깨진 것이 아니다. 하나씩 출력하면 한글로 출력된다.\n",
    "10번째를 출력하려면 [10], 한글이 잘 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---> 한글로 출력: 아파치\n"
     ]
    }
   ],
   "source": [
    "print \"---> 한글로 출력:\",wc.collect()[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 map-reduce를 같이 사용하여 단어를 세어 tuple로 만들어 보자.\n",
    "flatMap()은 앞서 설명한 바와 같이 파일 텍스트를 공백으로 분리한다.\n",
    "map()함수는 모든 단어에 대해 소문자로 만들고, **불필요한 구문 (new lines, commas, periods)을 제거**한 후 tuple로 만든다.\n",
    "즉 **(단어, 1) 구조**로 만들어 **같은 단어는 나중에 서로 더할 수** 있게 만들어 놓는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wc = spark.sparkContext.textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))\\\n",
    "    .flatMap(lambda x: x.split(' '))\\\n",
    "    .map(lambda x: (x.lower().rstrip().lstrip().rstrip(',').rstrip('.'), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "아직 단어별로 갯수를 계산하지 않았기 때문에, 모두 1인 값을 가진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1), (u'an', 1), (u'an', 1), (u'and', 1), (u'apache', 1)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.sortByKey().take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 빈도 집계\n",
    "\n",
    "이제 단어의 갯수를 합계내어 보자. 아래 방법 모두 **동일한 결과를 산출**한다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "reduceByKey(add) | 'add' operator를 사용하면, 단어튜플의 수를 키별로 더할 수 있다.\n",
    "groupByKey().mapValues(sum) | mapValues()를 사용하여 value의 'sum'을 계산할 수 있다.\n",
    "groupByKey().map(lambda (x,iter) : (x,len(iter))) | (key,value)의 구조를 사용하여 합계를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### reduceByKey()\n",
    "\n",
    "Python의 연산자 add() 함수를 사용해서 할 수 있다.\n",
    "**operator.add()**는 reduce()함수의 숫자 인자 x,y를 받아서 x+y 연산을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import add\n",
    "wcReduceByKey = wc.reduceByKey(add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1),\n",
       " (u'an', 2),\n",
       " (u'and', 1),\n",
       " (u'apache', 6),\n",
       " (u'at', 1),\n",
       " (u\"berkeley's\", 1),\n",
       " (u'california', 1),\n",
       " (u'cluster', 1),\n",
       " (u'clusters', 1),\n",
       " (u'codebase', 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcReduceByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### groupByKey()\n",
    "\n",
    "groupByKey()는 단어키로 동일한 단어는 집단화한다.\n",
    "집단화 하면 **PairRDD가 되고, 즉 key-value 쌍**으로 구성된다.\n",
    "**mapValues()는 각 쌍의 value에 대해서 sum 연산**을 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcGroupByKey = wc.groupByKey().mapValues(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1),\n",
       " (u'an', 2),\n",
       " (u'and', 1),\n",
       " (u'apache', 6),\n",
       " (u'at', 1),\n",
       " (u\"berkeley's\", 1),\n",
       " (u'california', 1),\n",
       " (u'cluster', 1),\n",
       " (u'clusters', 1),\n",
       " (u'codebase', 1)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcGroupByKey.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "mapValues()를 사용하지 않고 map()으로 **len()**갯수를 세어도 동일한 결과를 얻을 수 있다.\n",
    "**단어별로 1개씩 만들어 놓았으므로 len()으로 세면 단어갯수의 합계**가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wcGroupByKey2 = wc.groupByKey().map(lambda (x,v): (x,len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'amplab', 1),\n",
       " (u'an', 2),\n",
       " (u'and', 1),\n",
       " (u'apache', 6),\n",
       " (u'at', 1),\n",
       " (u\"berkeley's\", 1),\n",
       " (u'california', 1),\n",
       " (u'cluster', 1),\n",
       " (u'clusters', 1),\n",
       " (u'codebase', 1)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wcGroupByKey2.sortByKey().take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 줄로 구분하여 단어 빈도\n",
    "\n",
    "flatMap()을 사용하지 않으면 줄로 구분하여 단어의 빈도를 셀 수 있다.\n",
    "\n",
    "줄 | 설명\n",
    "-----|-----\n",
    "1 | sparkContext.textFile() 파일을 읽어 RDD 생성\n",
    "2 | 불필요한 컴마, 점, 하이픈을 제거하고 소문자로\n",
    "3 | 공백으로 분리\n",
    "4 | 반복문으로 단어별 튜플구조화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wc = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\\\n",
    "    .map(lambda x:x.split())\\\n",
    "    .map(lambda x:[(i,1) for i in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'wikipedia', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'is', 1), (u'an', 1), (u'open', 1), (u'source', 1), (u'cluster', 1), (u'computing', 1), (u'framework', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c\\ub294', 1), (u'\\uc624\\ud508', 1), (u'\\uc18c\\uc2a4', 1), (u'\\ud074\\ub7ec\\uc2a4\\ud130', 1), (u'\\ucef4\\ud4e8\\ud305', 1), (u'\\ud504\\ub808\\uc784\\uc6cc\\ud06c\\uc774\\ub2e4', 1)]\n",
      "[(u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1), (u'apache', 1), (u'spark', 1)]\n",
      "[(u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c', 1), (u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c', 1), (u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c', 1), (u'\\uc544\\ud30c\\uce58', 1), (u'\\uc2a4\\ud30c\\ud06c', 1)]\n",
      "[(u'originally', 1), (u'developed', 1), (u'at', 1), (u'the', 1), (u'university', 1), (u'of', 1), (u'california', 1), (u\"berkeley's\", 1), (u'amplab', 1)]\n",
      "[(u'the', 1), (u'spark', 1), (u'codebase', 1), (u'was', 1), (u'later', 1), (u'donated', 1), (u'to', 1), (u'the', 1), (u'apache', 1), (u'software', 1), (u'foundation', 1)]\n",
      "[(u'which', 1), (u'has', 1), (u'maintained', 1), (u'it', 1), (u'since', 1)]\n",
      "[(u'spark', 1), (u'provides', 1), (u'an', 1), (u'interface', 1), (u'for', 1), (u'programming', 1), (u'entire', 1), (u'clusters', 1), (u'with', 1)]\n",
      "[(u'implicit', 1), (u'data', 1), (u'parallelism', 1), (u'and', 1), (u'fault', 1), (u'tolerance', 1)]\n"
     ]
    }
   ],
   "source": [
    "for e in wc.collect():\n",
    "    print e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF (Term Frequency)\n",
    "\n",
    "* HashingTF: 단어ID로 Hash 값을 사용하여 단어빈도를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = spark.sparkContext.textFile(\"data/ds_spark_wiki.txt\")\\\n",
    "    .map(lambda line: line.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(1048576, {253068: 1.0}),\n",
       " SparseVector(1048576, {36751: 1.0, 50570: 1.0, 68380: 1.0, 415281: 1.0, 511377: 1.0, 728364: 1.0, 862087: 1.0, 938426: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {63234: 1.0, 340190: 1.0, 357478: 1.0, 375592: 1.0, 458138: 1.0, 486171: 1.0, 598772: 1.0}),\n",
       " SparseVector(1048576, {938426: 4.0, 999480: 4.0}),\n",
       " SparseVector(1048576, {486171: 4.0, 1016271: 4.0}),\n",
       " SparseVector(1048576, {36757: 1.0, 225801: 1.0, 323305: 1.0, 453405: 1.0, 498679: 1.0, 518030: 1.0, 688842: 1.0, 762570: 1.0, 959994: 1.0}),\n",
       " SparseVector(1048576, {420843: 1.0, 550676: 1.0, 725041: 1.0, 782544: 1.0, 938426: 1.0, 959994: 2.0, 991590: 1.0, 993084: 1.0, 996703: 1.0, 999480: 1.0}),\n",
       " SparseVector(1048576, {50573: 1.0, 263739: 1.0, 892834: 1.0, 1014710: 1.0, 1035538: 1.0}),\n",
       " SparseVector(1048576, {3932: 1.0, 36751: 1.0, 192182: 1.0, 358969: 1.0, 363244: 1.0, 496856: 1.0, 546913: 1.0, 938426: 1.0, 951974: 1.0}),\n",
       " SparseVector(1048576, {69621: 1.0, 157580: 1.0, 219357: 1.0, 297436: 1.0, 715648: 1.0})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.feature import HashingTF\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(documents)\n",
    "tf.collect()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
