{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Spark DataFrame\n",
    "\n",
    "* Last updated 20201013TUE1400 20191029_20170606_20170221_20161125\n",
    "\n",
    "## S.1 학습내용\n",
    "\n",
    "### S.1.1 목표\n",
    "\n",
    "* Spark DataFrame을 생성하고, API를 사용할 수 있다.\n",
    "* Spark SQL을 사용하여 데이터를 조회할 수 있다.\n",
    "* Spark 데이터를 MongoDB에 쓰고, 읽을 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.1.2 목차\n",
    "* S.2 Jupyter Notebook에서 SparkSession 생성\n",
    "* S.3 DataFrame: 특징, Schema, DataFrame의 API\n",
    "* S.4 DataFrame 생성\n",
    "* S.4.1 schema에서 생성하기\n",
    "* S.4.2 RDD에서 생성하기\n",
    "* S.4.3 Pandas\n",
    "* S.4.4 csv 파일 읽기\n",
    "* S.4.5 tsv 파일 읽기\n",
    "* S.4.6 JSON 파일 읽기\n",
    "* 문제: 월드컵 데이터 JSON\n",
    "* S.4.7 Parquet 파일 읽기, 쓰기\n",
    "* S.5 DataFrame API 사용해 보기\n",
    "    * 빈 DataFrame 생성\n",
    "    * range\n",
    "    * 컬럼 추가 withColumn, 컬럼 삭제 Drop\n",
    "    * 사용자정의 함수 udf\n",
    "    * 컬럼명 변경 withColumnRenamed\n",
    "    * 그래프\n",
    "    * 컬럼 조회 select\n",
    "    * filter\n",
    "    * regexp_replace 컬럼의 내용 변경\n",
    "    * groupBy\n",
    "    * F 함수\n",
    "    * 행 추가\n",
    "    * partition\n",
    "    * 통계 요약 describe\n",
    "    * 결측값\n",
    "* 문제: 년별 분기별 대여건수\n",
    "* S.6 Spark SQL\n",
    "* 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "* 문제 S-2: Twitter JSON 데이터 읽기\n",
    "* 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "* 문제 S-4: 우버 택시의 운행기록 분석\n",
    "* 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "* S.7 MongoDB Spark connector\n",
    "* S.7.1 설정\n",
    "* S.7.2 uri \n",
    "* S.7.3 MongoDB Python API \n",
    "* S.7.4 연습으로 쓰기, 읽기\n",
    "* S.8 spark-submit\n",
    "* S.8.1 간단한 작업\n",
    "* S.8.2 MongoDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## S.2 Jupyter Notebook에서 SparkSession 생성\n",
    "\n",
    "### Spark를 설치한 경우\n",
    "\n",
    "Spark를 설치했다면, Spark가 설치된 디렉토리를 SPARK_HOME으로 설정하고, 실행에 필요한 'py4j-0.10.1-src.zip', 'pyspark.zip' 라이브러리를 추가해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"SPARK_HOME\"]=os.path.join(os.environ['HOME'],'Downloads','spark-2.0.0-bin-hadoop2.7')\n",
    "os.environ[\"PYLIB\"]=os.path.join(os.environ[\"SPARK_HOME\"],'python','lib')\n",
    "\n",
    "# 스파크 풀 버전을 설치한 경우가 없을 것임으로 프리빌더 어서 안해줘도 될 듯\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'py4j-0.10.1-src.zip'))\n",
    "sys.path.insert(0,os.path.join(os.environ[\"PYLIB\"],'pyspark.zip'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "그러나 pyspark만을 설치한 경우에는, 위와 같이 별도로 경로와 라이브러리를 추가하지 않아도 된다.\n",
    "다만 Python 2, 3 여러 버전이 설치된 경우에는 아래와 같이 그 경로를 설정해 주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (spark.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 DataFrame\n",
    "\n",
    "### 특징\n",
    "\n",
    "DataFrame은 **행, 열로 구조화**된 데이터구조이다.\n",
    "관계형데이터베이스 RDB의 테이블이나 엑셀 쉬트sheet와 비슷하다.\n",
    "또는 Pandas 또는 R을 사용해 보았다면 거기서 제공되는 DataFrame과 유사하다.\n",
    "Apache Spark 1.0에서는 **SchemaRDD**라는 명칭으로 시험적으로 제공되었다. 이름에서 보듯이 RDD에 스키마를 얹어서 만든 개념이다.\n",
    "그러나 Spark의 DataFrame은 **대용량 데이터를 처리하기 위해 만들어진 프레임워크로 분산**해서 사용할 수 있게 고안되었다.\n",
    "\n",
    "앞서 사용했던 **RDD가 schema를 정하지 않는** 것과 달리, **DataFrame은 모델 schema**를 설정해서 사용을 한다. '열'에 대해 명칭 및 **데이터 타잎**을 가지고 있고, 이를 지켜서 데이터를 저장하게 된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Schema\n",
    "\n",
    "**Row**는 DataFrame의 행으로, 데이터 요소항목을 묶어서 구성한다. **Python list나 dict**를 사용하여 Row를 구성할 수 있다.\n",
    "**Column**은 DataFrame의 **열**이고, 다음과 같은 **데이터 타잎**을 가진다.\n",
    "\n",
    "```python\n",
    "- NullType\n",
    "- StringType\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- DateType\n",
    "- TimestampType\n",
    "- DoubleType\n",
    "- DecimalType\n",
    "- ShortType\n",
    "- ArrayType\n",
    "- MapType\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame의 API\n",
    "\n",
    "RDD와 마찬가지로 DataFrame을 구성하여 **머신러닝**의 입력데이터로 사용할 수 있다.\n",
    "현재 버전 2.0부터 RDD에 대한 지원은 줄여나가고, **버전 3.0 이후에는 DataFrame API를 공식적으로 지원**한다고 발표한 바 있다. RDD보다 우선적으로 사용하는 것이 좋겠다. 아래는 DataFrame에서 제공하는 일부 API이다.\n",
    "\n",
    "기능 | 설명 | 예제\n",
    "-------|-------|-------\n",
    "```json``` | json 파일에서 읽기 | ```spark.read.json(\"employee.json\")```\n",
    "```show``` | DataFrame에 로딩된 데이터 읽기 | ```df.show()```\n",
    "```schema``` | 데이터 schema 보기 | ```df.printSchema()```\n",
    "```select``` | 열을 선택 | ```df.select(\"name\")```\n",
    "```filter``` | 조건으로 선택하여 읽기 | ```df.filter(dfs(\"age\") > 23).show()```\n",
    "```groupBy``` | 그룹으로 나누기 | ```df.groupBy(\"age\").count().show()```\n",
    "```dropna``` | na를 삭제 | ```df.dropna()``` ```df.na.drop()```\n",
    "```fillna``` | na를 값으로 채우기 | ```fillna()```\n",
    "```count``` | 행 세기 | ```df.count()```\n",
    "```drop``` | 삭제 | ```df.drop(\"name\")```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 DataFrame 생성\n",
    "\n",
    "DataFrame은 관계형데이터베이스 RDB 테이블과 같이, schema를 정해서 생성한다.\n",
    "**schema를 정해주지 않으면, Spark가 자동으로 유추**하게 된다.\n",
    "Python 리스트, RDD, Pandas DataFrame, Hive, csv, JSON, RDB, XML, Parquet, Apache Cassandra 등 다양한 채널에서 읽어서 DataFrame을 만들 수 있다.\n",
    "\n",
    "* **```spark.createDataFrame()```** 함수는 Python List, RDD, Pandas DataFrame 등에서 읽어서 DataFrame을 만들 수 있다.\n",
    "* 또는 **```spark.read.text, spark.read.json, spark.read.parquet, spark.read.load```** 함수로 옵션을 설정해서 외부 파일 등에서 읽을 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.1 schema 생성하기\n",
    "\n",
    "DataFrame은 데이터 모델 schema를 정의하고, 각 컬럼의 명칭과 데이터타입을 정해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 자동으로 인식하는 schema\n",
    "\n",
    "우선 단순하게 Python 자료구조를 사용해서 생성해 본다.\n",
    "아래 열이 3개인 데이터를 **```createDataFrame()```** 함수를 사용하여 넣어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 튜플로 만듬\n",
    "print(f\"type: {type(('1','kim, js', 170))}\")\n",
    "\n",
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# context 접근 안함\n",
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 경우 Spark가 **자동으로 schema를 설정**한다.\n",
    "```printSchema()``` 함수로 schema를 출력해 보기로 하자.\n",
    "* **컬럼**명은 **일련번호**를 가지고 생성된다. schema를 정하지 않았으므로, 열은 '_1', '_2'와 같이 명명된다.\n",
    "* **데이터 타잎**도 유추해서 생성한다. 올바르게 되지 않을 경우도 있다는 점에 유의한다. 아래에서 보듯이 컬럼 1 학년은 ```String```, 컬럼 2 이름은 ```String```, 컬럼 3 키는 ```long```으로 인식하고 있다.\n",
    "* nullable은 결측값이 허용되는지를 말하는 것이다. true이면 허용된다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "한 줄을 출력해 보자. 출력은 Row() 타입으로 앞서 정의한 schema와 일치하는 값을 포함하여 출력한다.\n",
    "즉 첫째, 둘째는 문자열로 세째는 long 타입이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print (myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명 설정\n",
    "\n",
    "앞서 컬럼 Column을 정의하지 않고 DataFrame을 생성하였는데, 이번에는 **컬럼을 정해서** 생성하자.\n",
    "**```createDataFrame()```** 함수에 **인자로 컬럼명을 리스트 ```['year','name','height']```로** 정해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리고 한 줄 출력해 보면, 컬럼명이 변경되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 100개를 생성해 보자.\n",
    "우선 데이터의 원천이 되는 names, items를 정의하자.\n",
    "여기서 하나씩 선택하여 데이터를 생성하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "# long black 아메리카노\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이 names, items 배열에서 **modulus**를 활용하여 하나씩 선택하여 데이터를 생성한다.\n",
    "**names**는 4개이므로 4로 나눈 나머지와 **items**는 6개이므로 6으로 나눈 나머지를 하나씩 선택하고 있다.\n",
    "그리고 컬럼명을 ```[\"name\",\"coffee\"]```으로 정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "자동으로 생성된 schema가 데이터타입을 잘 집어내고 있다.\n",
    "name, coffee 모두 string이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row 객체를 사용해서 생성\n",
    "\n",
    "* Row 생성\n",
    "**Row**를 사용해 보자.\n",
    "Row는 **이름(Column)이 붙여진 행**으로 **관계형데이터베이스 레코드 Record**와 같다. \n",
    "속성 명은 'year', 'name', 'height'로 명명한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row 속성 읽기\n",
    "\n",
    "속성명을 읽을 때에는 **```row.key```** 또는 Python dict형식으로 **```row[key]```**와 같이 속성을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"row1: \", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Row를 Dictionary로 저장\n",
    "\n",
    "Row는 속성명과 값을 가지고 있기 때문에 Dictionary로 쉽게 변환할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1.asDict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dictionary에서 키 또는 값을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1.asDict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row1.asDict().values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Row에서 DataFrame 생성\n",
    "\n",
    "위에서 설정한 Row를 사용하여 DataFrame을 만들어 보자.\n",
    "**Python list에 Row를 넣어** 구성한다.\n",
    "첫번째는 앞서 만든 row1 객체를 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "printSchema()를 해보면, **데이터 타잎**은 string, long으로 Spark에서 **자동** 인식되었다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print (myDf.printSchema())\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema를 정의하고 생성\n",
    "\n",
    "모델 schema를 정하고, **데이터 타잎**을 정의해 DataFrame을 생성해 본다.\n",
    "**```StructType```**으로 구조체를 선언하고, 컬럼에 대해 **```StructField```**를 설정한다.\n",
    "* **컬럼**의 명칭\n",
    "* 앞서 소개했던 **데이터 타잎**\n",
    "* 마지막은 **NULL**이 허용되는지 여부\n",
    "\n",
    "```python\n",
    "StructType([\n",
    "    StructField(컬럼명, StringType(), True),\n",
    "    ...\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 **myRows**를 데이터로, **mySchema**에서는 컬럼 명과 데이터타잎을 정의하여 ```createDataFrame()```함수의 인자로 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 설정한 데이터타입으로 설정되어있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.2 RDD에서 생성하기\n",
    "\n",
    "RDD는 schema가 정해지지 않은 비구조적 데이터이다.\n",
    "이와 같이 **schema를 정의하지 않으면, Spark는 schema를 유추**하게 된다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 자동 인식\n",
    "\n",
    "RDD로부터 DataFrame을 생성할 수 있다. 이 경우 schema를 설정하지 않으면 자동으로 인식된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js',170), ('1','lee, sm', 175), ('2','lim, yg',180), ('2','lee',170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "myRdd = spark.sparkContext.parallelize(myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```toDF()```로 변환하거나 직접 ```createDataFrame()``` 함수를 사용하여 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDf=myRdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또는 RDD에서 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rddDf=spark.createDataFrame(myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rddDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Row를 사용\n",
    "\n",
    "학년year는 앞에서는 **string**으로 인식되었다. 이번 예제에서는 **형변환**을 해 본다.\n",
    "RDD의 ```map()``` 함수를 사용하여 각 속성을 읽고 ```int()``` 함수로 형변환을 한다.\n",
    "각 속성에 명칭, year, name, height를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "_myRdd=myRdd.map(lambda x:Row(year=int(x[0]), name=x[1], height=int(x[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf=spark.createDataFrame(_myRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_myDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```Row()```를 사용하여 RDD를 생성할 수도 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-d68bbfc5d2ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"js1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"js2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0m_myRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1=Row(name=\"js1\", age=10)\n",
    "r2=Row(name=\"js2\", age=20)\n",
    "_myRdd=spark.sparkContext.parallelize([r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_myRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "####  schema를 정의하고 생성\n",
    "\n",
    "앞서 보았듯이, schema를 정의하고 RDD에서 DataFrame을 생성할 수 있다.\n",
    "**```StructType```**을 선언하고,\n",
    "컬럼에 대해 **```StructField```**를 **컬럼명**, **데이터 타잎**, **NULL**이 허용되는지 여부를 설정한다.\n",
    "과거 버전에서는 컬럼명이 정렬되면서 age, name 순서가 변경되었다.\n",
    "현재는 **컬럼명을 정렬하지 않으므로, 순서대로** 아래와 같이 생성하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema=StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    #StructField(\"created\", TimestampType(), True)\n",
    "])\n",
    "_myDf=spark.createDataFrame(_myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "schema를 정해서 RDD로부터 DataFrame을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-fabdf4a8e43b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmyRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lee'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'park'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m70.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m schema = StructType([\n\u001b[1;32m      5\u001b[0m     \u001b[0mStructField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIntegerType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "myRdd=spark.sparkContext.parallelize([(1, 'kim', 50.0), (2, 'lee', 60.0), (3, 'park', 70.0)])\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "_myDf = spark.createDataFrame(myRdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e9bce26d4512>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_myDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_myDf' is not defined"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4.3 Pandas\n",
    "\n",
    "Spark Dataframe은 다른 프로그래밍 언어에서도 분석도구로 많이 사용되는 형식이다.\n",
    "엑셀 스프레드쉬트와 비슷하다. 또한 최근 많이 사용되는 **R**의 Dataframe이나 **Python Pandas**를 예로 들 수 있다.\n",
    "Spark와 Pandas의 Dataframe을 비교하면,\n",
    "**Pandas**는 데이터 양이 적은 경우, Spark는 분산처리할 수 있으므로 빅데이터에 보다 적합하다.\n",
    "API를 사용하게 되면 Spark Dataframe과 Pandas 간에는 차이가 있다.\n",
    "\n",
    "구분 | DataFrame | Pandas\n",
    "-------|-------|-------\n",
    "csv 파일 읽기 | read.json() | read_csv()\n",
    "데이터타입 | inferschema=True 설정하면 추정 | 모두 strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe을 Pandas로 변환\n",
    "\n",
    "Spark Dataframe을 ```toPandas()``` 함수를 사용하여 **Pandas로 변환**할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-dfc6316a0daa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Pandas에서 csv 쓰기\n",
    "\n",
    "Dataframe을 **csv**파일로 내보내고 Pandas로 읽어보자.\n",
    "\n",
    "Dataframe을 csv로 쓰려면 라이브러리 ```com.databricks.spark.csv```를 사용해야 한다. **파일이 아니라 디렉토리가 생성**되고 그 안에 파일로 쓰여지게 된다. Pandas를 사용하면 우리가 보통 사용하는 하나의 파일로 쓰여진다.\n",
    "\n",
    "한 번 생성이 되면 덮어쓰기를 하지 않기 때문에, 다시 write() 할 경우에는 이전 디렉토리를 삭제하도록 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d17b7ff91a7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'_myDf.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "myDf.write.format('com.databricks.spark.csv').save(os.path.join('data','_myDf.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l data/_myDf.csv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Pandas를 이용하여 Dataframe을 csv파일로 내보낼 수 있다.\n",
    "\n",
    "```python\n",
    ",year,name,height\n",
    "0,1,\"kim, js\",170\n",
    "1,1,\"lee, sm\",175\n",
    "2,2,\"lim, yg\",180\n",
    "3,2,lee,170\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-6ae6dc5d698f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'myDf.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.toPandas().to_csv(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas에서 컬럼을 생성,삭제 해보자.\n",
    "recode - 현재 변수 값을 다시 줄 경우\n",
    "\n",
    "나라변 국제전화코드는 Japan: 81, South Korea: 82, Hong Kong: 852, Australia: 61을 사용한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# json 포멧으로 만든다.\n",
    "icc = pd.DataFrame( { 'country': ['South Korea','Japan','Hong Kong'],'codes': [81, 82, 852] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Japan</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hong Kong</td>\n",
       "      <td>852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81\n",
       "1        Japan     82\n",
       "2    Hong Kong    852"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전화코드가 81인 경우 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>codes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>South Korea</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       country  codes\n",
       "0  South Korea     81"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "icc[icc['codes']==81]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.4 csv 파일에서 생성\n",
    "\n",
    "#### RDD에서 DataFrame\n",
    "\n",
    "앞서 RDD에서 읽었던 csv파일을 다시 읽어보자.\n",
    "\n",
    "```sparkContext.textFile()``` 함수로 읽은 파일은 RDD이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-36f67f758e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcfile\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ds_spark_2cols.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "cfile= os.path.join(\"data\", \"ds_spark_2cols.csv\")\n",
    "lines = spark.sparkContext.textFile(cfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD에서 ```Row()```를 사용하여 Dataframe으로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_col12 = lines.map(lambda l: l.split(\",\"))\n",
    "\n",
    "\n",
    "# strip 특수문자 기호, 공백 같은거 제거해줌\n",
    "col12 = _col12.map(lambda p: Row(col1=int(p[0].strip()), col2=int(p[1].strip())))\n",
    "\n",
    "_myDf = spark.createDataFrame(col12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "_myDf.printSchema()\n",
    "_myDf.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame으로 직접 읽기\n",
    "\n",
    "format().load() 또는 csv() 함수로 csv 파일을 읽어서 DataFrame을 만들 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# databrics 는 마우스 D&D firestore 에 넣거나 put 명령어 사용해라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark.csv\n",
    "1,2,3,4\n",
    "11,22,33,44\n",
    "111,222,333,444"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### format load\n",
    "\n",
    "csv 패키지를 사용해서 읽어 본다. 우선 [Spark의 csv 패키지를 추가한다.](https://spark-packages.org/package/databricks/spark-csv) 패키지는 설정파일 ```spark-defaults.conf```에 추가할 수 있다.\n",
    "\n",
    "```python\n",
    "$ vim conf/spark-defaults.conf\n",
    "spark.jars.packages=com.databricks:spark-csv_2.11:1.5.0\n",
    "```\n",
    "\n",
    "```format(\"csv\").load(\"path\")```이라고 하고, options() 설정을 미리 넣을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "inferschema를 제외하면, string으로 자동인식한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', delimiter=',')\\\n",
    "        .load(os.path.join('data','ds_spark.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### csv\n",
    "\n",
    "또는 \n",
    "```csv(\"path\")```로 직접 DataFrame으로 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = spark\\\n",
    "        .read\\\n",
    "        .options(header='true', inferschema='true', delimiter=',')\\\n",
    "        .csv(os.path.join('data', 'ds_spark.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.5 tsv 파일 읽기\n",
    "\n",
    "tsv (tab-separated values)는 **Tab으로 분리된 파일**을 말한다.\n",
    "'\\t'이 포함되어 있는 경우, 혹시 string으로 데이터타잎을 설정하기도 한다 (과거 Spark 버전에서)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TAB은 whitespace이므로 그냥 split()을 해도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.array([float(x) for x in '1.658985\t4.285136'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "URL로 가서 http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_020108_HeightsWeights\n",
    "마우스로 긁어서 50행만 복사해 보자.\n",
    "18세 1993년 18세 이하 225,000 건의 키 (in), 몸무게 (lbs) 데이터이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data/ds_spark_heightweight.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load data/ds_spark_heightweight.txt\n",
    "1\t65.78\t112.99\n",
    "2\t71.52\t136.49\n",
    "3\t69.40\t153.03\n",
    "4\t68.22\t142.34\n",
    "5\t67.79\t144.30\n",
    "6\t68.70\t123.30\n",
    "7\t69.80\t141.49\n",
    "8\t70.01\t136.46\n",
    "9\t67.90\t112.37\n",
    "10\t66.78\t120.67\n",
    "11\t66.49\t127.45\n",
    "12\t67.62\t114.14\n",
    "13\t68.30\t125.61\n",
    "14\t67.12\t122.46\n",
    "15\t68.28\t116.09\n",
    "16\t71.09\t140.00\n",
    "17\t66.46\t129.50\n",
    "18\t68.65\t142.97\n",
    "19\t71.23\t137.90\n",
    "20\t67.13\t124.04\n",
    "21\t67.83\t141.28\n",
    "22\t68.88\t143.54\n",
    "23\t63.48\t97.90\n",
    "24\t68.42\t129.50\n",
    "25\t67.63\t141.85\n",
    "26\t67.21\t129.72\n",
    "27\t70.84\t142.42\n",
    "28\t67.49\t131.55\n",
    "29\t66.53\t108.33\n",
    "30\t65.44\t113.89\n",
    "31\t69.52\t103.30\n",
    "32\t65.81\t120.75\n",
    "33\t67.82\t125.79\n",
    "34\t70.60\t136.22\n",
    "35\t71.80\t140.10\n",
    "36\t69.21\t128.75\n",
    "37\t66.80\t141.80\n",
    "38\t67.66\t121.23\n",
    "39\t67.81\t131.35\n",
    "40\t64.05\t106.71\n",
    "41\t68.57\t124.36\n",
    "42\t65.18\t124.86\n",
    "43\t69.66\t139.67\n",
    "44\t67.97\t137.37\n",
    "45\t65.98\t106.45\n",
    "46\t68.67\t128.76\n",
    "47\t66.88\t145.68\n",
    "48\t67.70\t116.82\n",
    "49\t69.82\t143.62\n",
    "50\t69.09\t134.93\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### RDD\n",
    "\n",
    "우선 RDD로 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "_tRdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "tsv는 '\\t'로 분리해도 되고, split() 함수는 <TAB>을 포함한 whitespace로 분할하게 되므로 그냥 두어도 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tRdd=rdd.map(lambda x:x.split('\\t'))\n",
    "_tRddSplitted = _tRdd.map(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 형변환\n",
    "\n",
    "위 tsv 파일에서 생성한 RDD를 탭으로 분리하면서, ```float()```로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#myRdd=rdd.map(lambda line:np.array([float(x) for x in line.split('\\t')]))\n",
    "tRdd=_tRdd.map(lambda line:[float(x) for x in line.split('\\t')])\n",
    "tRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### schema 설정\n",
    "\n",
    "schema를 **자동으로 설정하면, string**으로 읽혀진다.\n",
    "schema를 설정한다고 해도, string -> integer, double로 형변환은 이루어지지 않는다.\n",
    "string의 **형변환을 명시적**으로 해주어야 한다.\n",
    "\n",
    "```python\n",
    "mySchema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"weight\", DoubleType(), True),\n",
    "    StructField(\"height\", DoubleType(), True)\n",
    "])\n",
    "myDf=spark.createDataFrame(myRdd, mySchema)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### DataFrame 생성\n",
    "\n",
    "위 tRdd로부터 컬럼명을 주어 DataFrame을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDfNamed = spark.createDataFrame(tRdd, [\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDfNamed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tDfNamed.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼을 split으로 분할\n",
    "\n",
    "text() 함수를 이용해서 파일을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt = spark.read.text(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\\<TAB>\" 으로 분리되지 못해 전체를 변수명 value, 타입은 string으로 읽고 있어, 이를 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDftxt.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "pyspark.sql.functions은 함수이므로, import할 경우\n",
    "\n",
    "* ```import pyspark.sql.functions.split``` 이렇게 하지 않고, \n",
    "* ```from pyspark.sql.functions import split``` 이렇게 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "split_col = split(tDftxt['value'], '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "분리된 컬럼은 getItem() 함수로 가져와서 각 각 weight, height 컬럼이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_col.getItem(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumn 컬럼을 하나 생성해다오\n",
    "tDftxt = tDftxt.withColumn('weight', split_col.getItem(1))\n",
    "tDftxt = tDftxt.withColumn('height', split_col.getItem(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tDftxt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### csv 함수로 tsv 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3| 69.4|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79| 144.3|\n",
      "|  6| 68.7| 123.3|\n",
      "|  7| 69.8|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9| 67.9|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13| 68.3|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09| 140.0|\n",
      "| 17|66.46| 129.5|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23| 137.9|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.4.6 JSON 파일에서 생성\n",
    "\n",
    "#### JSON\n",
    "JSON은 JavaScript Object Notation, 즉 자바스크립트에서 사용되는 표기법. 사람이 읽을 수 있는 텍스트로 표기하며, key-value 쌍으로 되어 있다.\n",
    "현재 널리 쓰이고 있어 XML 대용으로 널리 쓰이고 있다.\n",
    "Spark example 폴더에 있는 ```people.json``` JSON 파일이다.\n",
    "\n",
    "```python\n",
    "{\"name\":\"Michael\"}\n",
    "{\"name\":\"Andy\", \"age\":30}\n",
    "{\"name\":\"Justin\", \"age\":19}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 트위터 데이터\n",
    "\n",
    "트위터 데이터는 JSON 형식을 가지고 있다.\n",
    "아래는 트윗 1개의 샘플이다.\n",
    "실제 트윗 데이터를 구할 수 없다면 아래 샘플을 파일로 저장한 후 사용하면 된다.\n",
    "\n",
    "```python\n",
    "{\"contributors\": null, \"truncated\": false, \"text\": \"RT @soompi: #SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fi…\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 801657325836763136, \"favorite_count\": 0, \"entities\": {\"symbols\": [], \"user_mentions\": [{\"id\": 17659206, \"indices\": [3, 10], \"id_str\": \"17659206\", \"screen_name\": \"soompi\", \"name\": \"Soompi\"}], \"hashtags\": [{\"indices\": [12, 22], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [100, 123], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"http://twitter.com/download/android\\\" rel=\\\"nofollow\\\">Twitter for Android</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"801657325836763136\", \"favorited\": false, \"retweeted_status\": {\"contributors\": null, \"truncated\": false, \"text\": \"#SEVENTEEN’s Mingyu, Jin Se Yeon, And Leeteuk To MC For 2016 Super Seoul Dream Concert \\nhttps://t.co/1XRSaRBbE0 https://t.co/fifXHpF8or\", \"is_quote_status\": false, \"in_reply_to_status_id\": null, \"id\": 800593781586132993, \"favorite_count\": 1649, \"entities\": {\"symbols\": [], \"user_mentions\": [], \"hashtags\": [{\"indices\": [0, 10], \"text\": \"SEVENTEEN\"}], \"urls\": [{\"url\": \"https://t.co/1XRSaRBbE0\", \"indices\": [88, 111], \"expanded_url\": \"http://www.soompi.com/2016/11/20/seventeens-mingyu-jin-se-yeon-leeteuk-mc-dream-concert/\", \"display_url\": \"soompi.com/2016/11/20/sev…\"}], \"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"retweeted\": false, \"coordinates\": null, \"source\": \"<a href=\\\"https://about.twitter.com/products/tweetdeck\\\" rel=\\\"nofollow\\\">TweetDeck</a>\", \"in_reply_to_screen_name\": null, \"in_reply_to_user_id\": null, \"retweet_count\": 1487, \"id_str\": \"800593781586132993\", \"favorited\": false, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": true, \"profile_use_background_image\": true, \"default_profile_image\": false, \"id\": 17659206, \"profile_background_image_url_https\": \"https://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"verified\": true, \"translator_type\": \"none\", \"profile_text_color\": \"999999\", \"profile_image_url_https\": \"https://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"profile_sidebar_fill_color\": \"000000\", \"entities\": {\"url\": {\"urls\": [{\"url\": \"http://t.co/3evT80UlR9\", \"indices\": [0, 22], \"expanded_url\": \"http://www.soompi.com\", \"display_url\": \"soompi.com\"}]}, \"description\": {\"urls\": []}}, \"followers_count\": 987867, \"profile_sidebar_border_color\": \"000000\", \"id_str\": \"17659206\", \"profile_background_color\": \"1E1E1E\", \"listed_count\": 3982, \"is_translation_enabled\": true, \"utc_offset\": -28800, \"statuses_count\": 80038, \"description\": \"The original K-pop community. We take gifs, OTPs, and reporting on your bias' fashion choices seriously. But not rumors. Ain't nobody got time for that.\", \"friends_count\": 3532, \"location\": \"Worldwide\", \"profile_link_color\": \"31B6F4\", \"profile_image_url\": \"http://pbs.twimg.com/profile_images/792117259489583104/4khJk3zz_normal.jpg\", \"following\": false, \"geo_enabled\": false, \"profile_banner_url\": \"https://pbs.twimg.com/profile_banners/17659206/1478803767\", \"profile_background_image_url\": \"http://pbs.twimg.com/profile_background_images/699864769/1cdde0a85f5c0a994ae1fb06d545a5ec.png\", \"screen_name\": \"soompi\", \"lang\": \"en\", \"profile_background_tile\": true, \"favourites_count\": 1493, \"name\": \"Soompi\", \"notifications\": false, \"url\": \"http://t.co/3evT80UlR9\", \"created_at\": \"Wed Nov 26 20:48:27 +0000 2008\", \"contributors_enabled\": false, \"time_zone\": \"Pacific Time (US & Canada)\", \"protected\": false, \"default_profile\": false, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Mon Nov 21 06:56:46 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"extended_entities\": {\"media\": [{\"expanded_url\": \"https://twitter.com/soompi/status/800593781586132993/photo/1\", \"display_url\": \"pic.twitter.com/fifXHpF8or\", \"url\": \"https://t.co/fifXHpF8or\", \"media_url_https\": \"https://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\", \"id_str\": \"800593115165798400\", \"sizes\": {\"small\": {\"h\": 382, \"resize\": \"fit\", \"w\": 680}, \"large\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"medium\": {\"h\": 449, \"resize\": \"fit\", \"w\": 800}, \"thumb\": {\"h\": 150, \"resize\": \"crop\", \"w\": 150}}, \"indices\": [112, 135], \"type\": \"photo\", \"id\": 800593115165798400, \"media_url\": \"http://pbs.twimg.com/media/CxxHMk8UsAA4cUT.jpg\"}]}, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}, \"user\": {\"follow_request_sent\": false, \"has_extended_profile\": false, \"profile_use_background_image\": true, \"default_profile_image\": true, \"id\": 791090169818521600, \"profile_background_image_url_https\": null, \"verified\": false, \"translator_type\": \"none\", \"profile_text_color\": \"333333\", \"profile_image_url_https\": \"https://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"profile_sidebar_fill_color\": \"DDEEF6\", \"entities\": {\"description\": {\"urls\": []}}, \"followers_count\": 0, \"profile_sidebar_border_color\": \"C0DEED\", \"id_str\": \"791090169818521600\", \"profile_background_color\": \"F5F8FA\", \"listed_count\": 0, \"is_translation_enabled\": false, \"utc_offset\": null, \"statuses_count\": 96, \"description\": \"\", \"friends_count\": 7, \"location\": \"\", \"profile_link_color\": \"1DA1F2\", \"profile_image_url\": \"http://abs.twimg.com/sticky/default_profile_images/default_profile_6_normal.png\", \"following\": false, \"geo_enabled\": false, \"profile_background_image_url\": null, \"screen_name\": \"enriquesanq\", \"lang\": \"es\", \"profile_background_tile\": false, \"favourites_count\": 161, \"name\": \"Enrique santos\", \"notifications\": false, \"url\": null, \"created_at\": \"Wed Oct 26 01:32:49 +0000 2016\", \"contributors_enabled\": false, \"time_zone\": null, \"protected\": false, \"default_profile\": true, \"is_translator\": false}, \"geo\": null, \"in_reply_to_user_id_str\": null, \"possibly_sensitive\": false, \"lang\": \"en\", \"created_at\": \"Thu Nov 24 05:22:55 +0000 2016\", \"in_reply_to_status_id_str\": null, \"place\": null, \"metadata\": {\"iso_language_code\": \"en\", \"result_type\": \"recent\"}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파일에서 트윗 읽기\n",
    "\n",
    "그러나 JSON파일은 **JSON이 아니라 문자열**이다. 파일에서 읽은 후 **JSON으로 변환**을 해야 한다.\n",
    "트윗은 '\\n'으로 하나씩 구분되어 있고 ```readlines()``` 함수로 전체를 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_jfname=os.path.join('data','ds_twitter_seoul_3.json')\n",
    "with open(_jfname, 'rb') as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_json_str = json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pandas에서  트윗 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 Tweet데이터는 json형식이다. 따라서 list 구조로 만들어 주려면, 앞 뒤로 대괄호 ```[ ]```를 넣고, 각 tweet은 컴마로 연결한다.\n",
    "```join()``` 함수는 인자를 구분자 \",\"로 병합한다.\n",
    "\n",
    "```python\n",
    "\",\".join([\"A\", \"B\", \"C\"]) # A,B,C\n",
    "```\n",
    "\n",
    "data는 bytes이고, 이를 join()하는 함수는 str을 대상으로 하기 때문에, 문자열에 ```b```를 붙여 bytes로 변환한 후 연결한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-1cece6c7cb6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_json_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"[\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mb','\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34mb\"]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data_json_str = b\"[\" + b','.join(data) + b\"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아직 JSON으로 변환이 되지 않았고, 문자열 전체 길이를 알아 보자. 파일의 크기를 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 Pandas로 읽어 보자. Pandas 라이브러리에서 제공하는 read_json()함수를 사용한다.\n",
    "\n",
    "read_json()에 경로를 넘겨주어도 되지 않나??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tweetPd = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shape()은 행과 열의 갯수를 알려준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetPd.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "count() 함수는 행의 갯수를 나타내는데, 숫자가 서로 다른 것은 비워있는 경우가 서로 다르기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetPd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2ef281468f5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtweetPd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetPd' is not defined"
     ]
    }
   ],
   "source": [
    "print (tweetPd.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼 'id'를 10개만 읽어 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetPd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-9d55ceb3b713>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetPd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetPd' is not defined"
     ]
    }
   ],
   "source": [
    "tweetPd['id'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame에서 트윗 읽기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-0d54aa4801b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mjfile\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ds_twitter_seoul_3.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtweetDf\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "jfile= os.path.join('data','ds_twitter_seoul_3.json')\n",
    "\n",
    "tweetDf= spark.read.json(jfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweetDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-57e00b3bd0eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetDf' is not defined"
     ]
    }
   ],
   "source": [
    "tweetDf.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DataFrame에서도 위 Pandas와 컬럼 'id'를 10개 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweetDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-67c7ab3e06cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtweetDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tweetDf' is not defined"
     ]
    }
   ],
   "source": [
    "tweetDf.select('id', 'text').show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 월드컵 데이터 JSON의 URL에서 DataFrame 생성\n",
    "\n",
    "url에서 데이터를 직접 읽어 DataFrame을 생성하는 방법은 지원되지 않고 있다.\n",
    "requests 라이브러리를 사용하여, url에서 데이터를 가져오고, DataFrame을 생성하여 보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### URL에서 JSON 읽기\n",
    "\n",
    "웹에서 읽는 JSON은 문자열이라는 점에 유의한다. 따라서 json()함수로 변환하는 것이 필요하다.\n",
    "```python\n",
    "[\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    \"Year\": 1930,\n",
    "    \"Team\": \"Argentina\",\n",
    "    \"Number\": \"\",\n",
    "    \"Position\": \"GK\",\n",
    "    \"FullName\": \"Ãngel Bossio\",\n",
    "    \"Club\": \"Club AtlÃ©tico Talleres de Remedios de Escalada\",\n",
    "    \"ClubCountry\": \"Argentina\",\n",
    "    \"DateOfBirth\": \"1905-5-5\",\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  {\n",
    "    \"Competition\": \"World Cup\",\n",
    "    ...\n",
    "    \"IsCaptain\": false\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requuest.get() 함수는 Response를 반환한다.\n",
    "이러한 Reponse에 포함된 텍스트를 사용하게 된다.\n",
    "웹에서 읽은 텍스트는 어떤 자료구조를 가졌다고 하더라도 단순 문자열이다. 텍스트를 살펴보고, 구조가 있다면 알맞게 변환해주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Response:  <class 'requests.models.Response'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Type of Response: \", type(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 읽은 텍스트를 json으로 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc=r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print (type(wc), type(wc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wc는 아래와 같이 ```[ {...}, {...}, ...]```, 즉 배열내에 dictionary가 컴마로 연결되어 있다.\n",
    "이와 같이 파일에서 읽을 경우, 그 구조를 정확하게 이해하는 것이 중요하다.\n",
    "\n",
    "```python\n",
    "[{'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Ãngel Bossio',\n",
    "  'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1905-5-5',\n",
    "  'IsCaptain': False},\n",
    " {'Competition': 'World Cup',\n",
    "  'Year': 1930,\n",
    "  'Team': 'Argentina',\n",
    "  'Number': '',\n",
    "  'Position': 'GK',\n",
    "  'FullName': 'Juan Botasso',\n",
    "  'Club': 'Quilmes AtlÃ©tico Club',\n",
    "  'ClubCountry': 'Argentina',\n",
    "  'DateOfBirth': '1908-10-23',\n",
    "  'IsCaptain': False},\n",
    " ... ]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Competition': 'World Cup',\n",
       " 'Year': 1930,\n",
       " 'Team': 'Argentina',\n",
       " 'Number': '',\n",
       " 'Position': 'GK',\n",
       " 'FullName': 'Ãngel Bossio',\n",
       " 'Club': 'Club AtlÃ©tico Talleres de Remedios de Escalada',\n",
       " 'ClubCountry': 'Argentina',\n",
       " 'DateOfBirth': '1905-5-5',\n",
       " 'IsCaptain': False}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### DataFrame 생성\n",
    "\n",
    "위 ```wc```는 JSON을 포함하는 리스트이다.\n",
    "JSON은 key, value로 구성되어, 컬럼명을 key에서 가져올 수 있다.\n",
    "이전 버전에서는 가능했으나, 현재는 Python dict로부터 DataFrame을 생성하려면 아래와 같이 pyspark.sql.Row를 활용해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kangjuyoung/Downloads/spark-2.0.0-bin-hadoop2.7/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1da61ec11b73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIG_IGN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preexec_fn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kangjuyoung/Downloads/spark-2.0.0-bin-hadoop2.7/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "config = pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master('local')\\\n",
    "    .appName('my')\\\n",
    "    .config(conf = config)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "_wcDf=spark.createDataFrame(wc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row를 이용해서 Dictionary에서 DataFrame 생성하기\n",
    "\n",
    "앞서 살펴보았듯이, wc는 리스트이고 그 요소는 dictionary이다.\n",
    "dictionary 구조를 풀어서 Row에 넘겨주어야 한다.\n",
    "우선 Python에서 어떻게 인자를 풀어서 넘겨주는지 배워보자.\n",
    "\n",
    "인자가 복수일 경우 **리스트, dictionary와 같은 구조**로 묶게 되는데, **함수 호출하면서 풀어야할 필요가 있을 경우에 별표 연산자** ```*args```, ```**args```를 사용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 별표 1개 ```*```: **리스트**에서 인자 풀어내기\n",
    "\n",
    "예를 들어, range() 함수는 시작과 끝을 인자로 가지는 함수이다.\n",
    "시작과 끝이라는 2개의 인자를 함수에 넘겨주어 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myList = [1, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음과 같이 정수인자 자리에 리스트를 넘겨주면 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(myList))  # TypeError: 'list' object cannot be interpreted as an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range(*myList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시작, 끝 변수가 따로 없을 경우, **리스트를 넘겨주고 여기서 풀어서 하나씩 인자를 사용할 경우** ```*```를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(*myList))  # unpack args and get 1 for the start and 6 for the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 예를 보자. f()는 복수의 인자를 받아서 하나씩 출력하는 함수이다.\n",
    "f()를 호출할 때 여러 개를 넘겨주면, 1개만 받으므로 타입오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(args):\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "복수의 인자를 받게 될 것이라면 * 연산자를 사용한다. 그러면 복수의 인자를 풀어서 사용하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(*args):\n",
    "    print(args)\n",
    "    print(type(args))\n",
    "    for i in args:\n",
    "        print(i, end=\"~\")\n",
    "\n",
    "f(0, 1, 2, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 별표 2 개 ```**```: **dictionary**에서 인자 풀어내기\n",
    "\n",
    "dictionary를 넘겨주고 여기서 key, value를 하나씩 사용할 경우 ```**```를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printCapital(name, year):\n",
    "    print(f\"{name} in {year}\")\n",
    "\n",
    "myDict = {\"name\": \"jsl\", \"year\": 2020}\n",
    "\n",
    "printCapital(**myDict)\n",
    "\n",
    "\n",
    "def test2(**test):\n",
    "    print(f\"{test}\")\n",
    "\n",
    "    \n",
    "myDict2 = {\"age\": 12, \"count\": 0}\n",
    "\n",
    "test2(myDict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(range(1, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary인자를 풀어서 Row에 넘겨주기\n",
    "\n",
    "반복문으로 하나의 dictionary를 가져와 ```Row()```를 생성하고 있다.\n",
    "별표 2개를 겹쳐 쓴 ```**```는 dictionary를 풀어서 Row 생성자에 넘겨주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 잘 읽혔는지, 일부를 출력해보자. 아래에서 보듯이, 아르헨티나 문자가 유니코드로 잘 출력이 되고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "###  DataFrame의 shema 설정\n",
    "\n",
    "wcRdd에서 DataFrame을 생성하려고 하면 아래와 같이 schema를 설정할 수 있다.\n",
    "그러나 영어가 아닌 다른 나라의 유니코드 문자 또는 생년월일의 결측값 등으로 오류가 발생한다.\n",
    "자동으로 인식한 Schema가 더 낫게 인식하고 있다.\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import *\n",
    "wcSchema=StructType([\n",
    "    StructField(\"Club\", StringType(), True),\n",
    "    StructField(\"ClubCountry\", StringType(), True),\n",
    "    StructField(\"Competition\", StringType(), True),\n",
    "    StructField(\"DateOfBirth\", DateType(), True),\n",
    "    StructField(\"FullName\", StringType(), True),\n",
    "    StructField(\"IsCaptain\", BooleanType(), True),\n",
    "    StructField(\"Number\", IntegerType(), True),\n",
    "    StructField(\"Position\", StringType(), True),\n",
    "    StructField(\"Team\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType(), True)\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### RDD 생성\n",
    "\n",
    "RDD를 통해서 DataFrame 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd=spark.sparkContext.parallelize(wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcRdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDfFromRdd = spark.createDataFrame(wcRdd)\n",
    "wcDfFromRdd.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wcDfFromRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측 값\n",
    "\n",
    "\n",
    "* ```null```은 결측, 즉 \"no value\" 또는 \"nothing\"을 말한다.\n",
    "* ```NaN```은 \"Not a Number\", 즉 수학에서 0.0/0.0과 같이 의미가 없는 연산의 결과를 말한다.\n",
    "\n",
    "```IsCaptain``` 항목은 boolean 타입이라 isnan() 함수가 오류를 발생한다. ```'isnan(`IsCaptain`)' due to data type mismatch: argument 1 requires (double or float) type, however, '`IsCaptain`' is of boolean type.```\n",
    "그래서 항목에서 제거하고 확인하기로 하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = wcDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불리언이라 isnan(${c}), col(${c}).isNull() 할 수 없어서 제거 \n",
    "cols.remove('IsCaptain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "wcDf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cols]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 형변환\n",
    "\n",
    "스키마를 자동으로 유추한 형식을 보면,\n",
    "DateOfBirth, Number 컬럼이 문자열로 인식되어 있어 만족스럽지 못하다.\n",
    "컬럼 'DateOfBirth'는 'DoB'로 ```DateType()```, 컬럼 'Number'는 'NumberInt'로 \"Integer\" 형으로 설정해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### ```DateType``` 형변환\n",
    "\n",
    "* Python ```datetime```을 사용한 DateType 컬럼 생성\n",
    "\n",
    "Python ```datetime```은 날짜 문자열을 다음과 같은 년, 월, 일 형식으로 변환해준다.\n",
    "\n",
    "표시 | 설명               |예\n",
    "---|--------------------|--------------------\n",
    "%d | 숫자로 표현한 2자리 일자 | 01, 02, ..., 31\n",
    "%m | 숫자로 표현한 2자리 월  | 01, 02, ..., 12\n",
    "%y | 숫자로 표현한 2자리 년수 (세기 불포함) | 00, 01, ..., 99\n",
    "%Y | 숫자로 표현한 4자리 년수 (세기 포함)  | 0001, 0001, ..., 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print (datetime.strptime(\"11/25/1991\", '%m/%d/%Y'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "datetime을 사용해서 일자를 DateType()으로 변환해보자.\n",
    "이 때 사용자정의함수 udf()를 만들어 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# date 함수\n",
    "toDate = udf(lambda x: datetime.strptime(x, '%m/%d/%Y'), DateType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn()은 컬럼을 추가하는 명령어이다. 방금 만든 udf 함수를 호출해서 형변환을 하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.withColumn('date1', toDate(wcDf['DateOfBirth']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 결과를 출력하면 아래와 같이 ```ValueError```가 출력된다.\n",
    "\n",
    "```python\n",
    "wcDf.take(1)\n",
    "\n",
    "ValueError: time data '1905-5-5' does not match format '%m/%d/%Y'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```to_date()``` 함수\n",
    "\n",
    "이번에는 pyspark에서 제공하는 to_date() 함수를 사용하자.\n",
    "방금 오류가 발생한 컬럼은 제거하고 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf = wcDf.drop('date1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to_date() 함수는 string (StringType)을 date (DateType) 으로 형변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# 포맷까지 줘야함\n",
    "_wcDfCasted=wcDf.withColumn('date2', to_date(wcDf['DateOfBirth'], 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* pyspark ```cast()``` 함수\n",
    "\n",
    "pyspark의 cast() 함수로 형변환을 해보자.\n",
    "DateOfBirth와 더불어 Number 컬럼도 같이 Integer로 형변환을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wcDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "wcDfCasted = _wcDfCasted.withColumn('date3', _wcDfCasted['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "wcDfCasted.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그대로 출력하면 오류가 발생할 수 있다.\n",
    "```SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0```라는 오류가 발생하면\n",
    "**```spark.sql.legacy.timeParserPolicy=LEGACY```**라고 설정을 변경해주어야 한다.\n",
    "\n",
    "아래와 같이 to_data() 또는 cast() 함수를 사용하여 일자 형변환 결과를 올바르게 출력하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드 정리\n",
    "\n",
    "지금까지 작성한 코드는 **상호대화형**으로 작성되었다.\n",
    "배우거나 연습하기 위해 작성된 코드가 포함되었기 때문에 기능이 중복된 코드가 있을 수 있다.\n",
    "\n",
    "프로그램을 하는 **목적**이 무엇인지를 생각하고, **필요한 코드**만 남겨두자.\n",
    "URL에서 JSON 데이터를 읽어 DataFrame을 생성하고, 한 줄 출력하는 코드만 필요하다.\n",
    "일괄실행하기 위해서는 spark를 생성하는 코드가 있어야 한다.\n",
    "정리하면서 라이브러리는 앞 부분으로 위치시킨다.\n",
    "주석도 코드를 이해하는데 도움이 되므로 넣어준다.\n",
    "프로그램 절차에 맞추어 수정이 필요한 부분도 있다. 예를 들어 wcDfCasted는 wcDf로 변경하였다.\n",
    "\n",
    "아래 프로그램에 main() 함수를 넣어서 정리하면 더욱 좋겠다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- v 붙여넣기\n",
    "- c 복사\n",
    "- shift + m 머지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/kangjuyoung/Downloads/spark-2.0.0-bin-hadoop2.7/./bin/spark-submit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-eda7b638bac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmyConf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkSession\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/session.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    226\u001b[0m                             \u001b[0msparkConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                         \u001b[0;31m# This SparkContext may be an existing one.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparkConf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m                     \u001b[0;31m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                     \u001b[0;31m# by all sessions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36mgetOrCreate\u001b[0;34m(cls, conf)\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgateway\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlaunch_gateway\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m                 \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gateway\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjvm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/java_gateway.py\u001b[0m in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIGINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSIG_IGN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'preexec_fn'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreexec_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m                 \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpopen_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;31m# preexec_fn not supported on Windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    949\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m    952\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@3.9/3.9.5/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1819\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1820\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1821\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1822\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/kangjuyoung/Downloads/spark-2.0.0-bin-hadoop2.7/./bin/spark-submit'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pyspark\n",
    "# os.environ[\"PYSPARK_PYTHON\"]=\"/usr/bin/python3\"\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"/usr/bin/python3\"\n",
    "\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# read url json\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "# read dictionary into Row\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)\n",
    "\n",
    "# cast DoB string into date, Number string into integer\n",
    "wcDfCasted = wcDf.withColumn('date3', wcDf['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.7 Parquet 파일 읽기, 쓰기\n",
    "\n",
    "Parquet(파케이)는 나무조각을 붙여넣은 마룻바닥이라는 뜻으로, Apache Hadoop에서는 컬럼별로 저장하는 데이터 압축형식으로, DataFrame에서 이 파일을 쓰거나, 읽을 수 있다. \n",
    "\n",
    "```python\n",
    "-rw-r--r-- 1 jsl jsl 522 10월  7 15:27 part-r-00000-0318688b-018f-4e55-858b-b4b78ac56532.snappy.parquet\n",
    "-rw-r--r-- 1 jsl jsl   0 10월  7 15:27 _SUCCESS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 사용했던 ```_myDf```를 parquet형식으로 저장해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.write.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 쓰여진 parquet으로부터 읽어서 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pDf=spark.read.parquet(os.path.join(\"data\",\"people.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_pDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 DataFrame API 사용해 보기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 빈 DataFrame 생성\n",
    "\n",
    "비어있는 DataFrame을 생성하려면, 빈 schema를 설정하고 emptyRdd()를 사용해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-153-b83125227ea2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStructType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0memptyDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memptyRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType\n",
    "\n",
    "schema = StructType([])\n",
    "emptyDf = spark.createDataFrame(spark.sparkContext.emptyRDD(), schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emptyDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### range\n",
    "\n",
    "비어 있는 DataFrame보다는 일련의 수를 가진 DataFrame을 만들어 보자.\n",
    "```range(start, end=None, setp=1, numPartitions=None)``` 함수는 Python range() 함수와 같이 정수를 생성하고, 컬럼명 id를 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(0, 10, 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 추가 withColumn, 컬럼 삭제 Drop\n",
    "\n",
    "```withColumn()```은 열을 추가한다.\n",
    "앞서 사용하였던 키, 몸무게 파일에서 DataFrame을 생성하고 컬럼을 생성해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-d22733da0828>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'false'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ds_spark_heightweight.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼은 자동 생성이 되었으므로, ```'_c0', '_c1', '_c2'```이고 사용하기 불편하므로 의미있는 컬럼으로 변경하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존에 있는 ```_1```행을 ```integer```로 형변환해서 ```id```행을 만들고, 기존의 ```_1```행을 삭제해보자.\n",
    "```drop()```은 열을 삭제할 때 사용한다. 같은 방식으로 나머지도 변경해보자.\n",
    "컬럼은 점연산자 tDf._c0 또는 인덱스 tDf['_c1'] 어느 방식이나 모두 가능하다.\n",
    "\n",
    "cast() 함수에는 \"integer\" 또는 Spark의 데이터타입 IntegerType()이라고 적어도 된다.\n",
    "또는 statement로 적어도 된다.\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "tDf.withColumn(\"id\", F.expr(\"CAST(id AS INTEGER)\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-151-9e915a49fe68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"integer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"height\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_c1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weight\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_c2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"double\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tDf' is not defined"
     ]
    }
   ],
   "source": [
    "tDf = tDf.withColumn(\"id\", tDf._c0.cast(\"integer\"))\n",
    "tDf = tDf.withColumn(\"height\", tDf['_c1'].cast(\"double\"))\n",
    "tDf = tDf.withColumn(\"weight\", tDf['_c2'].cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼이 중복되었으므로, 삭제하자. 하나씩 또는 다음과 같이 한꺼번에 삭제할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.drop('_c0').drop('_c1').drop('_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "형변환이 잘 되었고 컬럼명이 변경된 결과를 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사용자정의 함수 udf\n",
    "\n",
    "UDF User Defined Functions는 사용자 정의함수로서 DataFrame의 withColumn() 함수와 같이 사용되어 새로운 컬럼을만드는 경우 유용하다.\n",
    "보통 함수와 같이 함수명과 반환 값을 미리 정의해서 lambda 함수 또는 다른 함수를 사용할 수 있다.\n",
    "**다른 함수를 직접 사용할 수 없고, udf()를 통해서 호출**해야 한다.\n",
    "udf()는 **코드가 복잡한 경우에 함수를 분리해서 처리**하면 유용하겠다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 Pandas로 저장한 csv파일을 읽어서 DataFrame을 만들어 보자.\n",
    "header를 있는 그대로 가져오고,\n",
    "schema도 현재 데이터에서 추정하도록 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myDf = spark\\\n",
    "        .read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferschema='true')\\\n",
    "        .load(os.path.join('data','myDf.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```header='true'```라고 설정을 해서, 컬럼명은 있는 그대로 year, name, height로 읽는다.\n",
    "printSchema()를 출력해보면, year는 정수, name은 문자열, height는 정수로 정의되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+\n",
      "|_c0|year|   name|height|\n",
      "+---+----+-------+------+\n",
      "|  0|   1|kim, js|   170|\n",
      "|  1|   1|lee, sm|   175|\n",
      "|  2|   2|lim, yg|   180|\n",
      "|  3|   2|    lee|   170|\n",
      "+---+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 대문자 변환 withColumn\n",
    "\n",
    "대문자로 변환하고 반환하는 함수를 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uppercase(s):\n",
    "    return s.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uppercase(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의된 함수를 바로 호출하면 오류가 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-85e35a83822c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nameUpper\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-cc3ea1335364>\u001b[0m in \u001b[0;36muppercase\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muppercase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", uppercase(myDf.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "udf()를 통해 함수를 호출해야 한다.\n",
    "반환값의 타잎을 ```StringType()```으로 정의하고 있다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "upperUdf = udf(uppercase, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 정의한 udf() 함수를 withColumn()과 같이 사용하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = myDf.withColumn(\"nameUpper\", upperUdf(myDf['name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+\n",
      "|_c0|year|   name|height|nameUpper|\n",
      "+---+----+-------+------+---------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|\n",
      "|  3|   2|    lee|   170|      LEE|\n",
      "+---+----+-------+------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 Double변환 withColumn\n",
    "\n",
    "udf() 함수에 lambda를 사용해도 된다.\n",
    "**withColumn()에서 다른 함수를 호출하지 못하고, udf() 함수를 통해** 호출한다.\n",
    "\n",
    "앞서 배운 ```withColumn()```에서 udf를 호출해서 ```height``` 컬럼을 ```long```에서 ```DoubleType()```으로 만들어 보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "toDoublefunc = udf(lambda x: float(x), DoubleType())\n",
    "myDf = myDf.withColumn(\"heightD\", toDoublefunc(myDf.height))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```dtypes```로 컬럼명과 데이터타입을 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'int'),\n",
       " ('year', 'int'),\n",
       " ('name', 'string'),\n",
       " ('height', 'int'),\n",
       " ('nameUpper', 'string'),\n",
       " ('heightD', 'double')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 정수변환 withColumn\n",
    "\n",
    "year의 문자열을 정수로 형변환 해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import udf, struct\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "toint=udf(lambda x:int(x), IntegerType())\n",
    "myDf=myDf.withColumn(\"yearI\", toint(myDf['year']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      " |-- nameUpper: string (nullable = true)\n",
      " |-- heightD: double (nullable = true)\n",
      " |-- yearI: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "yearI 컬럼명만 선택하여 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|yearI|\n",
      "+-----+\n",
      "|    1|\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('yearI').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### udf함수로 조건에 따른 withColumn\n",
    "\n",
    "udf()함수를 사용하여, 조건에 따라 컬럼을 생성해보자.\n",
    "키를 175를 기준으로 이분화하고 반환값을 ```StringType()```으로 정의해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "height_udf = udf(lambda height: \"taller\" if height >=175 else \"shorter\", StringType())\n",
    "heightDf=myDf.withColumn(\"height>175\", height_udf(myDf.heightD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|height>175|\n",
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|   shorter|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    taller|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    taller|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|   shorter|\n",
      "+---+----+-------+------+---------+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경 withColumnRenamed\n",
    "\n",
    "앞서 ```withCoumun()``` 명령어로 열을 추가해 보았다. ```withColumnRenamed()``` 함수는 컬럼명을 변경한다. 컬럼명을 ```name```에서 ```full name```으로 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0a36f263b970>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ID'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tDf' is not defined"
     ]
    }
   ],
   "source": [
    "tDf=tDf.withColumnRenamed('id','ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7744383385d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tDf' is not defined"
     ]
    }
   ],
   "source": [
    "tDf.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 그래프\n",
    "\n",
    "#### plot\n",
    "\n",
    "Spark에는 그래프를 그리는 기능이 없다.\n",
    "Python matplotlib을 이용해서 그래프를 표현한다.\n",
    "2차원 ```plot()```을 하기 위해서는, x와 y축 값이 필요하다.\n",
    "DataFrame -> RDD로 변환하고, map() 함수를 사용하여 배열로부터 weight, height를 분리해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|_c0|  _c1|   _c2|\n",
      "+---+-----+------+\n",
      "|  1|65.78|112.99|\n",
      "|  2|71.52|136.49|\n",
      "|  3| 69.4|153.03|\n",
      "|  4|68.22|142.34|\n",
      "|  5|67.79| 144.3|\n",
      "|  6| 68.7| 123.3|\n",
      "|  7| 69.8|141.49|\n",
      "|  8|70.01|136.46|\n",
      "|  9| 67.9|112.37|\n",
      "| 10|66.78|120.67|\n",
      "| 11|66.49|127.45|\n",
      "| 12|67.62|114.14|\n",
      "| 13| 68.3|125.61|\n",
      "| 14|67.12|122.46|\n",
      "| 15|68.28|116.09|\n",
      "| 16|71.09| 140.0|\n",
      "| 17|66.46| 129.5|\n",
      "| 18|68.65|142.97|\n",
      "| 19|71.23| 137.9|\n",
      "| 20|67.13|124.04|\n",
      "+---+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf = spark\\\n",
    "    .read\\\n",
    "    .options(header='false', inferschema='true', delimiter='\\t')\\\n",
    "    .csv(os.path.join('data', 'ds_spark_heightweight.txt'))\n",
    "tDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65.78,\n",
       " 71.52,\n",
       " 69.4,\n",
       " 68.22,\n",
       " 67.79,\n",
       " 68.7,\n",
       " 69.8,\n",
       " 70.01,\n",
       " 67.9,\n",
       " 66.78,\n",
       " 66.49,\n",
       " 67.62,\n",
       " 68.3,\n",
       " 67.12,\n",
       " 68.28,\n",
       " 71.09,\n",
       " 66.46,\n",
       " 68.65,\n",
       " 71.23,\n",
       " 67.13,\n",
       " 67.83,\n",
       " 68.88,\n",
       " 63.48,\n",
       " 68.42,\n",
       " 67.63,\n",
       " 67.21,\n",
       " 70.84,\n",
       " 67.49,\n",
       " 66.53,\n",
       " 65.44,\n",
       " 69.52,\n",
       " 65.81,\n",
       " 67.82,\n",
       " 70.6,\n",
       " 71.8,\n",
       " 69.21,\n",
       " 66.8,\n",
       " 67.66,\n",
       " 67.81,\n",
       " 64.05,\n",
       " 68.57,\n",
       " 65.18,\n",
       " 69.66,\n",
       " 67.97,\n",
       " 65.98,\n",
       " 68.67,\n",
       " 66.88,\n",
       " 67.7,\n",
       " 69.82,\n",
       " 69.09]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_weightRdd=tDf.rdd.map(lambda fields:fields[1]).collect()\n",
    "_heightRdd=tDf.rdd.map(lambda fields:fields[2]).collect()\n",
    "_weightRdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x, y 값이 잘 추출되었는지 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[65.78 71.52 69.4  68.22 67.79]\n",
      "[112.99 136.49 153.03 142.34 144.3 ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print (np.array(_weightRdd)[:5])\n",
    "print (np.array(_heightRdd)[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 추출한 height, weight RDD를 numpy array를 사용해서 2차원 x, y로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAWV0lEQVR4nO3df5BdZX3H8c+nUJnq1IDsipQNBhZMB5gMgS3BzABpmAGkjlHbOoiMP8c0lJhRbFVkRhwdO7YWHJlo0igUHCQOUq0UpYSRIXGIhFlE1oBJ2QVMNgLZlBptsVjw2z/2bLh79+79ee49P+77NbOTe597c+6zD+F7n/N9vuc5jggBAMrl97LuAAAgfQR3ACghgjsAlBDBHQBKiOAOACV0eNYdkKSBgYFYtGhR1t0AgEJ56KGHDkTEYK3XchHcFy1apNHR0ay7AQCFYvvn871GWgYASojgDgAlRHAHgBIiuANACRHcAaCECO4A5ti4dULbJw7Mats+cUAbt05k1CO0iuAOYI4lQwu09taHDwX47RMHtPbWh7VkaEHGPUOzclHnDiBflg8PaP2lS7X21od12bLjdcuOPVp/6VItHx7IumtoEjN3ADUtHx7QZcuO1/X3juuyZccT2AuG4A6gpu0TB3TLjj1at/Ik3bJjz5wcPPKN4A5gjpkc+/pLl+rKCxYfStEQ4IuD4A5gjrHJg7Ny7DM5+LHJgxn3DM1yHu6hOjIyEmwcBgCtsf1QRIzUeo2ZOwCUEMEdAEqI4A4AJdQwuNu+0fZ+2zsr2j5te5/tnyQ/F1e8dpXtcdu7bV/YrY4DAObXzMz9JkkX1Wj/YkScnvx8X5JsnyLpEkmnJn/nK7YPS6uzAIDmNAzuEbFN0nNNHm+VpG9GxAsR8aSkcUlnddA/AEAbOsm5r7U9lqRtjkrajpO0t+I9k0nbHLZX2x61PTo1NdVBNwAA1doN7hskDUs6XdLTkq5t9QARsSkiRiJiZHCw5s27AQBtaiu4R8SzEfFSRPxO0lf1cupln6SFFW8dStoAAD3UVnC3fWzF07dJmqmkuUPSJbaPsH2CpJMlPdhZFwEArWq4n7vtzZJWSBqwPSnpGkkrbJ8uKSQ9JemvJCkiHrV9m6THJL0o6YqIeKkrPQcAzIu9ZQCgoNhbBgD6DMEdAEqI4A4AJURwB4ASIrgDQAkR3AGghAjuAFBCBHcUwsatE9o+cWBW2/aJA9q4dSKjHgH5RnBHISwZWqC1tz58KMBvnzigtbc+rCVDCzLuGZBPDbcfAPJg+fCA1l+6VGtvfViXLTtet+zYo/WXLtXy4YGsuwbkEjN3FMby4QFdtux4XX/vuC5bdjyBHaiD4I7C2D5xQLfs2KN1K0/SLTv2zMnBA3gZwR2FMJNjX3/pUl15weJDKZpeBngWdVEkBHcUwtjkwVk59pkc/NjkwZ71gUVdFAlb/gItmAnoeVzU3bh1QkuGFszqz/aJAxqbPKg15w33XT/6AVv+AinJ86JuXs4s8tKPfkdwB1qQ5aJuo5x/ZbnodVt2H1qj6PUXUF760e8I7kCTsl7UbWZGnJczi7z0o58R3IEmZb2o28yMOC/lonnpR6W+q3aKiMx/zjzzzADKbsN943H/+NSstvvHp2LDfeMtHefau3fF6z9+Z1x79645x1r6mS2HPqP6ea/kpR9F6VcnJI3GPHGVmTtQpVszvDQWGuvNiLM+s8hbP6r13VrAfFG/lz/M3FFLWjPdVnVzhjdzrGvv3tXyMcs488zCfGc+RSRm7iiirErqGs3wOpnZd7LQmNcZcbU857bzuBbQLQR35FaWp9H1gnAnXzqdBJc15w3P+d2XDw/k7sKgvNa5Z13t1HPzTel7+UNaBvVkcRrdKH3STnqln9IqnaSfuiWrNF83ibQMiiqL0+hmZnjtpFeKklZJQx7r3Ity5pMWgjtyK6vT6GaCcDtfOv0UXPopt51b803pZ34k3Shpv6SdNV77qKSQNJA8t6TrJY1LGpN0RqPjB2kZzCOvp9H9lF5pR5HHp1f/5tL6HHWYlrlJ0kXVjbYXSrpA0p6K5jdJOjn5WS1pQxvfN4Ck/M50+ym9Us98VTGbtj1R2PHp1WJwLz6nqS1/bS+SdGdEnFbRdrukz0r6rqSRiDhg+58k3RcRm5P37Ja0IiKernd8tvwFiqcybbZ8eGDO86Lq1bbOaXxO6lv+2l4laV9EPFL10nGS9lY8n0zaah1jte1R26NTU1PtdANAhsp6xWevFoO7/TktB3fbr5T0SUmf6uSDI2JTRIxExMjg4GAnhwKQkTxWxXSqV4vB3f6cdmbuw5JOkPSI7ackDUn6se3XSdonaWHFe4eSNgAlVLaqmF5VaPXic1oO7hHx04h4bUQsiohFmk69nBERz0i6Q9K7Pe1sSQcb5dsBFFMZr/js1WJ5Lz6n4YKq7c2SVkgakPSspGsi4oaK15/SywuqlrRe09U1z0t6X0Q0XCllQRUonn65V2qef896C6rcIBsA6shzVRA3yAa6LM87IaIzRa0KIrgDKcjrTohIRxGrgg7PugNAGVTO7rp98Qt6r7oq6Ozho3P/35aZO5CSIs7u0FhRq4II7kBKylbz3Q1FXJtopWwxT78fwR1IQVFnd73WrbWJbgbVVjawy9PaC8EdSAE7RTanW5UneQmqeaqsoc4dQM9dt2W3rr93XOtWnqQrL1icyjF7tZtjM7rx+9VCnTuA3OjW2kReFrTzsvZCcAf6RB4W+7q5NpGHoJqntReCO9An8pCX7tbaRF6Cap7WXsi5A30kT3npNOV5c69uqpdz5wpVoI9U5qXXrTypFIFdUs0Avnx4oDS/XztIywB9JA95afQGwR3oE3nJS6M3CO5An8jTYh+6jwVVACgoLmICgD5DcAeAEiK4A0AJEdwBoIQI7uhredhvBegGgjv6Wqv7rfBlgKIguKOvtXpzhTxsvgU0g71l0Pda2W+l8sugbJtvoVyYuaPvtbrfSl5uCgHUQ3BHX2tnvxU230IRNAzutm+0vd/2zoq2z9oes/0T21ts/1HSbtvX2x5PXj+jm50HOtXqfitsvoWiaLi3jO1zJf23pK9HxGlJ26sj4lfJ43WSTomINbYvlvQhSRdLWibpSxGxrFEn2FsGaevWzRv69aYQyKeO9paJiG2Snqtq+1XF01dJmvmGWKXpL4GIiAckHWn72Pa6jTLpdQlht6pa1pw3PCfHvnx4oOPATokl0tZ2zt3252zvlfQuSZ9Kmo+TtLfibZNJW62/v9r2qO3RqampdruBguh1CWGrJY5Zo8QSaWs7uEfE1RGxUNI3JK1t4+9vioiRiBgZHBxstxsoiCyCbZGqWpoZn7Rn95wtlFsa1TLfkPTnyeN9khZWvDaUtAE9D7ZFq2ppND5pz+45Wyi3toK77ZMrnq6StCt5fIekdydVM2dLOhgRT3fYR5REL4NtEataGo1P2mc/RUtdoUURUfdH0mZJT0v6P03n0D8g6V8k7ZQ0JunfJB2XvNeSvixpQtJPJY00On5E6MwzzwyU2/3jU7H0M1vi/vGpms/TtuG+8TnHvn98KjbcN96Vz+tUK+Nz7d274vUfvzOuvXtXKp+d9vHQO5JGY564ym320BOUENbX7PjMnJGktfVB2sdDb9UrhSS4o6cI8u2rTDUtHx6Y8zzr46H3uIcqcoNFvPa1ejVtr4+HfGHmjp4jFQCkg5k7cqVI9edAURHc0XNFqz8Hiojgjp4qYv05UEQEd/RU0RfxuGQfRUFwR091a1fFXkmr2ocvCXQbwR1oQVqX7FMSim4juBcMM77spVHtw74u6DaCe8Ew48teWtU+lISimwjuBcOML1tpVvs08yXBmRraRXAvIGZ8tfUiEKZV7dPslwRnamgXwb2AuAiotl4EwrSqfZr9kuBMDe1ib5mCYSe/+sq6b811W3br+nvHtW7lSbrygsVZdwc5wd4yJVL0i4C6rYwpq8ozta/+8El99YcTc14nB49qh2fdAbSm1un/8uGBUgSxNFSnrM4ePrrQY1N9ZvaHf3C4/u5703e1/OA5w7NeByoR3FEa1YHw7OGjC5+yqj5T++A501/u1215XL/+zYulSj0hXQR3lMambU/o8hUnzkpZXb7iRG3a9kRhg1+tM7UPnjOsX//mxUM5+KL+bugucu4ojdXnnqgN9z0xq1pmw31PaPW5J2bcs3RRLYVmMHNHaVSWDZatWmZGGVNP6A5m7iiVMlbLVKJaCs0iuKNU0kpZ5PWy/6JvmYzeIbijNNLc94XL/lF0BHcckqfZajt9STNlwWX/KDqCOw7J02y1nb6knbIoe/4e5Ua1DA7JU7VJHvpStqtd0V8aztxt32h7v+2dFW1fsL3L9pjt79g+suK1q2yP295t+8Iu9RtdkqfZapZ9STN/D2ShmbTMTZIuqmq7R9JpEbFE0n9IukqSbJ8i6RJJpyZ/5yu2D0utt+i6PF0gk2VfKDlE0TVMy0TENtuLqtq2VDx9QNJfJI9XSfpmRLwg6Unb45LOkvSjdLqLbsrTBTJZ94UN2lB0aSyovl/SXcnj4yTtrXhtMmmbw/Zq26O2R6emplLoBjqVp9lqnvoCFFFTN+tIZu53RsRpVe1XSxqR9PaICNvrJT0QEbckr98g6a6IuL3e8blZBwC0rt7NOtqulrH9XklvlnR+vPwNsU/Swoq3DSVtAIAeaistY/siSR+T9JaIeL7ipTskXWL7CNsnSDpZ0oOddxMA0IqGM3fbmyWtkDRge1LSNZqujjlC0j22pelUzJqIeNT2bZIek/SipCsi4qVudR4AUBs3yAaAguIG2QDQZwjuAFBCBHegjjztlAm0guAO1JGnnTKBVrArJFBHHnanBNrBzB1oIO3dKUn1oBcI7kADae9OSaoHvUBaBqijG7tTkupBLzBzB+ro1u6UebopCsqJmTtQR7f2decWfug2Zu5Aj3ELP/QCwR3oMW5Egl5g4zCgysatE1oytGBWmmT7xAGNTR6smaYBssLGYX2GOurOUKqIMiC4lxDBqTOVpYrXbdmd2U3CgU5QLVNC1FF3rrJUcd3Kkxg7FA4z95JqtY6aVM5saV+VCvQawb2kWg1OpHJeRqkiyoDgXkLtBCfyzC9rtlSRsx3kGcG9hNqto+aS+Glrzhue87svHx6YUwbJ2Q7yjDp3HDITnFiEbR5jhixR546GyDO3h7Md5BXBHZK4JL5dVNUgr0jLAG2q3uu9+jnQbaRlgC7gbAd5xswdAAqqo5m77Rtt77e9s6LtL20/avt3tkeq3n+V7XHbu21f2Hn3AQCtaiYtc5Oki6radkp6u6RtlY22T5F0iaRTk7/zFduHdd5NAEArGgb3iNgm6bmqtp9FxO4ab18l6ZsR8UJEPClpXNJZqfQUANC0tBdUj5O0t+L5ZNIGAOihzKplbK+2PWp7dGpqKqtuAEAppR3c90laWPF8KGmbIyI2RcRIRIwMDg6m3A0A6G9pB/c7JF1i+wjbJ0g6WdKDKX8GAKCBZkohN0v6kaTFtidtf8D222xPSnqjpO/ZvluSIuJRSbdJekzSv0u6IiJe6l73UURslQt0XzPVMu+MiGMj4vcjYigiboiI7ySPj4iIYyLiwor3fy4ihiNicUTc1d3uo4jYKhfoPu6hip7jHq9A97G3DDLBVrlAdxHckQm2ygW6i+CeYJGvd7gxCNB9BPcEi3zzS/uLj61yge5jy98K3A+zNm5KAeRTvS1/qZapULnIt27lSQSuBNUtQPGQlqnAIt/8qG4BioXgnmCRrz6++IBiIbgnWOSbH198QPGwoIqGNm6d0JKhBbNSMdsnDmhs8qDWnDecYc+A/lZvQZXgDgAF1dENsgEAxUNwB4ASIrgDQAkR3AGghAjuABpiY73iIbgDaIiN9YqHvWUANMT+QsXDzB1AU9hfqFgI7gCawv5CxUJwB9AQ+wsVD8EdQENsrFc87C0DAAXF3jIA0GcI7gBQQgR3ACihhsHd9o2299veWdH2Gtv32H48+fOopN22r7c9bnvM9hnd7DwAoLZmZu43Sbqoqu0Tkn4QESdL+kHyXJLeJOnk5Ge1pA3pdBMA0IqGwT0itkl6rqp5laSbk8c3S3prRfvXY9oDko60fWxKfQUANKndnPsxEfF08vgZScckj4+TtLfifZNJ2xy2V9setT06NTXVZjcAALV0vKAa04XyLRfLR8SmiBiJiJHBwcFOuwEAqNBucH92Jt2S/Lk/ad8naWHF+4aSNgBAD7Ub3O+Q9J7k8Xskfbei/d1J1czZkg5WpG8AAD3ScD9325slrZA0YHtS0jWSPi/pNtsfkPRzSe9I3v59SRdLGpf0vKT3daHPAIAGGgb3iHjnPC+dX+O9IemKTjvVyMatE1oytGDWftLbJw5obPKg1pw33O2PB4DcK+QVqtzyCwDqK+Rt9rjlFwDUV8iZu8QtvwCgnsIGd275BQDzK2Rw55ZfAFBfIYM7t/wCgPq4zR4AFBS32QOAPkNwB4ASIrgDQAkR3AGghAjuAFBCuaiWsT2l6d0lszYgiWL5xhinxhijxhijxhqN0esjoubdjnIR3PPC9uh8ZUV4GePUGGPUGGPUWCdjRFoGAEqI4A4AJURwn21T1h0oCMapMcaoMcaosbbHiJw7AJQQM3cAKCGCOwCUUF8Hd9tH2r7d9i7bP7P9xorXPmo7bPf1LZ7mGyPbH0raHrX9D1n3M0u1xsj26bYfsP0T26O2z8q6n1mxvTgZh5mfX9n+sO3X2L7H9uPJn0dl3dcs1RmnLyT/tsZsf8f2kU0dr59z7rZvlvTDiPia7VdIemVE/NL2Qklfk/THks6MiL690KLWGElaKulqSX8WES/Yfm1E7M+0oxmaZ4xuk/TFiLjL9sWSPhYRK7LsZx7YPkzSPknLJF0h6bmI+LztT0g6KiI+nmkHc6JqnBZLujciXrT995LUzDj17czd9gJJ50q6QZIi4rcR8cvk5S9K+pik/v3mU90xulzS5yPihaS9nwP7fGMUkl6dvG2BpF9k0sH8OV/SRET8XNIqSTcn7TdLemtWncqhQ+MUEVsi4sWk/QFJQ80coG+Du6QTJE1J+mfbD9v+mu1X2V4laV9EPJJx//Kg5hhJeoOkc2zvsL3V9p9k281MzTdGH5b0Bdt7Jf2jpKsy7GOeXCJpc/L4mIh4Onn8jKRjsulSLlWOU6X3S7qrmQP0c3A/XNIZkjZExFJJ/yPp05I+KelTGfYrT2qN0SeS9tdIOlvS30q6zbYz62W25hujyyV9JCIWSvqIkpl9P0tSVm+R9K3q12I6P9zXZ8oz5hsn21dLelHSN5o5Tj8H90lJkxGxI3l+u6b/Jz1B0iO2n9L06c+Pbb8umy5mbr4xmpT07Zj2oKTfaXqDo3403xi9R9K3k7ZvSerbBdUKb5L044h4Nnn+rO1jJSn5s2/Te1Wqx0m23yvpzZLeFU0ulPZtcI+IZyTttb04aTpf0wP62ohYFBGLNP0/7hnJe/vOPGP0mKR/lfSnkmT7DZJeoT7d3a/OGP1C0nlJ20pJj2fQvbx5p2anGu7Q9Jegkj+/2/Me5dOscbJ9kabXAN8SEc83e5B+r5Y5XdNVMa+Q9ISk90XEf1W8/pSkkT6vljldVWOk6dTDjZJOl/RbSX8TEfdm1MXMzTNGp0r6kqbTNv8r6a8j4qGs+pi1ZB1ij6QTI+Jg0na0pquKjtf0lt/viIjnsutl9uYZp3FJR0j6z+RtD0TEmobH6ufgDgBl1bdpGQAoM4I7AJQQwR0ASojgDgAlRHAHgBIiuANACRHcAaCE/h9UCQhTvHpIXAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(np.array(_weightRdd), np.array(_heightRdd),'x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### boxplot, violinplot\n",
    "\n",
    "하나의 컬럼만 선택해서 그래프를 그려보자.\n",
    "* boxplot의 가운데 박스 IQR (Interquartile Range)은 4분위 값의 2, 3번째와 50% 값, 즉 25%~75%의 구간을 의미한다.\n",
    "그리고 위 아래 가로선은 IQR 값의 1.5배되는 구역을 나타낸다. 값의 분포와 outlier를 확인하기 편리하다.\n",
    "* violin plot 역시 boxplot과 유사한 기능이지만, 밀도를 보여주고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tDf = tDf.withColumnRenamed('_c2', 'height')\n",
    "\n",
    "height = tDf.select(\"height\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>128.842000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.888889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>97.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>120.870000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>129.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>140.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>153.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           height\n",
       "count   50.000000\n",
       "mean   128.842000\n",
       "std     12.888889\n",
       "min     97.900000\n",
       "25%    120.870000\n",
       "50%    129.500000\n",
       "75%    140.075000\n",
       "max    153.030000"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax1 = fig.add_subplot(1, 2, 1)  # subplot location\n",
    "ax1 = plt.boxplot(height)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2 = sns.violinplot(data=height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aggregate functions\n",
    "\n",
    "avg, collect_list, countDistinct, count, kurtosis, max, min, mean, skewness, stddev, sum, variance 등의 함수가 지원된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dictionary 형식\n",
    "\n",
    "agg() 함수에 dictionary 형식으로 ```컬렴명: aggregate functions```으로 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|count(height)|\n",
      "+-------------+\n",
      "|           50|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|   kurtosis(height)|\n",
      "+-------------------+\n",
      "|-0.6127861004407515|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"kurtosis\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|avg(height)|\n",
      "+-----------+\n",
      "|    128.842|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tDf.agg({\"height\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(height)|\n",
      "+-----------+\n",
      "|       97.9|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "tDf.agg(F.min(\"height\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 컬럼 조회 select\n",
    "\n",
    "컬럼은 아래와 같이 ```select()```로 선택할 수 있다.\n",
    "컬럼은 그 명칭을 사용하거나 ```myDf.name```, 인덱스 ```myDf['name']```로 선택할 수 있다.\n",
    "\n",
    "컬럼 선택 | 예제 | 권고\n",
    "-----|-----|-----\n",
    "점 연산자로 컬럼을 선택 | myDf.name | N\n",
    "인덱스로 컬럼을 선택 | myDf['name'] | Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 컬럼명으로 직접 조회 못해\n",
    "\n",
    "컬럼명만 아래와 같이 적어주어도 조회할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'name'>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf['name']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 컬럼 데이터를 조회하기 위해 컬럼에 show(), collect() 함수를 사용해서는 안된다. ```select()```를 사용해서 컬럼을 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Column' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-2fdc8d3e858e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'Column' object is not callable"
     ]
    }
   ],
   "source": [
    "myDf['name'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼 조회"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|kim, js|\n",
      "|lee, sm|\n",
      "|lim, yg|\n",
      "|    lee|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name')\n",
    "_name.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여러 컬럼을 조회하기 위해서는 해당 컬럼을 넣어주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_name=myDf.select('name', 'height').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 리스트를 넣어주어 여러 컬럼을 선택할 수도 있다.\n",
    "**리스트를 풀어야 하므로, 앞서 배웠던 ```*``` 연산자**를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|lee, sm|   175|\n",
      "|lim, yg|   180|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['name', 'height']\n",
    "myDf.select(*cols).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 컬럼을 List로 변환\n",
    "\n",
    "select() 함수는 Row()로 구성된 컬럼을 선택하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-5f580f753f1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.select('name').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List로 변환하려면, map() 함수로는 가능하지 않다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ce6fdc5ce5cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2차원 Row List를 제거하기 위해서 인덱스를 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는  flatMap()으로 2차원 구조 Row List를 1차원 List로 변환한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kim, js', 'lee, sm', 'lim, yg', 'lee']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.select('name').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select like\n",
    "\n",
    "```%``` 연산자는 0 또는 그 이상의 문자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------+\n",
      "|   name|height|name LIKE %lee%|\n",
      "+-------+------+---------------+\n",
      "|kim, js|   170|          false|\n",
      "|lee, sm|   175|           true|\n",
      "|lim, yg|   180|          false|\n",
      "|    lee|   170|           true|\n",
      "+-------+------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.like(\"%lee%\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select startswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---------------------+\n",
      "|   name|height|startswith(name, kim)|\n",
      "+-------+------+---------------------+\n",
      "|kim, js|   170|                 true|\n",
      "|lee, sm|   175|                false|\n",
      "|lim, yg|   180|                false|\n",
      "|    lee|   170|                false|\n",
      "+-------+------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.startswith(\"kim\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### select endswith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-222ce78d5e90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"height\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lee\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.select(\"name\", \"height\", myDf.name.endswith(\"lee\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### alias\n",
    "\n",
    "이름을 변경할 경우 alias() 함수를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame의 명칭을 변경해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf1 = myDf.alias(\"myDf1\")\n",
    "myDf1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컬럼의 명칭을 변경해보자.\n",
    "그러기 위해서는 우선 컬럼을 **```select()```** 함수로 골라낸 후, **```alias```**로 **컬럼명**을 정할 수 있다.\n",
    "name 컬럼을 **```substr```**으로 1,3문자를 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|short name|\n",
      "+----------+\n",
      "|       kim|\n",
      "|       lee|\n",
      "|       lim|\n",
      "+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf1.select(myDf1.name.substr(1,3).alias(\"short name\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택 select, when, otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------+\n",
      "|height|CASE WHEN (height < 175) THEN 1 ELSE 0 END|\n",
      "+------+------------------------------------------+\n",
      "|   170|                                         1|\n",
      "|   175|                                         0|\n",
      "|   180|                                         0|\n",
      "|   170|                                         1|\n",
      "+------+------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", when(myDf.height < 175, 1).otherwise(0)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alias 명령어로 컬럼을 변경해 줄 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|height|<175|\n",
      "+------+----+\n",
      "|   170|   1|\n",
      "|   175|   0|\n",
      "|   180|   0|\n",
      "|   170|   1|\n",
      "+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "myDf.select(\"height\", (when(myDf.height < 175, 1).otherwise(0)).alias('<175')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또는 0, 1이 아닌 문자열로 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "_myDf=myDf.select(when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\").alias(\"how tall\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|how tall|\n",
      "+--------+\n",
      "|    <175|\n",
      "|    <175|\n",
      "|    >175|\n",
      "|    <175|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "withColumn() 함수를 사용하면, DataFrame에 컬럼을 추가하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|how tall|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|    <175|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|    <175|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|    >175|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|    <175|\n",
      "+---+----+-------+------+---------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.withColumn('how tall', when(myDf['heightD'] >175.0, \">175\").otherwise(\"<175\"))\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 행과 열을 선택  where, select\n",
    "\n",
    "\n",
    "DataFrame은 관계형데이터베이스의 테이블과 매우 유사하다. **SQL 명령**을 사용하듯이 ```where()```, ```select()```, ```groupby()``` 함수를 사용할 수 있다.\n",
    "\n",
    "* 행: ```where()```에 따라 컬럼의 **조건에 맞는 행을 선택**하고 (filter),\n",
    "* 열: 앞서 배운 ```select()```로 열을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|height|\n",
      "+-------+------+\n",
      "|kim, js|   170|\n",
      "|    lee|   170|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.where(myDf['height'] < 175)\\\n",
    "    .select(myDf['name'], myDf['height']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filter\n",
    "\n",
    "```filter()```는 조건에 따라 데이터를 걸러낸다.\n",
    "앞서 배웠던 ```where()```와 유사한 기능을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.filter(myDf['height'] > 175).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### regexp_replace 컬럼의 내용 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|nameNew|\n",
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|kim, js|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|lim, sm|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|lim, yg|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|    lim|\n",
      "+---+----+-------+------+---------+-------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "_heightDf = myDf.withColumn('nameNew', regexp_replace('name', 'lee', 'lim'))\n",
    "_heightDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy\n",
    "\n",
    "컬럼을 학년에 따라,\n",
    "```groupBy()```하면 아래와 같이 데이터만 집단화하게 된다.\n",
    "집단화하면 개수를 세거나, 합계를 내거나 어떤 통계량을 계산이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x143a6e490>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.groupby(myDf['year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy하고 max\n",
    "\n",
    "컬럼을 기준으로 구분지어서 평균, 합계, 갯수, 최대, 최소 등을 구할 수 있다.\n",
    "첫 컬럼 학년을 ```groupby()```해서 최대값 ```max()```를 구해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+---------+-----------+------------+----------+\n",
      "|year|max(_c0)|max(year)|max(height)|max(heightD)|max(yearI)|\n",
      "+----+--------+---------+-----------+------------+----------+\n",
      "|   1|       1|        1|        175|       175.0|         1|\n",
      "|   2|       3|        2|        180|       180.0|         2|\n",
      "+----+--------+---------+-----------+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupby(myDf['year']).max().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy, agg\n",
    "\n",
    "```agg()```는 합계 함수를 계산할 수 있으며, 지원하는 함수는 ```avg, max, min, sum, count```이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```year``` 컬럼에 대해 ```agg()``` 함수로 계산할 수 있다.\n",
    "\n",
    "dictionary 형식으로 key는 컬럼명, value는 합계 함수를 적어준다.\n",
    "예를 들어 {\"heightD\":\"avg\"}에서 \"heightD\"는 컬럼명, \"avg\"는 합계함수이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+\n",
      "|year|avg(heightD)|\n",
      "+----+------------+\n",
      "|   1|       172.5|\n",
      "|   2|       175.0|\n",
      "+----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('year').agg({\"heightD\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 인원수\n",
    "\n",
    "월드컵 데이터를 groupBy 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Competition='World Cup', Year=1930, Team='Argentina', Number='', Position='GK', FullName='Ãngel Bossio', Club='Club AtlÃ©tico Talleres de Remedios de Escalada', ClubCountry='Argentina', DateOfBirth='1905-5-5', IsCaptain=False, date3=datetime.date(1905, 5, 5), NumberInt=None)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import Row \n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# read url json\n",
    "r=requests.get(\"https://raw.githubusercontent.com/jokecamp/FootballData/master/World%20Cups/all-world-cup-players.json\")\n",
    "wc=r.json()\n",
    "\n",
    "# read dictionary into Row\n",
    "wcDf = spark.createDataFrame(Row(**x) for x in wc)\n",
    "\n",
    "# cast DoB string into date, Number string into integer\n",
    "wcDfCasted = wcDf.withColumn('date3', wcDf['DateOfBirth'].cast(DateType()))\n",
    "wcDfCasted = wcDfCasted.withColumn('NumberInt', wcDfCasted['Number'].cast(\"integer\"))\n",
    "\n",
    "wcDfCasted.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|ClubCountry|count|\n",
      "+-----------+-----+\n",
      "|   England |    4|\n",
      "|   Paraguay|   93|\n",
      "|     Russia|   51|\n",
      "|        POL|   11|\n",
      "|        BRA|   27|\n",
      "|    Senegal|    1|\n",
      "|     Sweden|  154|\n",
      "|   Colombia|    1|\n",
      "|        FRA|  155|\n",
      "|        ALG|    8|\n",
      "|   England |    1|\n",
      "|       RUS |    1|\n",
      "|     Turkey|   65|\n",
      "|      Zaire|   22|\n",
      "|       Iraq|   22|\n",
      "|    Germany|  206|\n",
      "|        RSA|   16|\n",
      "|        ITA|  224|\n",
      "|        UKR|   38|\n",
      "|        GHA|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy(wcDf.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupBy 국가별 포지션별 인원수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+----+----+----+----+\n",
      "|ClubCountry|    |  DF|  FW|  GK|  MF|\n",
      "+-----------+----+----+----+----+----+\n",
      "|   England |null|null|   2|null|   2|\n",
      "|   Paraguay|null|  26|  37|  10|  20|\n",
      "|     Russia|null|  20|  11|   4|  16|\n",
      "|        POL|null|   2|   2|   3|   4|\n",
      "|        BRA|null|   7|   5|   4|  11|\n",
      "|    Senegal|null|null|null|   1|null|\n",
      "|     Sweden|null|  40|  47|  25|  42|\n",
      "|   Colombia|null|null|   1|null|null|\n",
      "|        ALG|null|   2|null|   6|null|\n",
      "|        FRA|null|  46|  41|  18|  50|\n",
      "|   England |null|null|null|null|   1|\n",
      "|       RUS |null|null|null|   1|null|\n",
      "|     Turkey|null|  20|  13|  12|  20|\n",
      "|      Zaire|null|   6|   5|   3|   8|\n",
      "|       Iraq|null|   6|   4|   3|   9|\n",
      "|    Germany|null|  64|  51|  16|  75|\n",
      "|        RSA|null|   5|   2|   3|   6|\n",
      "|        UKR|null|  13|   7|   4|  14|\n",
      "|        ITA|null|  74|  42|  19|  89|\n",
      "|        CMR|null|   1|   1|   1|null|\n",
      "+-----------+----+----+----+----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F 함수\n",
    "\n",
    "또는 아래와 같이 별도 ```pyspark.sql.functions```을 사용할 수 있다.\n",
    "앞서 pyspark.sql.functions은 함수이므로, ```from pyspark.sql.functions import split``` 이렇게 한다.\n",
    "또는 ```from pyspark.sql import functions as F```라고 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+------------+------------+\n",
      "|min(heightD)|max(heightD)|avg(heightD)|sum(heightD)|\n",
      "+------------+------------+------------+------------+\n",
      "|       170.0|       180.0|      173.75|       695.0|\n",
      "+------------+------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 행 추가\n",
    "\n",
    "행을 추가하려면, DataFrame을 서로 합치는 방법으로 가능하다.\n",
    "추가할 행으로 DataFrame을 만들고, union() 함수로 합쳐야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "createFrame() 함수에는 리스트를 넣어주어야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "toAppendDf = spark.createDataFrame([Row(4, 1, \"choi, js\", 177)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+------+---------+-------+-----+\n",
      "|_c0|year|   name|height|nameUpper|heightD|yearI|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "|  0|   1|kim, js|   170|  KIM, JS|  170.0|    1|\n",
      "|  1|   1|lee, sm|   175|  LEE, SM|  175.0|    1|\n",
      "|  2|   2|lim, yg|   180|  LIM, YG|  180.0|    2|\n",
      "|  3|   2|    lee|   170|      LEE|  170.0|    2|\n",
      "+---+----+-------+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf\n",
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 4 columns;\n'Union false, false\n:- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, heightD#128, <lambda>(year#17) AS yearI#136]\n:  +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#128]\n:     +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#120]\n:        +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#112]\n:           +- Project [_c0#16, year#17, name#18, height#19, uppercase(name#18) AS nameUpper#79]\n:              +- Project [_c0#16, year#17, name#18, height#19, uppercase(name#18) AS nameUpper#46]\n:                 +- Relation[_c0#16,year#17,name#18,height#19] csv\n+- LogicalRDD [_1#5283L, _2#5284L, _3#5285, _4#5286L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-dce48b789b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_myDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoAppendDf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36munion\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0mAlso\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstandard\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mSQL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0mresolves\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0mby\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mby\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \"\"\"\n\u001b[0;32m-> 1828\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 7 columns and the second table has 4 columns;\n'Union false, false\n:- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, heightD#128, <lambda>(year#17) AS yearI#136]\n:  +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#128]\n:     +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#120]\n:        +- Project [_c0#16, year#17, name#18, height#19, nameUpper#79, <lambda>(height#19) AS heightD#112]\n:           +- Project [_c0#16, year#17, name#18, height#19, uppercase(name#18) AS nameUpper#79]\n:              +- Project [_c0#16, year#17, name#18, height#19, uppercase(name#18) AS nameUpper#46]\n:                 +- Relation[_c0#16,year#17,name#18,height#19] csv\n+- LogicalRDD [_1#5283L, _2#5284L, _3#5285, _4#5286L], false\n"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.union(toAppendDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### partition 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-718e7d2f3461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### repartition\n",
    "\n",
    "repartition()은 partition의 개수를 늘리거나 줄이거나 재설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-eb050c726ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_myDf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_myDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "_myDf = myDf.repartition(4)\n",
    "print(_myDf.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### coalesce\n",
    "\n",
    "coalesce()는 partition을 **줄일 때** 사용한다.\n",
    "앞서 4개의 partition을 가진 ```_myDf```를 2로 줄여보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-c3e0f235b98f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_myDf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_myDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_myDf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_myDf' is not defined"
     ]
    }
   ],
   "source": [
    "_myDf2 = _myDf.coalesce(2)\n",
    "print(_myDf2.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 통계 요약 describe\n",
    "\n",
    "column이 연산가능한 데이터타잎인 경우, 요약 값을 볼 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "|summary|               _c0|              year|   name|            height|nameUpper|           heightD|             yearI|\n",
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "|  count|                 4|                 4|      4|                 4|        4|                 4|                 4|\n",
      "|   mean|               1.5|               1.5|   null|            173.75|     null|            173.75|               1.5|\n",
      "| stddev|1.2909944487358056|0.5773502691896257|   null|4.7871355387816905|     null|4.7871355387816905|0.5773502691896257|\n",
      "|    min|                 0|                 1|kim, js|               170|  KIM, JS|             170.0|                 1|\n",
      "|    max|                 3|                 2|lim, yg|               180|  LIM, YG|             180.0|                 2|\n",
      "+-------+------------------+------------------+-------+------------------+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값\n",
    "\n",
    "결측값을 채우는 함수이다.\n",
    "* df.na.fill(0) 모든 컬럼의 na를 0으로 교체\n",
    "* df.fillna( { 'c0':0, 'c1':0 } ) 컬럼 c0, c1의 na를 0으로 교체\n",
    "\n",
    "결측값을 삭제할 수도 있다.\n",
    "* df.na.drop(subset=[\"c0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: int, year: int, name: string, height: int, nameUpper: string, heightD: double, yearI: int]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.where(F.col(\"height\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-018866693d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0misnan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(isnan(c), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+------+---------+-------+-----+\n",
      "|_c0|year|name|height|nameUpper|heightD|yearI|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "|  0|   0|   0|     0|        0|      0|    0|\n",
      "+---+----+----+------+---------+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(col(c).isNull(), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문제: 년별 분기별 대여건수\n",
    "\n",
    "서울시 열린데이터 https://data.seoul.go.kr/ 에서 제공하는 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv```를 분석해보자.\n",
    "파일은 웹 검색을 해서 다운로드해서 사용하면 된다.\n",
    "데이터는 일자별로, 대여건수이이고, 몇 줄만 출력해보면 다음과 같다.\n",
    "\n",
    "|      date| count|\n",
    "|----------|------|\n",
    "|2018-01-01|  4950|\n",
    "|2018-01-02|  7136|\n",
    "|2018-01-03|  7156|\n",
    "|2018-01-04|  7102|\n",
    "|2018-01-05|  7705|\n",
    "\n",
    "### 문제 1-1: 년도별 대여건수 합계\n",
    "데이터는 2018, 2019년 15개월 간의 대여건수이다. 년도별로 대여건수의 합계를 계산해서 출력하자.\n",
    "\n",
    "|year|sum(count)|\n",
    "|----|----------|\n",
    "|2018|  10124874|\n",
    "|2019|   1871935|\n",
    "\n",
    "\n",
    "### 문제 1-2: 년도별, 월별 대여건수 합계\n",
    "년별, 월별로 대여건수를 계산하여 합계를 계산하여 출력한다.\n",
    "\n",
    "### 문제 1-3: 년도별, 월별 대여건수 그래프\n",
    "문제 1-2의 출력을 선 그래프로 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 읽기\n",
    "\n",
    "서울시 열린데이터에서 데이터 ```서울특별시_공공자전거 일별 대여건수_(2018~2019.03).csv``` 를 다운로드 받아서 저장한다.\n",
    "csv 형식으로 schema는 자동 인식하도록 읽는다.\n",
    "일자는 ```timestamp```로 건수는 ```integer```로 인식되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-3f60780186f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_bicycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'com.databricks.spark.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/seoulBicycleDailyCount_2018_201903.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "_bicycle = spark.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true').load('data/seoulBicycleDailyCount_2018_201903.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-6b22654652d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_bicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "_bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "전체 건수는 455건, 5건의 데이터만 읽어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-b506ccb14df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_bicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "_bicycle.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-19fd3f239055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_bicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "_bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼명 변경\n",
    "\n",
    "앞서 보듯이 파일을 읽으면서 컬럼명이 인식되었는데 \" count\"가 맨 앞에 공백이 하나 있게 되어 변경해보자.\n",
    "일단 붙여진 컬럼의 명칭을 변경하려면 ```withColumnRenamed()```를 연결하여 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-726ec305132e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_bicycle\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" count\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Count\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle=_bicycle\\\n",
    "    .withColumnRenamed(\"date\", \"Date\")\\\n",
    "    .withColumnRenamed(\" count\", \"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: substr\n",
    "\n",
    "```substr()``` 함수는 인자가 2개로서, 앞글자 '1'은 시작 '4'는 4글자를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-55c2da141dbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"year\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"year\",bicycle.Date.substr(1, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-72-07e5c9a7479c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"month\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"month\",bicycle.Date.substr(6, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 컬럼 만들기: F 함수\n",
    "\n",
    "함수를 이용해 년, 월, 일 등을 추출할 수 있다.\n",
    "먼저 앞서 생성된 column을 삭제하고 나서 해보자.\n",
    "여러 컬럼을 삭제하기 위해서는 ```*```를 앞에 붙여 준다.\n",
    "물론 하나씩 삭제할 수도 있고, 그러면 별표는 불필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-e1cead4cfa15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcolumns_to_drop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcolumns_to_drop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "columns_to_drop = ['year','month']\n",
    "df = bicycle.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year, month 컬럼이 삭제되고 Date, Count 컬럼만 남겨졌다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-09aa658342f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyspark.sql.functions의 year(), month() 함수를 사용하여 년, 월을 추출하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-1e9318244696>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbicycle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbicycle\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "bicycle = bicycle\\\n",
    "    .withColumn('year', F.year('date'))\\\n",
    "    .withColumn('month', F.month('date'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 substr() 결과와 비교해보자. year, month가 올바르게 추출되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-76-7796b56c9ccb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show()는 앞 부분 보여주고 있다. 월이 1만 보여서, 다른 월의 결과를 보고 싶다면 filter()해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-34642b6094f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.filter(bicycle.month == 2).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기\n",
    "\n",
    "1월 2월 3월은 1분기, 4~6은 2분기, 7~9는 3분기, 10~12는 4분기로 구분한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classifyQuarter(s):\n",
    "    q=\"\"\n",
    "    if 1<=s and s< 4:\n",
    "        q=\"Q1\"\n",
    "    elif 4<=s and s<7:\n",
    "        q=\"Q2\"\n",
    "    elif 7<=s and s<10:\n",
    "        q=\"Q3\"\n",
    "    elif 10<=s and s<=12:\n",
    "        q=\"Q4\"\n",
    "    else:\n",
    "        q=\"no\"\n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "quarter_udf = udf(classifyQuarter, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-8c93ea28a077>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quarter\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquarter_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle=bicycle.withColumn(\"quarter\", quarter_udf(bicycle.month))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잘 분류되었는지 건수를 확인해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-3165e422a010>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-4f316c8ed34b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 분기별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-74d0988a83dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-ec9778353088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'quarter'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"avg\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('quarter').agg({\"count\":\"avg\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년도별, 월별 (분기별) 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-85-9a936f34ae24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycle.groupBy('year').pivot('quarter').agg({\"count\":\"sum\"}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e285525a90c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mbicycleP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "bicycleP = bicycle.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas의 info() 함수는 DataFrame의 컬럼, 데이터타입 dtypes를 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-87-6296a280b58b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycleP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycleP' is not defined"
     ]
    }
   ],
   "source": [
    "bicycleP.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별 대여건수 합계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-83901dad68c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#bicycleP.groupby('year').aggregate({'Count':np.sum})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbicycleP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'Count'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycleP' is not defined"
     ]
    }
   ],
   "source": [
    "#bicycleP.groupby('year').aggregate({'Count':np.sum})\n",
    "bicycleP.groupby('year').aggregate({'Count':'sum'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 년도별, 월별 대여건수 합계\n",
    "\n",
    "index는 행, columns는 열 데이터를 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-822b81d179b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggfunc\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycleP' is not defined"
     ]
    }
   ],
   "source": [
    "pd.pivot_table(bicycleP, values = 'Count', index = ['year'], columns = ['month'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2018년만 선택해서 년도별 x 분기별 대여건수를 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-ea18de6f18c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycleP2018\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbicycleP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycleP' is not defined"
     ]
    }
   ],
   "source": [
    "bicycleP2018=bicycleP[bicycleP['year']==2018]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bicycleP2018byQ = pd.pivot_table(bicycleP2018, values = 'Count', index = ['year'], columns = ['quarter'], aggfunc= 'sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycleP2018byQ' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-525d0df7016a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbicycleP2018byQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycleP2018byQ' is not defined"
     ]
    }
   ],
   "source": [
    "bicycleP2018byQ.iloc[:,0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 년별 월별 대여건수 그래프\n",
    "\n",
    "앞장 RDD에서 만들어진 단어빈도는 리스트에 저장되었다. 따라서 리스트에서 데이터를 추출하여 그래프를 그렸다.\n",
    "groupBy에서 생성된 월별 대여건수는 pandas로 변환하여 그려보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 년별 월별 대여건수 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bicycle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-53d170a3d85f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msumMonthly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbicycle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'month'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"count\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bicycle' is not defined"
     ]
    }
   ],
   "source": [
    "sumMonthly=bicycle.groupBy('year').pivot('month').agg({\"count\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sumMonthly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-565e43930b9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msumMonthly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sumMonthly' is not defined"
     ]
    }
   ],
   "source": [
    "pdf=sumMonthly.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-9750bac3a30b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \n",
    "\n",
    "위 데이터에서 'year' 컬럼이 없어야 그래프를 그릴 수 있다.\n",
    "* drop() 명령어에 삭제할 컬럼명 'year'와 1을 적어준다.\n",
    "0은 행 (index), 1은 컬럼을 삭제한다는 의미이다.\n",
    "* transpose() 함수를 통해 열로 변환하여 (행 데이터는 plot을 할 수 없다), 그래프를 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-95-4ee16b105462>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "my=pdf.drop('year', 1).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 pdf를 변환한 my를 출력하면, 명령어가 적용되어 year 컬럼이 삭제되고, transpose되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-8c42a99f2faf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my' is not defined"
     ]
    }
   ],
   "source": [
    "my"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```plot()``` 함수는 컬럼을 적지 않으면 모든 컬럼에 대해서 plot한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-b566faf0dcc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2018\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2019\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'my' is not defined"
     ]
    }
   ],
   "source": [
    "my.columns=[2018, 2019]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "my.plot(kind='line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Spark SQL\n",
    "\n",
    "관계형 데이터베이스 RDB에서 사용하는 Sql을 사용하여 DataFrame으로부터 데이터를 조회할 수 있다. DataFrame과 달리, RDD는 비구조적인 경우에 사용하므로 테이블로 변환한 후 Sql을 사용하게 된다.\n",
    "\n",
    "* Spark SQL 구성\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "Language API | Python, Java, Scala, Hive QL API를 제공\n",
    "Schema RDD | RDD에 Schema를 적용해 임시 테이블로 변환한다.<br>createOrReplaceTempView<br>createGlobalTempView\n",
    "Data Sources | 다양한 형식 지원 - HDFS, Cassandra, HBase, RDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "앞서 만들어 놓은 World Cup 데이터를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-91fc19e81888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwcDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'wcDf' is not defined"
     ]
    }
   ],
   "source": [
    "wcDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이제 임시 테이블 ```wc```를 만들고, Sql문으로 데이터를 조회해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-99-af701be3b6cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwcDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"wc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select Club,Team,Year from wc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wcDf' is not defined"
     ]
    }
   ],
   "source": [
    "wcDf.createOrReplaceTempView(\"wc\")\n",
    "spark.sql(\"select Club,Team,Year from wc\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-90f9f290840e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwcPlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"select FullName,Club,Team,Year from wc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwcPlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "wcPlayers=spark.sql(\"select FullName,Club,Team,Year from wc\")\n",
    "wcPlayers.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-101-e71b94cb765f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatalog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistTables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```wcPlayers```를 RDD로 변환해서 이름만 출력해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcPlayers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-102-1ede557947b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnamesRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwcPlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Full name: \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnamesRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wcPlayers' is not defined"
     ]
    }
   ],
   "source": [
    "namesRdd=wcPlayers.rdd.map(lambda x: \"Full name: \"+x[0])\n",
    "for e in namesRdd.take(5):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### sql.functions and join\n",
    "\n",
    "리스트에 포함되어 있는 과일에 고유번호를 할당해 보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-94eb032189f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n\u001b[0m\u001b[1;32m      2\u001b[0m                                 [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n\u001b[1;32m      3\u001b[0m                                [\"bucketId\",\"items\"])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "bucketDf=spark.createDataFrame([[1,[\"orange\", \"apple\", \"pineapple\"]],\n",
    "                                [2,[\"watermelon\",\"apple\",\"bananas\"]]],\n",
    "                               [\"bucketId\",\"items\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```truncate```는 행의 값을 잘라내지 않고 출력한다.\n",
    "```show(bucketDf.count(), truncate=False)```는 모든 행을 완전하게 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucketDf.show(bucketDf.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* explode\n",
    "\n",
    "컬럼에 List 또는 배열이 포함된 경우 ```explode()``` 함수는 이를 flat해서 새로운 컬럼을 생성하게 된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucketDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-e4e1c89e45a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbucketDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucketId\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbucketDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'item'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bucketDf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "bDf=bucketDf.select(bucketDf.bucketId, explode(bucketDf.items).alias('item'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "또 다른 DataFrame을 생성해보자. 나중에 앞의 DataFrame과 join하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-c4fabcdb58d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n\u001b[0m\u001b[1;32m      2\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"F2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0;34m\"pineapple\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F3\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                             \u001b[0;34m[\u001b[0m\u001b[0;34m\"watermelon\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"F4\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                             [\"bananas\",\"F5\"]],\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "fDf=spark.createDataFrame([[\"orange\", \"F1\"],\n",
    "                            [\"\", \"F2\"],\n",
    "                            [\"pineapple\",\"F3\"],\n",
    "                            [\"watermelon\",\"F4\"],\n",
    "                            [\"bananas\",\"F5\"]],\n",
    "                            [\"item\",\"itemId\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* join\n",
    "\n",
    "join은 ```inner, cross, outer, full, full_outer, left, left_outer, right, right_outer, left_semi, left_anti``` 여러 종류가 있다. ```inner```기준으로 item이 일치하지 않는 것은 제외하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-9a9c37a45e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoinDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbDf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mbDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'fDf' is not defined"
     ]
    }
   ],
   "source": [
    "joinDf=fDf.join(bDf, fDf.item==bDf.item, \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joinDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-2af41745284d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoinDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitemId\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbucketId\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'joinDf' is not defined"
     ]
    }
   ],
   "source": [
    "joinDf.select(fDf.itemId,fDf.item,bDf.bucketId).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-1: 네트워크에 불법적으로 침입하는 사용자의 분석\n",
    "\n",
    "### 문제\n",
    "\n",
    "네트워크에 불법적으로 침입하는 시도는 허용되어서는 안된다.\n",
    "1998년 MIT Lincoln Labs에서 DARPA Intrusion Detection Evaluation Program을 연구하였다.\n",
    "이 데이터의 일부가 1999년 KDD로 만들어져 배포되고 있다.\n",
    "https://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\n",
    "\n",
    "### 해결\n",
    "\n",
    "마지막 행에 attack의 유형이 구분되어 있다. 네트워크 침입 유형의 특징을 분석해 보자.\n",
    "탐지예방 모델을 구축할 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "KDD데이터는 41 항목으로 구성되어 있다.\n",
    "\n",
    "```python\n",
    "연결(초) | duration: continuous.\n",
    "프로토콜 (tcp,udp,etc) | protocol_type: symbolic.\n",
    "서비스 (http,telnet, etc) | service: symbolic.\n",
    "flag: symbolic.\n",
    "src_bytes: continuous.\n",
    "dst_bytes: continuous.\n",
    "land: symbolic.\n",
    "wrong_fragment: continuous.\n",
    "urgent: continuous.\n",
    "hot: continuous.\n",
    "num_failed_logins: continuous.\n",
    "logged_in: symbolic.\n",
    "num_compromised: continuous.\n",
    "root_shell: continuous.\n",
    "su_attempted: continuous.\n",
    "num_root: continuous.\n",
    "num_file_creations: continuous.\n",
    "num_shells: continuous.\n",
    "num_access_files: continuous.\n",
    "num_outbound_cmds: continuous.\n",
    "is_host_login: symbolic.\n",
    "is_guest_login: symbolic.\n",
    "count: continuous.\n",
    "srv_count: continuous.\n",
    "serror_rate: continuous.\n",
    "srv_serror_rate: continuous.\n",
    "rerror_rate: continuous.\n",
    "srv_rerror_rate: continuous.\n",
    "same_srv_rate: continuous.\n",
    "diff_srv_rate: continuous.\n",
    "srv_diff_host_rate: continuous.\n",
    "dst_host_count: continuous.\n",
    "dst_host_srv_count: continuous.\n",
    "dst_host_same_srv_rate: continuous.\n",
    "dst_host_diff_srv_rate: continuous|.\n",
    "dst_host_same_src_port_rate: continuous.\n",
    "dst_host_srv_diff_host_rate: continuous.\n",
    "dst_host_serror_rate: continuous.\n",
    "dst_host_srv_serror_rate: continuous.\n",
    "dst_host_rerror_rate: continuous.\n",
    "dst_host_srv_rerror_rate: continuous.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 파일 내려받기\n",
    "\n",
    "KDD 파일은 **gz** 압축되어 있다. 파일 확장자 'gz'은 'gzip'이라는 압축 도구에서 생성된 파일이다. 지금은 WinZip에서 읽을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "_url = 'http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz'\n",
    "_fname = os.path.join(os.getcwd(),'data','kddcup.data_10_percent.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "파일이 로컬 디렉토리 ```data```에 존재하면, 즉 이미 내려받았으므로 또 내려받지 않는다. 그렇지 않을 경우에만 ```urlretrieve()``` 함수로 내려받는다. 오류가 발생하면, (1) 파일이 없거나, (2) 파일을 모두 내려 받지 않았거나, (3) 파일이 깨져있을 수 있다. 내려받은 디렉토리로 가서 그 파일이 존재하는지, winzip같은 유틸리티로 해당 gz을 풀어보고 확인하든지, 적당한 에디터로 해당 파일에 내용이 있는지 확인해야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "if(not os.path.exists(_fname)):\n",
    "    print (\"{} data does not exist! retrieving..\".format(_fname))\n",
    "    _f=urlretrieve(_url,_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD 생성\n",
    "\n",
    "**RDD**는 gz와 같은 **압축파일에서 데이터를 읽어서** 생성할 수 있다.\n",
    "\n",
    "반면, DataFrame은 구조schema를 정의해야 하기 때문에 쉽지 않다. 여기서는 **오류**가 발생한다.\n",
    "따라서 RDD를 생성하고 난 후, 그로부터 DataFrame을 생성하고, Sql을 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```textFile()``` 함수로 RDD를 생성한다. ```count()```는 행의 수를 돌려주는 action 함수이다. action 함수는 바로 실행되므로 시간이 좀 걸린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-117f57316556>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_rdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_fname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "_rdd = spark.sparkContext.textFile(_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-299c2597b8cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```map()``` 함수를 사용하여 csv 형식으로 구성된 파일을 컴마(,)로 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-cac4ca6edc38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_allRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "_allRdd=_rdd.map(lambda x: x.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_allRdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정상, 공격 건수\n",
    "\n",
    "데이터가 ```normal```인 경우와 아닌 경우로 구분하자.\n",
    "```filter()```는 41번째 행을 조건에 따라 데이터를 구분한다.\n",
    "```count()``` 함수로 건수를 계산하면 'normal' 97,278, 'attack'은 396,743 건이다.\n",
    "\n",
    "침입구분 | 건수\n",
    "-------|-------\n",
    "normal | 97278\n",
    "attack | 396743\n",
    "전체 | 494021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_allRdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6b897cdbab54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_normalRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_allRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"normal.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_attackRdd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_allRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;34m\"normal.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_allRdd' is not defined"
     ]
    }
   ],
   "source": [
    "_normalRdd=_allRdd.filter(lambda x: x[41]==\"normal.\")\n",
    "_attackRdd=_allRdd.filter(lambda x: x[41]!=\"normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_normalRdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-d8efdc443e34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_normalRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_normalRdd' is not defined"
     ]
    }
   ],
   "source": [
    "_normalRdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_attackRdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-deebe7e9a595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_attackRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_attackRdd' is not defined"
     ]
    }
   ],
   "source": [
    "_attackRdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack별 건수\n",
    "\n",
    "**attack 종류**는 41번째 열에 구분되어 있다. 총 494,021건을 정상 'noraml'과 나머지는 'attack'으로 구분한다.\n",
    "'attack'은 크게 4종류로 나눈다. DOS는 서비스 거부, R2L 원격침입, U2R은 루트권한침입, probing은 탐지이다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "열41에 대해 건수를 세어보자.\n",
    "```reduceByKey()```는 인자로 '함수'가 필요. 키별로 '함수를 사용해서' 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_allRdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a8c993c5a174>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_41\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_allRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_41\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduceByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_allRdd' is not defined"
     ]
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "_41.reduceByKey(lambda x,y: x+y).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```groupByKey()```는 키별로 group한다. 위 ```reduceByKey()```와 달리 ```mapValues()```를 사용해 값을 별도로 계산한다는 점에 유의하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_allRdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-0d317cd3ca9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_41\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_allRdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m41\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0m_41\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_allRdd' is not defined"
     ]
    }
   ],
   "source": [
    "_41 = _allRdd.map(lambda x: (x[41], 1))\n",
    "def f(x): return len(x)\n",
    "_41.groupByKey().mapValues(f).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dataframe 생성\n",
    "\n",
    "열 0, 1, 2, 3, 4, 5, 41을 선별하여 스키마를 정해서 RDD를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-6f4352c1d5e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0m_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m _csvRdd = _csv.map(lambda p: \n\u001b[1;32m      5\u001b[0m     Row(\n",
      "\u001b[0;31mNameError\u001b[0m: name '_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "_csv = _rdd.map(lambda l: l.split(\",\"))\n",
    "_csvRdd = _csv.map(lambda p: \n",
    "    Row(\n",
    "        duration=int(p[0]), \n",
    "        protocol=p[1],\n",
    "        service=p[2],\n",
    "        flag=p[3],\n",
    "        src_bytes=int(p[4]),\n",
    "        dst_bytes=int(p[5]),\n",
    "        attack=p[41]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RDD를 Dataframe으로 변환한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-dccdecf3b386>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_csvRdd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "_df=spark.createDataFrame(_csvRdd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-4a804634c4b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name '_df' is not defined"
     ]
    }
   ],
   "source": [
    "_df.printSchema()\n",
    "_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack 분류\n",
    "\n",
    "네트워크 침입이 'attack' 또는 'normal'에 따라 구분해서 ```attackB``` 컬럼을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-0e0724d75922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mattack_udf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"normal\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\"normal.\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"attack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attackB\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "attack_udf = udf(lambda x: \"normal\" if x ==\"normal.\" else \"attack\", StringType())\n",
    "myDf=_df.withColumn(\"attackB\", attack_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-122-1f2514aaa7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "네트워크 침입 attack을 세분화하여 normal, dos, r2l, u2r, probling으로 **5종류**로 구분한다.\n",
    "구분 문자열이 **점('.')**으로 끝난다는 점에 주의하다.\n",
    "\n",
    "attack 4종류 | 설명 | 41번째 열\n",
    "-----|-----|-----\n",
    "DOS | denial-of-service, e.g. syn flood | back, land, neptune, pod, smurf, teardrop\n",
    "R2L | unauthorized access from a remote machine | ftp_write, guess_passwd, imap, multihop, phf, spy, warezclient, warezmaster\n",
    "U2R | unauthorized access to local superuser (root) privileges | buffer_overflow, loadmodule, perl, rootkit\n",
    "probing | surveillance and other probing | ipsweep, nmap, portsweep, satan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 표에 따라 ```udf()``` 함수를 사용해서 if문으로 'noraml' 및 'attack'을 총 5가지 종류로 구분한다.\n",
    "반환 값은 ```StringType()```이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "def classify41(s):\n",
    "    _5=\"\"\n",
    "    if s==\"normal.\":\n",
    "        _5=\"normal\"\n",
    "    elif s==\"back.\" or s==\"land.\" or s==\"neptune.\" or s==\"pod.\" or s==\"smurf.\" or s==\"teardrop.\":\n",
    "        _5=\"dos\"\n",
    "    elif s==\"ftp_write.\" or s==\"guess_passwd.\" or s==\"imap.\" or s==\"multihop.\" or s==\"phf.\" or\\\n",
    "        s==\"spy.\" or s==\"warezclient.\" or s==\"warezmaster.\":\n",
    "        _5=\"r2l\"\n",
    "    elif s==\"buffer_overflow.\" or s==\"loadmodule.\" or s==\"perl.\" or s==\"rootkit.\":\n",
    "        _5=\"u2r\"\n",
    "    elif s==\"ipsweep.\" or s==\"nmap.\" or s==\"portsweep.\" or s==\"satan.\":\n",
    "        _5=\"probing\"\n",
    "    return _5\n",
    "\n",
    "attack5_udf = udf(classify41, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-85a012a5c491>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"attack5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattack5_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf=myDf.withColumn(\"attack5\", attack5_udf(_df.attack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-1f2514aaa7a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "잘 분류되었는지 일부 데이터를 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-126-f9fbd7adee3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### attack, normal 특징 분석\n",
    "\n",
    "```attack5``` 별로 건수를 세어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-25e720fc7f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attack5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```attack5``` 별로 공격의 특징을 분석해보자. 어떤 ```protocol```, ```src_bytes```, ```duration```이 어떤지 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-128-47c0012d550f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy(\"protocol\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-d35768444e87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attackB'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'protocol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB','protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-b21149c106d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attackB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'protocol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attackB').pivot('protocol').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-131-e3c9ef86314c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attack5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'protocol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'src_bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').pivot('protocol').avg('src_bytes').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-52e3c54793a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attack5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'duration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.groupBy('attack5').avg('duration').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-02e6cafcc25d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'attackB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'protocol'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dst_bytes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.groupBy('attackB').pivot('protocol').agg(F.max('dst_bytes')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "좀 더 세밀한 조건으로 ```duration>1000)```, ```dst_bytes==0```인 경우의 건수를 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'myDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-9dd3c3f0fdc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmyDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"duration\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dst_bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst_bytes\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"protocol\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'myDf' is not defined"
     ]
    }
   ],
   "source": [
    "myDf.select(\"protocol\", \"duration\", \"dst_bytes\")\\\n",
    "    .filter(_df.duration>1000)\\\n",
    "    .filter(_df.dst_bytes==0)\\\n",
    "    .groupBy(\"protocol\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SQL\n",
    "\n",
    "SQL을 사용해보자. 위에 사용했던 ```_df```에서 임시 테이블 ```_tab```을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-ded967a72532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0m_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregisterTempTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_tab\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name '_df' is not defined"
     ]
    }
   ],
   "source": [
    "_df.registerTempTable(\"_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-ef425c3f20a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m tcp_interactions = spark.sql(\n\u001b[0m\u001b[1;32m      2\u001b[0m \"\"\"\n\u001b[1;32m      3\u001b[0m     \u001b[0mSELECT\u001b[0m \u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_bytes\u001b[0m \u001b[0mFROM\u001b[0m \u001b[0m_tab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mWHERE\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'tcp'\u001b[0m \u001b[0mAND\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0mAND\u001b[0m \u001b[0mdst_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "tcp_interactions = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT duration, dst_bytes FROM _tab\n",
    "    WHERE protocol = 'tcp' AND duration > 1000 AND dst_bytes = 0\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tcp_interactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-3a24fa8f97a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtcp_interactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tcp_interactions' is not defined"
     ]
    }
   ],
   "source": [
    "tcp_interactions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tcp_interactions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-bf7094f16f6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtcp_interactions_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcp_interactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Duration: {}, Dest. bytes: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdst_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tcp_interactions' is not defined"
     ]
    }
   ],
   "source": [
    "tcp_interactions_out = tcp_interactions.rdd\\\n",
    "    .map(lambda p: \"Duration: {}, Dest. bytes: {}\".format(p.duration, p.dst_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(ti_out)? (<ipython-input-139-a9ffbd19a28f>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-139-a9ffbd19a28f>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    print ti_out\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(ti_out)?\n"
     ]
    }
   ],
   "source": [
    "for i,ti_out in enumerate(tcp_interactions_out.collect()):\n",
    "    if(i%10==0):\n",
    "        print ti_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-2: Twitter JSON 데이터 읽기\n",
    "\n",
    "* [nok] 현재 디렉토리 _tweet.json\n",
    "    * src/ds_twitter_3.py로 변경 (ds_twitter_3.json으로 저장)\n",
    "\n",
    "\n",
    "\n",
    "* Twitter JSON을 읽을 경우\n",
    "\n",
    "구분 | 예\n",
    "-------|-------\n",
    "unicode를 사용하면 backslash | \"{\\\"created_at\\\":\\\"Sun Nov 13 00:05:19 +0000 2016\\\"\n",
    "보통 | {\"created_at\":\"Sun Nov 13 00:05:19 +0000 2016\"\n",
    "\n",
    "* allowBackslashEscapingAnyCharacter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-3: 뉴욕에서 출생한 신생아 분석\n",
    "\n",
    "### 뉴욕에서 출생한 신생아가 년도별 성별에 차이가 있을까?\n",
    "\n",
    "뉴욕에서 2007년 출생한 유아의 기록이다.\n",
    "https://health.data.ny.gov/Health/Baby-Names-Beginning-2007/jxy9-yhdk\n",
    "\n",
    "Column Name | 설명\n",
    "-----|-----\n",
    "Year | Year data was collected.\n",
    "First Name | 이름\n",
    "County | Location where the baby’s mother resided as stated on their birth certificate.\n",
    "Sex | F= Female M= Male\n",
    "Count | Five (5) or more of the same baby name in a county outside of NYC; Ten (10) or more of the same baby name in a NYC borough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-4: 우버 택시의 운행기록 분석\n",
    "\n",
    "* 질문: 2015년 가장 많은 운행을 한 base는?\n",
    "https://github.com/tmcgrath/spark-with-python-course/blob/master/Spark-SQL-CSV-with-Python.ipynb\n",
    "\n",
    "* fivethirtyeight\n",
    "    * git clone https://github.com/fivethirtyeight/uber-tlc-foil-response.git\n",
    "        daily Uber trip statistics in January and February 2015\n",
    "\n",
    "dispatching_base_number | date | active_vehicles | trips\n",
    "----------|----------|----------|----------\n",
    "B02512 | 1/1/2015 | 190 | 1132\n",
    "B02765 | 1/1/2015 | 225 | 1765\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-5: JDBC를 사용해서 데이터 읽기\n",
    "\n",
    "* jdbc를 연결하는 방식은 Java와 같이 'driver', 'url'을 설정하면 된다.\n",
    "* 여기서는 sqlite를 실습한다.\n",
    "\n",
    "* sqlite와 같이 Spark 패키지가 없는 경우, jar를 다운로드하고\n",
    "설정파일 conf/spark-defaults.conf에 'spark.driver.extraClassPath'를 추가한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.7 MongoDB Spark connector\n",
    "\n",
    "* Spark에서 MongoDB에 저장된 데이터를 읽어 온다.\n",
    "* 참조: pymongo-spark (Spark와 PyMongo를 사용하는 Python 라이브러리, 설치하려면 pip install pymongo-spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.1 설정\n",
    "\n",
    "* 참조 https://docs.mongodb.com/spark-connector/\n",
    "* 설정파일 conf/spark-defaults.conf 수정\n",
    "    * Spark 버전에 맞는 jar를 선택한다.\n",
    "    * MongoDB<3.2인 경우, spark.mongodb.input.partitioner가 필요하다.\n",
    "    * packages 여러 개를 넣을 경우에는 컴마로 분리한다.\n",
    "\n",
    "```python\n",
    "$vim conf/spark-defaults.conf \n",
    "spark.jars.packages=org.mongodb.spark:mongo-spark-connector_2.10:1.1.0\n",
    "spark.mongodb.input.partitioner=MongoPaginateBySizePartitioner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-140-8d7789a7805a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-140-8d7789a7805a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    print spark.conf.get('spark.jars.packages')\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "print spark.conf.get('spark.jars.packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.2 uri\n",
    "\n",
    "* SparkSession에 uri를 설정할 수 있다. 연결에 필요한 ip, database, collection을 정의한다.\n",
    "\n",
    "```python\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "* 또는 실행시점에 설정할 수 있다 (아래 참조)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.7.3 MongoDB Python API\n",
    "\n",
    "* format은 \"com.mongodb.spark.sql.DefaultSource\"로 설정한다.\n",
    "* 'option'을 사용해서 실행시점에 Database, Colleciton 명을 설정할 수 있다.\n",
    "\n",
    "구분 | 명령어 예\n",
    "-----|-----\n",
    "쓰기 | DataFrame.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.mode(\"overwrite\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\<br>.save()\n",
    "읽기 | spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\<br>.option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\<br>.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.7.4 연습으로 쓰기, 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-010b3b3f9ad2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeople\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kim\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lee\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"choi\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"park\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "people = spark.createDataFrame([(\"kim\",10),(\"lee\",20),(\"choi\",30),(\"park\",40)],[\"name\", \"age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'people' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-49c4bcc9409b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpeople\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mongodb://127.0.0.1/myDB.ds_spark_ml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'people' is not defined"
     ]
    }
   ],
   "source": [
    "people.write.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .mode(\"append\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/myDB.ds_spark_ml\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-09aa658342f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-7111773ec9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.select('name').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-4c74a3ae0ebe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"com.mongodb.spark.sql.DefaultSource\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uri\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"mongodb://127.0.0.1/ds_twitter.seoul\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "    .option(\"uri\",\"mongodb://127.0.0.1/ds_twitter.seoul\")\\\n",
    "    .load()\n",
    "df.select('text').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-b666bf274d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.8 spark-submit\n",
    "\n",
    "* spark-submit는 일괄실행 (self-contained app in quick-start 참조)\n",
    "\n",
    "* MongoDB를 사용하려면, spark-defaults.conf에 jar를 추가한다 (앞서 미리 설정하였다.)\n",
    "\n",
    "* spark-submit을 실행하기 전, 'conf/log4j.properties'를 수정 log level을 ERROR로 설정하였다.\n",
    "```python\n",
    "log4j.rootCategory=ERROR, console\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.8.1 간단한 작업\n",
    "\n",
    "* DataFrame 만들고, 출력하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_sql.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_sql.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "\n",
    "def doIt():\n",
    "    d = [{'name': 'Alice', 'age': 1}]\n",
    "    print spark.createDataFrame(d).collect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(conf=myConf)\\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit\r\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_sql.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.8.2 MongoDB\n",
    "\n",
    "* Database, Collection 읽기, 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_spark_mongo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_spark_mongo.py\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: UTF-8 -*-\n",
    "import pyspark\n",
    "def doIt():\n",
    "    print \"---------RESULT-----------\"\n",
    "    print \"------mongodb write-------\"\n",
    "    myRdd = spark.sparkContext.parallelize([\n",
    "        (\"js\", 150),\n",
    "        (\"Gandalf\", 1000),\n",
    "        (\"Thorin\", 195),\n",
    "        (\"Balin\", 178),\n",
    "        (\"Kili\", 77),\n",
    "        (\"Dwalin\", 169),\n",
    "        (\"Oin\", 167),\n",
    "        (\"Gloin\", 158),\n",
    "        (\"Fili\", 82),\n",
    "        (\"Bombur\", None)\n",
    "    ])\n",
    "    myDf = spark.createDataFrame(myRdd, [\"name\", \"age\"])\n",
    "    print myDf\n",
    "    myDf.write.format(\"com.mongodb.spark.sql.DefaultSource\").mode(\"overwrite\").save()\n",
    "    print \"---------read-----------\"\n",
    "    df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\").load()\n",
    "    print df.printSchema()\n",
    "    df.registerTempTable(\"myTable\")\n",
    "    myTab = spark.sql(\"SELECT name, age FROM myTable WHERE age >= 100\")\n",
    "    myTab.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    myConf=pyspark.SparkConf()\n",
    "    spark = pyspark.sql.SparkSession.builder\\\n",
    "        .master(\"local\")\\\n",
    "        .appName(\"myApp\")\\\n",
    "        .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/myDB.ds_spark_df_mongo\") \\\n",
    "        .getOrCreate()\n",
    "    doIt()\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: no such file or directory: /home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit\r\n"
     ]
    }
   ],
   "source": [
    "!/home/jsl/Downloads/spark-2.0.0-bin-hadoop2.7/bin/spark-submit src/ds_spark_mongo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 문제 S-6: MongoDB 저장된 열린데이터 읽어오는 spark-submit\n",
    "\n",
    "* MongoDB에 저장된 데이터 읽기\n",
    "\n",
    "구분 | 명\n",
    "-----|-----\n",
    "Database | ds_open_subwayPassengersDb\n",
    "Collection | db_open_subwayTable\n",
    "key | JSON 계층구조를 따라 읽는다. CardSubwayStatisticsService.row.RIDE_PASGR_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
