{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scrapy\n",
    "\n",
    "* Last updated: 20170326 20161004\n",
    "* todo rename ds_web_data -> ds_web_scrapy\n",
    "\n",
    "## 1.1 학습내용\n",
    "\n",
    "### 1.1.1 목표\n",
    "\n",
    "* Scrapy를 사용하여 데이터를 추출할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1.2 목차\n",
    "\n",
    "* 1.2 단순페이지가 아닌 경우의 웹데이터 수집하기\n",
    "* 1.3 Scrapy\n",
    "* 1.4 설치\n",
    "* 1.5 Scrapy shell\n",
    "* 1.6 Scrapy project\n",
    "* 1.7 Basic Scrapy\n",
    "* 1.8 Link Extractors\n",
    "* 1.9 Setting\n",
    "* 1.10 Logging\n",
    "* 1.11 Pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1.3 문제\n",
    "\n",
    "* 웹데이터-1: 페이지가 있는 사이트 크롤링하기\n",
    "    * toscrape.com 가상의 서점과 인용문을 제공하고 있어, 연습하기 좋은 사이트\n",
    "* 웹데이터-2: 국제학회 목록 크롤링하기\n",
    "* 웹데이터-3: Reddit 크롤링하기\n",
    "    * Scrapy xpath\n",
    "* 웹데이터 : 네이버 음악 목록 가져오기 (다음 페이지)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1.4 연습\n",
    "* 연습 웹데이터-1: 코스피200 데이터 가져오기\n",
    "    * Google Finance에서 Scrapy를 사용해서 가져오기\n",
    "    * xpath\n",
    "    * sqlite db에 저장\n",
    "* 연습 웹데이터-2: Scrapy를 사용하여 Tripadvisor 크롤링하기\n",
    "    * unicode\n",
    "    * css with spaces, ::pseudo-elements\n",
    "    * [nok] next_link의 onclick() 미완성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 단순페이지가 아닌 경우의 웹데이터 수집하기\n",
    "\n",
    "* 웹페이지가 단순한 경우, 크롤링해서 데이터를 가져오는 것은 비교적 어렵지 않다.\n",
    "* 웹서버에서 반환하는 response를 가져와서 데이터를 추출한다.\n",
    "* 그러나, 페이지가 복잡한 경우, 링크를 포함해서 따라가야 하거나 javascript를 실행해야 내용이 나타나는 경우 추가 작업이 필요하다.\n",
    "\n",
    "구분 | 설명\n",
    "-----|-----\n",
    "링크 | 페이지에 링크가 포함되어 있는 경우\n",
    "페이징 | 여러 페이지로 되어 있어 일련번호, 다음을 눌러 다음 페이지로 이동하는 경우\n",
    "이미지 | 페이지에 이미지를 내려받아야 하는 경우\n",
    "폼 | 페이지에 폼\n",
    "동적페이지 | 자바스크립트가 실행, dynamic pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 도구\n",
    "\n",
    "도구 | 설명 | 적합한 경우\n",
    "-----|-----|-----\n",
    "Selenium | 화면을 테스트하기 위한 자동화 도구, 크롤링에 사용할 수 있다. | javascript, ajax 등을 포함하는 dynamic pages에 사용\n",
    "Scrapy | crawling framework | multiple pages. form과 같이 javascript을 포함한 페이지도 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 Srapy: Crawling framework\n",
    "\n",
    "### 1.3.1 개요\n",
    "\n",
    "* 웹사이트 크롤링 프레임워크.\n",
    "* xpath, css selector를 사용할 수 있다. Scrapy는 lxml을 사용하고 있다.\n",
    "* 병렬작업이 가능하다. Python으로 만들어진 비동기 네트워크 라이브러리 Twisted를 사용한다.\n",
    "* 웹데이터를 엑셀과 같이 구조적인 데이터 형식으로 만들기\n",
    "* 상속을 사용한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"154px\" style=\"width:265px;height:154px;\" version=\"1.1\" viewBox=\"0 0 265 154\" width=\"265px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><filter height=\"300%\" id=\"f1\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"45\" x2=\"45\" y1=\"38.2969\" y2=\"116.5625\"/><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"214.5\" x2=\"214.5\" y1=\"38.2969\" y2=\"116.5625\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"70\" x=\"8\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"56\" x=\"15\" y=\"22.9951\">browser</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"70\" x=\"8\" y=\"115.5625\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"56\" x=\"15\" y=\"135.5576\">browser</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"87\" x=\"169.5\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"73\" x=\"176.5\" y=\"22.9951\">webServer</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"87\" x=\"169.5\" y=\"115.5625\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"73\" x=\"176.5\" y=\"135.5576\">webServer</text><polygon fill=\"#A80036\" points=\"203,65.2969,213,69.2969,203,73.2969,207,69.2969\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"45\" x2=\"209\" y1=\"69.2969\" y2=\"69.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"79\" x=\"52\" y=\"64.3638\">request (url)</text><polygon fill=\"#A80036\" points=\"56,94.4297,46,98.4297,56,102.4297,52,98.4297\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"50\" x2=\"214\" y1=\"98.4297\" y2=\"98.4297\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"146\" x=\"62\" y=\"93.4966\">response (a web page)</text></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "browser -> webServer:request (url)\n",
    "webServer -> browser:response (a web page)\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.4 설치\n",
    "\n",
    "* 설치 요건 - python 2.7 or above 3.3, openssl, lxml, twisted\n",
    "* pip\n",
    "```\n",
    "pip install scrapy\n",
    "```\n",
    "\n",
    "* anaconda\n",
    "    * anaconda prompt 단말을 열어서 명령어를 입력한다.\n",
    "```\n",
    "C:\\> conda install -c scrapinghub scrapy\n",
    "```\n",
    "\n",
    "    * anaconda prompt에서 scrapy shell을 입력하면 shell환경이 실행된다.\n",
    "```\n",
    "C:\\> scrapy shell\n",
    "In [1]:\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 설치하고 나면 버전을 확인한다.\n",
    "```\n",
    "scrapy version\n",
    "```\n",
    "\n",
    "* scrapy cli 명령어\n",
    "\n",
    "명령어 | 설명\n",
    "-----|-----\n",
    "genspider | Generate new spider using pre-defined templates\n",
    "runspider | 프로젝트를 생성하지 않고 실행 (프로젝트 생성하면 crawl명령어)\n",
    "settings  | 설정 내용 가져오기\n",
    "shell     | 콘솔 기능 사용하기\n",
    "startproject | 프로젝트 생성하기\n",
    "version    | 버전 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* spider 종류:\n",
    "    * scrapy genspider명령어로 목록을 볼 수 있다.\n",
    "```\n",
    "scrapy genspider -l\n",
    "```\n",
    "\n",
    "타잎 | 설명\n",
    "-------|-------\n",
    "Spider | 가장 단순한 'basic' spider, start_urls 또는 start_requests()에서 반환된 url을 크롤링\n",
    "CrawlSpider | regex rules에 따라 link follow\n",
    "XMLFeedSpider | XML을 parse\n",
    "CSVFeedSpider | CSV feeds\n",
    "SiteMapSpider | site map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available templates:\r\n",
      "  basic\r\n",
      "  crawl\r\n",
      "  csvfeed\r\n",
      "  xmlfeed\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy genspider -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.5 Scrapy shell\n",
    "\n",
    "* /usr/local/bin/scrapy를 실행\n",
    "* Python shell이 실행되면서, 사용할 수 있는 객체를 생성하여 제공한다.\n",
    "    * scrapy, crawler, spider, request, response...\n",
    "* Scrapy shell 연습 예제\n",
    "```\n",
    "$ scrapy shell http://stackoverflow.com\n",
    ">>> response.url\n",
    ">>> response.headers\n",
    ">>> response.xpath('//title')\n",
    ">>> response.xpath('//title/text()')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scrapy shell 연습 예제 - xpath, css\n",
    "\n",
    "```\n",
    "$ scrapy shell \"https://data.go.kr\"\n",
    "In [1]: response.xpath('//title/text()')\n",
    "Out[1]: [<Selector xpath='//title/text()' data=u'\\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138'>]\n",
    "\n",
    "In [2]: response.xpath('//title/text()').extract()\n",
    "Out[2]: [u'\\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138']\n",
    "\n",
    "In [3]: response.css('title').extract()\n",
    "Out[3]: [u'<title>\\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138</title>']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.6 Scrapy Project\n",
    "\n",
    "### 1.6.1 프로젝트 절차\n",
    "\n",
    "* 단계1: 프로젝트 생성\n",
    "    * 프로젝트를 생성할 디렉토리로 이동\n",
    "```\n",
    "cd mypjt\n",
    "scrapy startproject myspider\n",
    "```\n",
    "        \n",
    "        * 1 'scrapy': 프레임워크를 사용\n",
    "        * 2 'startproject': 프로젝트를 생성\n",
    "        * 3 'myspider': 프로젝트 명칭\n",
    "        * 4 크롤링 대상 도메인\n",
    "\n",
    "    * 생성내용\n",
    "```\n",
    "mypjt (내가 만듦)\n",
    "├── myspider (디렉토리명은 내가 만듦, 안의 모든 파일들은 자동 생성)\n",
    "│   ├── __init__.py\n",
    "│   ├── items.py\n",
    "│   ├── pipelines.py\n",
    "│   ├── settings.py\n",
    "│   └── spiders\n",
    "│       ├── __init__.py\n",
    "│       └── hello.py (내가 만듦)\n",
    "└── scrapy.cfg\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계2 'items.py': item 정의\n",
    "```\n",
    "cd myspider/\n",
    "vim items.py\n",
    "```\n",
    "\n",
    "    * 클래스를 만들어 가져올 데이터 항목을 정의한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계3 'hello.py': Spider 클래스\n",
    "```\n",
    "cd spiders/\n",
    "vim hello.py\n",
    "```\n",
    "\n",
    "    * 1 명칭은 프로젝트마다 다르게 명명한다.\n",
    "    * 2 'start_urls'에서 시작\n",
    "    * 3 parse()함수에서 원하는 항목을 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계4: 테스트\n",
    "```\n",
    "scrapy crawl 또는\n",
    "scrapy crawl _myAppName (name=\"_myAppName\", Spider 클래스에 정의된 명칭)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 단계5: 저장\n",
    "    * pipeline을 이용해서 database에 저장할 수 있다.\n",
    "```\n",
    "scrapy crawl _myAppName -o _myAppName.csv -t csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.6.2 실행\n",
    "\n",
    "* 프로젝트의 spider명으로 실행한다.\n",
    "* 출력파일을 적어준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!scrapy crawl _myAppName -o _myAppName.csv -t csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.7 Basic Scrapy\n",
    "\n",
    "* 단순한 예제\n",
    "* scrapy.spiders.Spider를 사용 (CrawlSpider는 다음에 사용한다.)\n",
    "* 절차\n",
    "    * Item클래스로 데이터 항목을 정의\n",
    "    * start_urls (또는 start_requests())의 url로 Request를 생성\n",
    "    * callback 함수 parse() 호출\n",
    "        * Request로 요청\n",
    "        * Response (dict, Item, Request의 형태로 반환)\n",
    "        * 돌려 받은 response(s)에서 데이터 항목을 추출한다.\n",
    "        * 추출한 항목을 반환\n",
    "        * url이 한 개이므로 다음 페이지 처리가 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"368px\" style=\"width:406px;height:368px;\" version=\"1.1\" viewBox=\"0 0 406 368\" width=\"406px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><filter height=\"300%\" id=\"f1\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><polygon fill=\"#FFFFFF\" filter=\"url(#f1)\" points=\"22,230,120,230,127,252.2969,130,252.2969,130,342,22,342,22,230\" style=\"stroke: #000000; stroke-width: 2.0;\"/><line style=\"stroke: #000000; stroke-width: 2.0;\" x1=\"22\" x2=\"127\" y1=\"252.2969\" y2=\"252.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"92\" x=\"26\" y=\"244.9951\">scrapy.item</text><polygon fill=\"#FFFFFF\" filter=\"url(#f1)\" points=\"141,24,264,24,271,46.2969,283,46.2969,283,187,141,187,141,24\" style=\"stroke: #000000; stroke-width: 2.0;\"/><line style=\"stroke: #000000; stroke-width: 2.0;\" x1=\"141\" x2=\"271\" y1=\"46.2969\" y2=\"46.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"117\" x=\"145\" y=\"38.9951\">scrapy.spiders</text><polygon fill=\"#FFFFFF\" filter=\"url(#f1)\" points=\"154,211,250,211,257,233.2969,399,233.2969,399,361,154,361,154,211\" style=\"stroke: #000000; stroke-width: 2.0;\"/><line style=\"stroke: #000000; stroke-width: 2.0;\" x1=\"154\" x2=\"257\" y1=\"233.2969\" y2=\"233.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" font-weight=\"bold\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"90\" x=\"158\" y=\"225.9951\">scrapy.http</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"60.8047\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"59\" x=\"54.5\" y=\"265\"/><ellipse cx=\"69.5\" cy=\"281\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M72.4688,286.6406 Q71.8906,286.9375 71.25,287.0859 Q70.6094,287.2344 69.9063,287.2344 Q67.4063,287.2344 66.0859,285.5859 Q64.7656,283.9375 64.7656,280.8125 Q64.7656,277.6875 66.0859,276.0313 Q67.4063,274.375 69.9063,274.375 Q70.6094,274.375 71.2578,274.5313 Q71.9063,274.6875 72.4688,274.9844 L72.4688,277.7031 Q71.8438,277.125 71.25,276.8516 Q70.6563,276.5781 70.0313,276.5781 Q68.6875,276.5781 68,277.6484 Q67.3125,278.7188 67.3125,280.8125 Q67.3125,282.9063 68,283.9766 Q68.6875,285.0469 70.0313,285.0469 Q70.6563,285.0469 71.25,284.7734 Q71.8438,284.5 72.4688,283.9219 L72.4688,286.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"27\" x=\"83.5\" y=\"285.1543\">Item</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"55.5\" x2=\"112.5\" y1=\"297\" y2=\"297\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"31\" x=\"60.5\" y=\"311.2104\">fields</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"55.5\" x2=\"112.5\" y1=\"317.8047\" y2=\"317.8047\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"112.0234\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"110\" x=\"157\" y=\"59\"/><ellipse cx=\"189.1\" cy=\"75\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M192.0688,80.6406 Q191.4906,80.9375 190.85,81.0859 Q190.2094,81.2344 189.5063,81.2344 Q187.0063,81.2344 185.6859,79.5859 Q184.3656,77.9375 184.3656,74.8125 Q184.3656,71.6875 185.6859,70.0313 Q187.0063,68.375 189.5063,68.375 Q190.2094,68.375 190.8578,68.5313 Q191.5063,68.6875 192.0688,68.9844 L192.0688,71.7031 Q191.4438,71.125 190.85,70.8516 Q190.2563,70.5781 189.6313,70.5781 Q188.2875,70.5781 187.6,71.6484 Q186.9125,72.7188 186.9125,74.8125 Q186.9125,76.9063 187.6,77.9766 Q188.2875,79.0469 189.6313,79.0469 Q190.2563,79.0469 190.85,78.7734 Q191.4438,78.5 192.0688,77.9219 L192.0688,80.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"40\" x=\"206.9\" y=\"79.1543\">Spider</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"158\" x2=\"266\" y1=\"91\" y2=\"91\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"32\" x=\"163\" y=\"105.2104\">name</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"98\" x=\"163\" y=\"118.0151\">allowed_domains</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"55\" x=\"163\" y=\"130.8198\">start_urls</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"94\" x=\"163\" y=\"143.6245\">custom_settings</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"158\" x2=\"266\" y1=\"150.2188\" y2=\"150.2188\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"95\" x=\"163\" y=\"164.4292\">parse(response)</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"99.2188\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"94\" x=\"289\" y=\"246\"/><ellipse cx=\"304\" cy=\"262\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M306.9688,267.6406 Q306.3906,267.9375 305.75,268.0859 Q305.1094,268.2344 304.4063,268.2344 Q301.9063,268.2344 300.5859,266.5859 Q299.2656,264.9375 299.2656,261.8125 Q299.2656,258.6875 300.5859,257.0313 Q301.9063,255.375 304.4063,255.375 Q305.1094,255.375 305.7578,255.5313 Q306.4063,255.6875 306.9688,255.9844 L306.9688,258.7031 Q306.3438,258.125 305.75,257.8516 Q305.1563,257.5781 304.5313,257.5781 Q303.1875,257.5781 302.5,258.6484 Q301.8125,259.7188 301.8125,261.8125 Q301.8125,263.9063 302.5,264.9766 Q303.1875,266.0469 304.5313,266.0469 Q305.1563,266.0469 305.75,265.7734 Q306.3438,265.5 306.9688,264.9219 L306.9688,267.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"62\" x=\"318\" y=\"266.1543\">Response</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"290\" x2=\"382\" y1=\"278\" y2=\"278\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"15\" x=\"295\" y=\"292.2104\">url</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"36\" x=\"295\" y=\"305.0151\">status</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"47\" x=\"295\" y=\"317.8198\">headers</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"27\" x=\"295\" y=\"330.6245\">body</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"290\" x2=\"382\" y1=\"337.2188\" y2=\"337.2188\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"99.2188\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"84\" x=\"170\" y=\"246\"/><ellipse cx=\"185\" cy=\"262\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M187.9688,267.6406 Q187.3906,267.9375 186.75,268.0859 Q186.1094,268.2344 185.4063,268.2344 Q182.9063,268.2344 181.5859,266.5859 Q180.2656,264.9375 180.2656,261.8125 Q180.2656,258.6875 181.5859,257.0313 Q182.9063,255.375 185.4063,255.375 Q186.1094,255.375 186.7578,255.5313 Q187.4063,255.6875 187.9688,255.9844 L187.9688,258.7031 Q187.3438,258.125 186.75,257.8516 Q186.1563,257.5781 185.5313,257.5781 Q184.1875,257.5781 183.5,258.6484 Q182.8125,259.7188 182.8125,261.8125 Q182.8125,263.9063 183.5,264.9766 Q184.1875,266.0469 185.5313,266.0469 Q186.1563,266.0469 186.75,265.7734 Q187.3438,265.5 187.9688,264.9219 L187.9688,267.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"52\" x=\"199\" y=\"266.1543\">Request</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"171\" x2=\"253\" y1=\"278\" y2=\"278\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"15\" x=\"176\" y=\"292.2104\">url</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"43\" x=\"176\" y=\"305.0151\">method</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"47\" x=\"176\" y=\"317.8198\">headers</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"27\" x=\"176\" y=\"330.6245\">body</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"171\" x2=\"253\" y1=\"337.2188\" y2=\"337.2188\"/><path d=\"M172.551,171.012 C151.581,200.256 126.465,235.281 108.347,260.548 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"#A80036\" points=\"105.2747,264.832,113.7687,259.8469,108.1874,260.768,107.2663,255.1866,105.2747,264.832\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M250.216,171.012 C265.825,193.482 283.804,219.363 299.258,241.61 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"#A80036\" points=\"302.113,245.719,300.2661,236.0449,299.2615,241.6118,293.6946,240.6072,302.113,245.719\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M212,171.012 C212,193.109 212,218.505 212,240.499 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"#A80036\" points=\"212,245.719,216,236.719,212,240.719,208,236.719,212,245.719\" style=\"stroke: #A80036; stroke-width: 1.0;\"/></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "class scrapy.item.Item {\n",
    "    fields\n",
    "}\n",
    "class scrapy.spiders.Spider {\n",
    "    name\n",
    "    allowed_domains\n",
    "    start_urls\n",
    "    custom_settings\n",
    "    parse(response)\n",
    "}\n",
    "class scrapy.http.Response {\n",
    "    url\n",
    "    status\n",
    "    headers\n",
    "    body\n",
    "}\n",
    "class scrapy.http.Request {\n",
    "    url\n",
    "    method\n",
    "    headers\n",
    "    body\n",
    "}\n",
    "scrapy.spiders.Spider -> scrapy.item.Item\n",
    "scrapy.spiders.Spider -> scrapy.http.Response\n",
    "scrapy.spiders.Spider -> scrapy.http.Request\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.7.1 데이터 항목을 정의\n",
    "\n",
    "클래스 | 설명\n",
    "-----|-----\n",
    "scrapy.item.Item | 웹에서 추출한 데이터 항목을 저장하는 데이터 구조, dict를 사용\n",
    "scrapy.item.Field | 데이터 개별 항목을 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class myItem(scrapy.item.Item):\n",
    "    title = scrapy.item.Field()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "my=myItem(title='hello')\n",
    "print my['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.7.2 Requests와 Responses\n",
    "\n",
    "* 앞서 크롤링하는 것과 다르지 않게\n",
    "    * HTTP request를 만들어 'scrapy.http.Request' 객체를 생성해서 요청하고,\n",
    "    * 그 결과 HTTP response를 'scrapy.http.Response' 객체로 반환\n",
    "\n",
    "* Request는 url목록을 가져와서 생성한다.\n",
    "    * allowed_domains: 허용되는 도메인 목록, python list.\n",
    "    * start_urls: url 목록.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"173px\" style=\"width:593px;height:173px;\" version=\"1.1\" viewBox=\"0 0 593 173\" width=\"593px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><filter height=\"300%\" id=\"f1\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"84\" x=\"21.5\" y=\"8\"/><ellipse cx=\"36.5\" cy=\"24\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M39.4688,29.6406 Q38.8906,29.9375 38.25,30.0859 Q37.6094,30.2344 36.9063,30.2344 Q34.4063,30.2344 33.0859,28.5859 Q31.7656,26.9375 31.7656,23.8125 Q31.7656,20.6875 33.0859,19.0313 Q34.4063,17.375 36.9063,17.375 Q37.6094,17.375 38.2578,17.5313 Q38.9063,17.6875 39.4688,17.9844 L39.4688,20.7031 Q38.8438,20.125 38.25,19.8516 Q37.6563,19.5781 37.0313,19.5781 Q35.6875,19.5781 35,20.6484 Q34.3125,21.7188 34.3125,23.8125 Q34.3125,25.9063 35,26.9766 Q35.6875,28.0469 37.0313,28.0469 Q37.6563,28.0469 38.25,27.7734 Q38.8438,27.5 39.4688,26.9219 L39.4688,29.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"52\" x=\"50.5\" y=\"28.1543\">Request</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"22.5\" x2=\"104.5\" y1=\"40\" y2=\"40\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"22.5\" x2=\"104.5\" y1=\"48\" y2=\"48\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"115\" x=\"6\" y=\"116\"/><ellipse cx=\"21\" cy=\"132\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M23.9688,137.6406 Q23.3906,137.9375 22.75,138.0859 Q22.1094,138.2344 21.4063,138.2344 Q18.9063,138.2344 17.5859,136.5859 Q16.2656,134.9375 16.2656,131.8125 Q16.2656,128.6875 17.5859,127.0313 Q18.9063,125.375 21.4063,125.375 Q22.1094,125.375 22.7578,125.5313 Q23.4063,125.6875 23.9688,125.9844 L23.9688,128.7031 Q23.3438,128.125 22.75,127.8516 Q22.1563,127.5781 21.5313,127.5781 Q20.1875,127.5781 19.5,128.6484 Q18.8125,129.7188 18.8125,131.8125 Q18.8125,133.9063 19.5,134.9766 Q20.1875,136.0469 21.5313,136.0469 Q22.1563,136.0469 22.75,135.7734 Q23.3438,135.5 23.9688,134.9219 L23.9688,137.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"83\" x=\"35\" y=\"136.1543\">FormRequest</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"7\" x2=\"120\" y1=\"148\" y2=\"148\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"7\" x2=\"120\" y1=\"156\" y2=\"156\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"94\" x=\"325.5\" y=\"8\"/><ellipse cx=\"340.5\" cy=\"24\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M343.4688,29.6406 Q342.8906,29.9375 342.25,30.0859 Q341.6094,30.2344 340.9063,30.2344 Q338.4063,30.2344 337.0859,28.5859 Q335.7656,26.9375 335.7656,23.8125 Q335.7656,20.6875 337.0859,19.0313 Q338.4063,17.375 340.9063,17.375 Q341.6094,17.375 342.2578,17.5313 Q342.9063,17.6875 343.4688,17.9844 L343.4688,20.7031 Q342.8438,20.125 342.25,19.8516 Q341.6563,19.5781 341.0313,19.5781 Q339.6875,19.5781 339,20.6484 Q338.3125,21.7188 338.3125,23.8125 Q338.3125,25.9063 339,26.9766 Q339.6875,28.0469 341.0313,28.0469 Q341.6563,28.0469 342.25,27.7734 Q342.8438,27.5 343.4688,26.9219 L343.4688,29.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"62\" x=\"354.5\" y=\"28.1543\">Response</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"326.5\" x2=\"418.5\" y1=\"40\" y2=\"40\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"326.5\" x2=\"418.5\" y1=\"48\" y2=\"48\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"120\" x=\"156.5\" y=\"116\"/><ellipse cx=\"171.5\" cy=\"132\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M174.4688,137.6406 Q173.8906,137.9375 173.25,138.0859 Q172.6094,138.2344 171.9063,138.2344 Q169.4063,138.2344 168.0859,136.5859 Q166.7656,134.9375 166.7656,131.8125 Q166.7656,128.6875 168.0859,127.0313 Q169.4063,125.375 171.9063,125.375 Q172.6094,125.375 173.2578,125.5313 Q173.9063,125.6875 174.4688,125.9844 L174.4688,128.7031 Q173.8438,128.125 173.25,127.8516 Q172.6563,127.5781 172.0313,127.5781 Q170.6875,127.5781 170,128.6484 Q169.3125,129.7188 169.3125,131.8125 Q169.3125,133.9063 170,134.9766 Q170.6875,136.0469 172.0313,136.0469 Q172.6563,136.0469 173.25,135.7734 Q173.8438,135.5 174.4688,134.9219 L174.4688,137.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"88\" x=\"185.5\" y=\"136.1543\">TextResponse</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"157.5\" x2=\"275.5\" y1=\"148\" y2=\"148\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"157.5\" x2=\"275.5\" y1=\"156\" y2=\"156\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"122\" x=\"311.5\" y=\"116\"/><ellipse cx=\"326.5\" cy=\"132\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M329.4688,137.6406 Q328.8906,137.9375 328.25,138.0859 Q327.6094,138.2344 326.9063,138.2344 Q324.4063,138.2344 323.0859,136.5859 Q321.7656,134.9375 321.7656,131.8125 Q321.7656,128.6875 323.0859,127.0313 Q324.4063,125.375 326.9063,125.375 Q327.6094,125.375 328.2578,125.5313 Q328.9063,125.6875 329.4688,125.9844 L329.4688,128.7031 Q328.8438,128.125 328.25,127.8516 Q327.6563,127.5781 327.0313,127.5781 Q325.6875,127.5781 325,128.6484 Q324.3125,129.7188 324.3125,131.8125 Q324.3125,133.9063 325,134.9766 Q325.6875,136.0469 327.0313,136.0469 Q327.6563,136.0469 328.25,135.7734 Q328.8438,135.5 329.4688,134.9219 L329.4688,137.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"90\" x=\"340.5\" y=\"136.1543\">HtmlResponse</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"312.5\" x2=\"432.5\" y1=\"148\" y2=\"148\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"312.5\" x2=\"432.5\" y1=\"156\" y2=\"156\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"48\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"115\" x=\"469\" y=\"116\"/><ellipse cx=\"484\" cy=\"132\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M486.9688,137.6406 Q486.3906,137.9375 485.75,138.0859 Q485.1094,138.2344 484.4063,138.2344 Q481.9063,138.2344 480.5859,136.5859 Q479.2656,134.9375 479.2656,131.8125 Q479.2656,128.6875 480.5859,127.0313 Q481.9063,125.375 484.4063,125.375 Q485.1094,125.375 485.7578,125.5313 Q486.4063,125.6875 486.9688,125.9844 L486.9688,128.7031 Q486.3438,128.125 485.75,127.8516 Q485.1563,127.5781 484.5313,127.5781 Q483.1875,127.5781 482.5,128.6484 Q481.8125,129.7188 481.8125,131.8125 Q481.8125,133.9063 482.5,134.9766 Q483.1875,136.0469 484.5313,136.0469 Q485.1563,136.0469 485.75,135.7734 Q486.3438,135.5 486.9688,134.9219 L486.9688,137.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"83\" x=\"498\" y=\"136.1543\">XmlResponse</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"470\" x2=\"583\" y1=\"148\" y2=\"148\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"470\" x2=\"583\" y1=\"156\" y2=\"156\"/><path d=\"M63.5,76.0236 C63.5,89.5792 63.5,104.0381 63.5,115.6784 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"none\" points=\"56.5001,76.0005,63.5,56,70.5001,76.0004,56.5001,76.0005\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M322.051,67.2792 C298.66,83.1734 271.475,101.6451 250.553,115.8617 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"none\" points=\"318.174,61.451,338.65,56,326.042,73.0307,318.174,61.451\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M372.5,76.0236 C372.5,89.5792 372.5,104.0381 372.5,115.6784 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"none\" points=\"365.5,76.0005,372.5,56,379.5,76.0004,365.5,76.0005\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M422.64,67.512 C445.655,83.3535 472.33,101.7145 492.884,115.8617 \" fill=\"none\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><polygon fill=\"none\" points=\"418.421,73.1062,405.916,56,426.359,61.574,418.421,73.1062\" style=\"stroke: #A80036; stroke-width: 1.0;\"/></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "Request <|-- FormRequest\n",
    "Response <|-- TextResponse\n",
    "Response <|-- HtmlResponse\n",
    "Response <|-- XmlResponse\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "resp=scrapy.http.Request(url=\"http://www.example.com/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scrapy.http.request.Request"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.7.3 callback\n",
    "\n",
    "* scrapy.http.Request에 callback 함수를 정의할 수 있다.\n",
    "* Request가 실행되어 Response를 가져오면, callback함수가 호출된다.\n",
    "\n",
    "```\n",
    "scrapy.Request(\"http://www.example.com/some_page.html\",\n",
    "    callback=self.parse_this)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 1.7.4 Selectors\n",
    "\n",
    "* 모든 항목 추출 '.extract()'\n",
    "* 첫째 항목 추출 '.extract_first()'\n",
    "\n",
    "* xpath .text()\n",
    "* CSS selectors는 text or attribute nodes를 'CSS3 pseudo-elements' '::'를 사용해서 추출\n",
    "'::selection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "class scrapy.selector.Selector {\n",
    "    xpath(query)\n",
    "    css(query)\n",
    "    re(regex)\n",
    "    extract()\n",
    "}\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "htmlstr='''<html>\n",
    " <head>\n",
    "  <title>My Website</title>\n",
    " </head>\n",
    " <body>\n",
    "  <span>Hello world!!!</span>\n",
    "  <div class='links'>\n",
    "   <a href='one.html'>Link 1<img src='image1.jpg'/>Name: test image name</a>\n",
    "   <a href='two.html'>Link 2<img src='image2.jpg'/></a>\n",
    "   <a href='three.html'>Link 3<img src='image3.jpg'/></a>\n",
    "  </div>\n",
    " </body>\n",
    "</html>'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Hello world!!!']\n",
      "[u'Hello world!!!']\n"
     ]
    }
   ],
   "source": [
    "from scrapy.selector import Selector\n",
    "from scrapy.http import HtmlResponse\n",
    "\n",
    "print Selector(text=htmlstr).xpath('//span/text()').extract()\n",
    "print Selector(text=htmlstr).css('span::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'Link 1', u'Name: test image name', u'Link 2', u'Link 3']\n",
      "[u'Link 1', u'Name: test image name', u'Link 2', u'Link 3']\n"
     ]
    }
   ],
   "source": [
    "print Selector(text=htmlstr).xpath('//div[@class=\"links\"]/a/text()').extract()\n",
    "print Selector(text=htmlstr).css('.links > a::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).xpath('//a[contains(@href, \"image\")]/text()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'test image name']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).xpath('//a[contains(@href, \"html\")]/text()').re(r'Name:\\s*(.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'<title>My Website</title>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).css('title').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'My Website']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).css('title::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'one.html', u'two.html', u'three.html']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).xpath('//a[contains(@href, \"html\")]/@href').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'one.html', u'two.html', u'three.html']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Selector(text=htmlstr).css('a[href*=html]::attr(href)').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_web_data_hello_scrapy.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_hello_scrapy.py\n",
    "import scrapy\n",
    "\n",
    "class myItem(scrapy.item.Item):\n",
    "    title = scrapy.item.Field()\n",
    "\n",
    "class myCrawler(scrapy.spiders.Spider):\n",
    "    name = 'myAppName'\n",
    "    start_urls = ['https://data.go.kr']\n",
    "    def parse(self, response):\n",
    "        item = myItem()\n",
    "        #title = scrapy.selector.Selector(response).xpath('//title/text()')\n",
    "        title = scrapy.selector.Selector(response).css('title').extract()\n",
    "        print \"---Hello---\", title\n",
    "        item['title']=title\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scrapy crawl은 프로젝트에서 사용하는 명령어, 프로젝트를 생성하지 않고 하나의 파일로 실습하였기 때문에 오류.\n",
    "```\n",
    "scrapy crawl <spider>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 1.3.2 - no active project\r\n",
      "\r\n",
      "Unknown command: crawl\r\n",
      "\r\n",
      "Use \"scrapy\" to see available commands\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl myAppName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scrapy 실행\n",
    "    * scrapy runspider를 실행하면, 화면에 결과를 볼 수 있다. (윈도우에서 실행 ok!)\n",
    "```\n",
    "scrapy runspider src/ds_web_data_hello_scrapy.py\n",
    "```\n",
    "        \n",
    "    * 가져온 데이터는 --output에, 로그는 --logfile에 저장된다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-03-16 06:58:36 [scrapy.utils.log] INFO: Scrapy 1.3.2 started (bot: scrapybot)\n",
      "2017-03-16 06:58:36 [scrapy.utils.log] INFO: Overridden settings: {}\n",
      "2017-03-16 06:58:36 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.corestats.CoreStats']\n",
      "2017-03-16 06:58:36 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2017-03-16 06:58:36 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2017-03-16 06:58:36 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2017-03-16 06:58:36 [scrapy.core.engine] INFO: Spider opened\n",
      "2017-03-16 06:58:36 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2017-03-16 06:58:36 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023\n",
      "2017-03-16 06:58:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://data.go.kr> (referer: None)\n",
      "---Hello--- [u'<title>\\r\\n    \\r\\n        \\r\\n        \\r\\n            \\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138\\r\\n        \\r\\n    \\r\\n\\r\\n</title>']\n",
      "2017-03-16 06:58:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://data.go.kr>\n",
      "{'title': [u'<title>\\r\\n    \\r\\n        \\r\\n        \\r\\n            \\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138\\r\\n        \\r\\n    \\r\\n\\r\\n</title>']}\n",
      "2017-03-16 06:58:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2017-03-16 06:58:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 208,\n",
      " 'downloader/request_count': 1,\n",
      " 'downloader/request_method_count/GET': 1,\n",
      " 'downloader/response_bytes': 49601,\n",
      " 'downloader/response_count': 1,\n",
      " 'downloader/response_status_count/200': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2017, 3, 15, 21, 58, 37, 630240),\n",
      " 'item_scraped_count': 1,\n",
      " 'log_count/DEBUG': 3,\n",
      " 'log_count/INFO': 7,\n",
      " 'response_received_count': 1,\n",
      " 'scheduler/dequeued': 1,\n",
      " 'scheduler/dequeued/memory': 1,\n",
      " 'scheduler/enqueued': 1,\n",
      " 'scheduler/enqueued/memory': 1,\n",
      " 'start_time': datetime.datetime(2017, 3, 15, 21, 58, 36, 96440)}\n",
      "2017-03-16 06:58:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_hello_scrapy.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 데이터의 저장\n",
    "    * JSON\n",
    "    * CSV\n",
    "    * XML\n",
    "    * URI를 사용하여, 파일, S3, ftp 등으로 출력할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Hello--- [u'<title>\\r\\n    \\r\\n        \\r\\n        \\r\\n            \\uacf5\\uacf5\\ub370\\uc774\\ud130\\ud3ec\\ud138\\r\\n        \\r\\n    \\r\\n\\r\\n</title>']\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_hello_scrapy.py --output='src/ds_web_data_hello.csv' -t csv --logfile='src/ds_web_data_hello.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.8 Link Extractors\n",
    "\n",
    "* scrapy.http.Response 객체에서 링크를 추출해서 다음 페이지로 계속하기\n",
    "* scrapy.CrawlSpider를 사용할 수 있다\n",
    "* basic spider에서도 rules을 사용해서 추출할 수 있다.\n",
    "\n",
    "참조\n",
    "https://www.tutorialspoint.com/scrapy/scrapy_following_links.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 웹데이터-1: 페이지가 있는 사이트 크롤링하기\n",
    "\n",
    "* http://doc.scrapy.org/en/1.1/intro/tutorial.html 예제\n",
    "\n",
    "* 절차\n",
    "    * 처음\n",
    "        * start_urls로 Request생성\n",
    "        * callback 함수 호출 - parse()\n",
    "            * xpath 데이터 항목 추출\n",
    "            * iterator생성 (yield)\n",
    "    * 다음\n",
    "        * 다음 페이지 url로 Request생성\n",
    "        * callback 함수 호출 - parse()\n",
    "\n",
    "* 다음 페이지 가져오기\n",
    "    * 주소창에는 '/page/다음페이지'가 호출된다.\n",
    "```\n",
    "http://quotes.toscrape.com/page/2/\n",
    "http://quotes.toscrape.com/page/3/\n",
    "```\n",
    "    \n",
    "    * 콘솔창을 열어 보면, \"Next\"에 다음 페이지의 링크가 걸려 있다.\n",
    "    * '/page/다음페이지' xpath를 추출한다.\n",
    "    * start_urls의 도메인 'http://quotes.toscrape.com/'에 덧붙인다.\n",
    "    * 다음 페이지를 가져오는 yield는 return과 같은 기능이지만, generator이다 (iterator)\n",
    "```\n",
    "<li class=\"next\">...\n",
    "    <a href=\"/page/2/\">...</a>\n",
    "</li>\n",
    "추출하는 xpath는 response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_web_data_paging.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_paging.py\n",
    "import scrapy\n",
    "\n",
    "class QuoteItem(scrapy.Item):\n",
    "    text = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'http://quotes.toscrape.com',\n",
    "    ]\n",
    "    def parse(self, response):\n",
    "        for quote in response.xpath('//div[@class=\"quote\"]'):\n",
    "            item = QuoteItem()\n",
    "            item['text'] = quote.xpath('span[@class=\"text\"]/text()').extract_first()\n",
    "            item['author'] = quote.xpath('span/small/text()').extract_first()\n",
    "            print \"crawling \",item['author']\n",
    "            yield item\n",
    "        next_page = response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n",
    "        if next_page:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            print \"--> visiting \",next_page\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scrapy shell\n",
    "```\n",
    "jsl@jsl-smu$ scrapy shell http://quotes.toscrape.com/page/1/\n",
    "In [1]: response.xpath('//div[@class=\"quote\"]')\n",
    "Out[1]: \n",
    "[<Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>,\n",
    " <Selector xpath='//div[@class=\"quote\"]' data=u'<div class=\"quote\" itemscope itemtype=\"h'>]\n",
    "\n",
    "In [2]: quotes=response.xpath('//div[@class=\"quote\"]')\n",
    "\n",
    "In [3]: for quote in quotes:\n",
    "   ...:     print quote.xpath('span[@class=\"text\"]/text()').extract_first()\n",
    "   ...:     \n",
    "“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”\n",
    "“It is our choices, Harry, that show what we truly are, far more than our abilities.”\n",
    "“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”\n",
    "“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
    "“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\n",
    "“All that is gold does not glitter,\n",
    "“It is better to be hated for what you are than to be loved for what you are not.”\n",
    "“I have not failed. I've just found 10,000 ways that won't work.”\n",
    "“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\n",
    "“A day without sunshine is like, you know, night.”\n",
    "\n",
    "In [4]: response.xpath('//li[@class=\"next\"]/a/@href').extract_first()\n",
    "Out[4]: u'/page/2/'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling  Albert Einstein\n",
      "crawling  J.K. Rowling\n",
      "crawling  Albert Einstein\n",
      "crawling  Jane Austen\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Albert Einstein\n",
      "crawling  André Gide\n",
      "crawling  Thomas A. Edison\n",
      "crawling  Eleanor Roosevelt\n",
      "crawling  Steve Martin\n",
      "--> visiting  http://quotes.toscrape.com/page/2/\n",
      "crawling  Marilyn Monroe\n",
      "crawling  J.K. Rowling\n",
      "crawling  Albert Einstein\n",
      "crawling  Bob Marley\n",
      "crawling  Dr. Seuss\n",
      "crawling  Douglas Adams\n",
      "crawling  Elie Wiesel\n",
      "crawling  Friedrich Nietzsche\n",
      "crawling  Mark Twain\n",
      "crawling  Allen Saunders\n",
      "--> visiting  http://quotes.toscrape.com/page/3/\n",
      "crawling  Pablo Neruda\n",
      "crawling  Ralph Waldo Emerson\n",
      "crawling  Mother Teresa\n",
      "crawling  Garrison Keillor\n",
      "crawling  Jim Henson\n",
      "crawling  Dr. Seuss\n",
      "crawling  Albert Einstein\n",
      "crawling  J.K. Rowling\n",
      "crawling  Albert Einstein\n",
      "crawling  Bob Marley\n",
      "--> visiting  http://quotes.toscrape.com/page/4/\n",
      "crawling  Dr. Seuss\n",
      "crawling  J.K. Rowling\n",
      "crawling  Bob Marley\n",
      "crawling  Mother Teresa\n",
      "crawling  J.K. Rowling\n",
      "crawling  Charles M. Schulz\n",
      "crawling  William Nicholson\n",
      "crawling  Albert Einstein\n",
      "crawling  Jorge Luis Borges\n",
      "crawling  George Eliot\n",
      "--> visiting  http://quotes.toscrape.com/page/5/\n",
      "crawling  George R.R. Martin\n",
      "crawling  C.S. Lewis\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Albert Einstein\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Martin Luther King Jr.\n",
      "crawling  J.K. Rowling\n",
      "crawling  James Baldwin\n",
      "--> visiting  http://quotes.toscrape.com/page/6/\n",
      "crawling  Jane Austen\n",
      "crawling  Eleanor Roosevelt\n",
      "crawling  Marilyn Monroe\n",
      "crawling  Albert Einstein\n",
      "crawling  Haruki Murakami\n",
      "crawling  Alexandre Dumas fils\n",
      "crawling  Stephenie Meyer\n",
      "crawling  Ernest Hemingway\n",
      "crawling  Helen Keller\n",
      "crawling  George Bernard Shaw\n",
      "--> visiting  http://quotes.toscrape.com/page/7/\n",
      "crawling  Charles Bukowski\n",
      "crawling  Suzanne Collins\n",
      "crawling  Suzanne Collins\n",
      "crawling  C.S. Lewis\n",
      "crawling  J.R.R. Tolkien\n",
      "crawling  J.K. Rowling\n",
      "crawling  Ernest Hemingway\n",
      "crawling  Ralph Waldo Emerson\n",
      "crawling  Mark Twain\n",
      "crawling  Dr. Seuss\n",
      "--> visiting  http://quotes.toscrape.com/page/8/\n",
      "crawling  Alfred Tennyson\n",
      "crawling  Charles Bukowski\n",
      "crawling  Terry Pratchett\n",
      "crawling  Dr. Seuss\n",
      "crawling  J.D. Salinger\n",
      "crawling  George Carlin\n",
      "crawling  John Lennon\n",
      "crawling  W.C. Fields\n",
      "crawling  Ayn Rand\n",
      "crawling  Mark Twain\n",
      "--> visiting  http://quotes.toscrape.com/page/9/\n",
      "crawling  Albert Einstein\n",
      "crawling  Jane Austen\n",
      "crawling  J.K. Rowling\n",
      "crawling  Jane Austen\n",
      "crawling  Jane Austen\n",
      "crawling  C.S. Lewis\n",
      "crawling  C.S. Lewis\n",
      "crawling  Mark Twain\n",
      "crawling  Mark Twain\n",
      "crawling  C.S. Lewis\n",
      "--> visiting  http://quotes.toscrape.com/page/10/\n",
      "crawling  J.K. Rowling\n",
      "crawling  Jimi Hendrix\n",
      "crawling  J.M. Barrie\n",
      "crawling  E.E. Cummings\n",
      "crawling  Khaled Hosseini\n",
      "crawling  Harper Lee\n",
      "crawling  Madeleine L'Engle\n",
      "crawling  Mark Twain\n",
      "crawling  Dr. Seuss\n",
      "crawling  George R.R. Martin\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_paging.py -o src/ds_web_data_paging.json -t json --logfile src/ds_web_data_paging.logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 웹데이터-2: 국제학회 목록 크롤링하기\n",
    "\n",
    "* 단일 페이지는 비교적 쉽게 크롤링할 수 있다.\n",
    "* 목록을 가져온다.\n",
    "    * chrome javascript console\n",
    "```\n",
    "> nodes=$$('div.content-r-full table.nogrid-nopad tr')\n",
    "```\n",
    "\n",
    "    * scrapy shell\n",
    "```\n",
    "$ scrapy shell 'http://www.ieee.org/conferences_events/conferences/search/index.html'\n",
    ">>> nodes=response.css('div.content-r-full table.nogrid-nopad tr')\n",
    ">>> for node in nodes:\n",
    "       print node.css('p > a[href]::text').extract()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 페이징 기능이 포함된 pagination\n",
    "    * 페이지 1 2 3 ... 일련번호 seletor를 찾는다\n",
    "        * ieee 페이지는 '|>'를 찾아야 한다.\n",
    "        * .pagination > a[href]의 몇 번째인지 수작업으로 찾는다 (12번째?)\n",
    "    * href의 값을 xpath에서 꺼낸다.\n",
    "    ```\n",
    "    response.css('.pagination > a[href]:nth-child(10)').xpath('@href').extract_first()\n",
    "    ```\n",
    "    \n",
    "    * 이 href가 start_urls와 합쳐져서 다음 페이지로 링크가 만들어지고, 이 링크를 검색한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 해결 - for문 처음에 crawling 6[]이 왜?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_web_data_ieee.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_ieee.py\n",
    "import scrapy\n",
    "\n",
    "class IeeeItem(scrapy.Item):\n",
    "    confName = scrapy.Field()\n",
    "    confDate = scrapy.Field()\n",
    "    confPlace = scrapy.Field()\n",
    "\n",
    "class IeeeSpider(scrapy.Spider):\n",
    "    name = \"ieeeConf\"\n",
    "    start_urls = [\n",
    "        'http://www.ieee.org/conferences_events/conferences/search/index.html'\n",
    "    ]\n",
    "    def parse(self, response):\n",
    "        nodes=response.css('div.content-r-full table.nogrid-nopad tr')\n",
    "        #nodes=response.css('div.content-r-full table.nogrid-nopad tr.even')\n",
    "        for node in nodes:\n",
    "            ahrefs=node.css('p>a[href]')\n",
    "            print len(ahrefs),\n",
    "            item = IeeeItem()\n",
    "            item['confName'] = ahrefs[0].css('::text').extract()\n",
    "            item['confDate'] = ahrefs[1].css('::text').extract()\n",
    "            item['confPlace'] = ahrefs[2].css('::text').extract()\n",
    "            #print \"crawling \",item['confName']\n",
    "            yield item\n",
    "        next_page = response.css('.pagination > a[href]:nth-child(12)').xpath('@href').extract_first() \n",
    "        if next_page:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            print \"--> visiting \",next_page\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 페이지가 많으면 프로그램이 오랫동안 실행된다. 명령창에서 실행하면서 중단할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!scrapy runspider src/ds_web_data_ieee.py -o src/ds_web_data_ieee.json -t json --logfile src/ds_web_data_ieee.logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.9 Settings\n",
    "\n",
    "방법 | 설명\n",
    "-----|-----\n",
    "command line으로 설정 | -s LOG_FILE=scrapy.log\n",
    "Spider에 설정 | custom_settings (참조: 아래 예제)\n",
    "project 설정 | settings.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class MySpider(scrapy.Spider):\n",
    "    name = 'myspider'\n",
    "    start_urls = ['http://example.com']\n",
    "    def parse(self, response):\n",
    "        print(\"Existing settings: %s\" % self.settings.attributes.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MySpider' object has no attribute 'settings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9199bfcd0cbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMySpider\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscrapy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_urls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-b288209cb238>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, response)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstart_urls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'http://example.com'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Existing settings: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattributes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'MySpider' object has no attribute 'settings'"
     ]
    }
   ],
   "source": [
    "my=MySpider()\n",
    "resp=scrapy.http.Request(my.start_urls[0])\n",
    "my.parse(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://example.com']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my.start_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.10 Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 웹데이터-3: Reddit 크롤링하기\n",
    "\n",
    "* small project는 그냥 아래처럼 만듦\n",
    "* Items\n",
    "* settings.py\n",
    "    custome_settings\n",
    "* link를 따라가면서 크롤링할 경우 위 문제 참조\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_web_data_textpost.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_textpost.py\n",
    "import scrapy\n",
    "\n",
    "class TextPostItem(scrapy.item.Item):\n",
    "    title = scrapy.item.Field()\n",
    "    url = scrapy.item.Field()\n",
    "    submitted = scrapy.item.Field()\n",
    "\n",
    "class RedditCrawler(scrapy.spiders.CrawlSpider):\n",
    "    name = 'reddit_crawler'\n",
    "    allowed_domains = ['reddit.com']\n",
    "    start_urls = ['https://www.reddit.com/r/learnpython/new']\n",
    "    custom_settings = {\n",
    "        'BOT_NAME': 'reddit-scraper',\n",
    "        'DEPTH_LIMIT': 3,\n",
    "        'DOWNLOAD_DELAY': 3\n",
    "    }\n",
    "    def parse(self, response):\n",
    "        s = scrapy.selector.Selector(response)\n",
    "        next_link = s.xpath('//span[@class=\"nextprev\"]//a/@href').extract()[0]\n",
    "        if len(next_link):\n",
    "            print \"--> visiting \",next_link\n",
    "            yield self.make_requests_from_url(next_link)\n",
    "        posts = scrapy.selector.Selector(response).xpath('//div[@id=\"siteTable\"]/div[@onclick=\"click_thing(this)\"]')\n",
    "        for post in posts:\n",
    "            i = TextPostItem()\n",
    "            i['title'] = post.xpath('div[2]/p[1]/a/text()').extract()[0]\n",
    "            i['url'] = post.xpath('div[2]/ul/li[1]/a/@href').extract()[0]\n",
    "            i['submitted'] = post.xpath('div[2]/p[2]/time/@title').extract()[0]\n",
    "            print \"crawling \",i['title']\n",
    "            yield i\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 출력없이 실행\n",
    "    * log 저장, 출력 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> visiting  https://www.reddit.com/r/learnpython/new?count=25&after=t3_533qt2\n",
      "crawling  Separate lines to words\n",
      "crawling  First time writing a bot using Python, not sure where I'm going wrong. [x-post /r/learnprogramming]\n",
      "crawling  Having trouble with the eval function, what should I try next?\n",
      "crawling  Bizarre music note symbols instead of newlines on pywin32 with Word\n",
      "crawling  Checking for anagrams\n",
      "crawling  Memoization example - dictionary inside method, or outside?\n",
      "crawling  Tensorboard Explained in 5 Min\n",
      "crawling  Script running the both if and else statement EVERYTIME and also printing \\n and \\b\n",
      "crawling  [Newbie] Code to remove vowels from string\n",
      "crawling  Does PyQt5 or PySide currently work with the most recent Qt 5 QML?\n",
      "crawling  benefits of “from X import *” on “import X” if there are any\n",
      "crawling  What's wrong with this code?\n",
      "crawling  How do I get the if to run\n",
      "crawling  can someone please give me a clue on how to append the return value of a function into a list\n",
      "crawling  Reading \"Test-Driven Development\" - Skip Selenium when using a REST API?\n",
      "crawling  [Help] Sack problem\n",
      "crawling  Anyone have experience using Python-Whois?\n",
      "crawling  Trying to unrar rar files recursively and remove the files afterwards. But gettings error in sub-dirs\n",
      "crawling  find executable with distutils.spawn\n",
      "crawling  I'm getting an error 503 in this program\n",
      "crawling  Help with randrange function (complete beginner!)\n",
      "crawling  Correct way to handle unicode JSON parsing between Python2 and Python3?\n",
      "crawling  Trouble installing python-docx.\n",
      "crawling  Best way to install PyQt5 and Py Creator\n",
      "crawling  is it possible to run a compiled (py2exe, pyinstaller) python script as a standalone windows service (= without a wrapper)?\n",
      "--> visiting  https://www.reddit.com/r/learnpython/new?count=26&before=t3_533pq0\n",
      "crawling  Help with elif statement.\n",
      "crawling  How to understand code faster?\n",
      "crawling  Recursively finding keys in complicated dictionary.\n",
      "crawling  While [object]: I don't get it\n",
      "crawling  Loop through website pages\n",
      "crawling  MIT Problem Set 1.3 -- Python\n",
      "crawling  Beginner question - abs() causing infinite recursion\n",
      "crawling  Downloaded a script, having trouble with selenium chromedriver?\n",
      "crawling  Question with Pycurl and how to check if a file is completely downloaded before opening it.\n",
      "crawling  Which is more pythonic? (values from a tuple)\n",
      "crawling  Looping with Indices\n",
      "crawling  HL7 to JSON converter\n",
      "crawling  Navigating pagination with Python 2.7 and selenium\n",
      "crawling  Need help webscraping\n",
      "crawling  eval(int(input())) or int(input())\n",
      "crawling  Stuck! Please help\n",
      "crawling  write a json...\n",
      "crawling  __init__ doesn't get called as soon as the instance is created\n",
      "crawling  Faster algorithm for Matrix factorization with regularization as described here\n",
      "crawling  Is there any difference between != and <> operators?\n",
      "crawling  Is Python the right for my project?\n",
      "crawling  Help with my first simple GUI\n",
      "crawling  Max retries exceeded with url (with sleep)\n",
      "crawling  Posting large amounts of code for review(?)\n",
      "crawling  Is there any python version for Windows without installation requirements.\n",
      "--> visiting  https://www.reddit.com/r/learnpython/new?count=25&after=t3_533qt2\n",
      "crawling  Separate lines to words\n",
      "crawling  First time writing a bot using Python, not sure where I'm going wrong. [x-post /r/learnprogramming]\n",
      "crawling  Having trouble with the eval function, what should I try next?\n",
      "crawling  Bizarre music note symbols instead of newlines on pywin32 with Word\n",
      "crawling  Checking for anagrams\n",
      "crawling  Memoization example - dictionary inside method, or outside?\n",
      "crawling  Tensorboard Explained in 5 Min\n",
      "crawling  Script running the both if and else statement EVERYTIME and also printing \\n and \\b\n",
      "crawling  [Newbie] Code to remove vowels from string\n",
      "crawling  Does PyQt5 or PySide currently work with the most recent Qt 5 QML?\n",
      "crawling  benefits of “from X import *” on “import X” if there are any\n",
      "crawling  What's wrong with this code?\n",
      "crawling  How do I get the if to run\n",
      "crawling  can someone please give me a clue on how to append the return value of a function into a list\n",
      "crawling  Reading \"Test-Driven Development\" - Skip Selenium when using a REST API?\n",
      "crawling  [Help] Sack problem\n",
      "crawling  Anyone have experience using Python-Whois?\n",
      "crawling  Trying to unrar rar files recursively and remove the files afterwards. But gettings error in sub-dirs\n",
      "crawling  find executable with distutils.spawn\n",
      "crawling  I'm getting an error 503 in this program\n",
      "crawling  Help with randrange function (complete beginner!)\n",
      "crawling  Correct way to handle unicode JSON parsing between Python2 and Python3?\n",
      "crawling  Trouble installing python-docx.\n",
      "crawling  Best way to install PyQt5 and Py Creator\n",
      "crawling  is it possible to run a compiled (py2exe, pyinstaller) python script as a standalone windows service (= without a wrapper)?\n",
      "--> visiting  https://www.reddit.com/r/learnpython/new?count=26&before=t3_533pq0\n",
      "crawling  Help with elif statement.\n",
      "crawling  How to understand code faster?\n",
      "crawling  Recursively finding keys in complicated dictionary.\n",
      "crawling  While [object]: I don't get it\n",
      "crawling  Loop through website pages\n",
      "crawling  MIT Problem Set 1.3 -- Python\n",
      "crawling  Beginner question - abs() causing infinite recursion\n",
      "crawling  Downloaded a script, having trouble with selenium chromedriver?\n",
      "crawling  Question with Pycurl and how to check if a file is completely downloaded before opening it.\n",
      "crawling  Which is more pythonic? (values from a tuple)\n",
      "crawling  Looping with Indices\n",
      "crawling  HL7 to JSON converter\n",
      "crawling  Navigating pagination with Python 2.7 and selenium\n",
      "crawling  Need help webscraping\n",
      "crawling  eval(int(input())) or int(input())\n",
      "crawling  Stuck! Please help\n",
      "crawling  write a json...\n",
      "crawling  __init__ doesn't get called as soon as the instance is created\n",
      "crawling  Faster algorithm for Matrix factorization with regularization as described here\n",
      "crawling  Is there any difference between != and <> operators?\n",
      "crawling  Is Python the right for my project?\n",
      "crawling  Help with my first simple GUI\n",
      "crawling  Max retries exceeded with url (with sleep)\n",
      "crawling  Posting large amounts of code for review(?)\n",
      "crawling  Is there any python version for Windows without installation requirements.\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_textpost.py -o src/ds_web_data_textpost.json -t json --logfile src/ds_web_data_textpost.logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.11 Pipelines\n",
    "\n",
    "* Item클래스의 데이터항목이 추출되고 나면, 다음 단계로 Pipeline클래스를 실행할 수 있다.\n",
    "* Pipeline은 추출 데이터 항목의 정련, 검증 또는 데이터베이스에 저장하는 용도로 사용한다.\n",
    "* Pipeline은 클래스로 구현한다. callback으로 'process_item()'이 호출된다.\n",
    "```\n",
    "class Pipeline(object):\n",
    "    def process_item(self, item, spider):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"116px\" style=\"width:210px;height:116px;\" version=\"1.1\" viewBox=\"0 0 210 116\" width=\"210px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><filter height=\"300%\" id=\"f1\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"99.2188\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"195\" x=\"6\" y=\"8\"/><ellipse cx=\"74.75\" cy=\"24\" fill=\"#ADD1B2\" rx=\"11\" ry=\"11\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><path d=\"M77.7188,29.6406 Q77.1406,29.9375 76.5,30.0859 Q75.8594,30.2344 75.1563,30.2344 Q72.6563,30.2344 71.3359,28.5859 Q70.0156,26.9375 70.0156,23.8125 Q70.0156,20.6875 71.3359,19.0313 Q72.6563,17.375 75.1563,17.375 Q75.8594,17.375 76.5078,17.5313 Q77.1563,17.6875 77.7188,17.9844 L77.7188,20.7031 Q77.0938,20.125 76.5,19.8516 Q75.9063,19.5781 75.2813,19.5781 Q73.9375,19.5781 73.25,20.6484 Q72.5625,21.7188 72.5625,23.8125 Q72.5625,25.9063 73.25,26.9766 Q73.9375,28.0469 75.2813,28.0469 Q75.9063,28.0469 76.5,27.7734 Q77.0938,27.5 77.7188,26.9219 L77.7188,29.6406 Z \"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"12\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"49\" x=\"95.25\" y=\"28.1543\">Pipeline</text><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"7\" x2=\"200\" y1=\"40\" y2=\"40\"/><line style=\"stroke: #A80036; stroke-width: 1.5;\" x1=\"7\" x2=\"200\" y1=\"48\" y2=\"48\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"183\" x=\"12\" y=\"62.2104\">process_item(self, item, spider)</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"143\" x=\"12\" y=\"75.0151\">open_spider(self, spider)</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"145\" x=\"12\" y=\"87.8198\">close_spider(self, spider)</text><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"11\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"149\" x=\"12\" y=\"100.6245\">from_crawler(cls, crawler)</text></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "class Pipeline {\n",
    "    process_item(self, item, spider)\n",
    "    open_spider(self, spider)\n",
    "    close_spider(self, spider)\n",
    "    from_crawler(cls, crawler)\n",
    "}\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* Pipeline을 settings.py에 등록해 놓아야 실행된다.\n",
    "    * ITEM_PIPELINES는 dict로 설정, 숫자는 우선순위.\n",
    "```\n",
    "ITEM_PIPELINES = {'myproject.pipelines.myPipeline': 100}\n",
    "```\n",
    "\n",
    "* 현재 파일의 custome_settings로 설정할 수 있다.\n",
    "    * 현재 프로그램의 파일명을 사용해서 Pipeline클래스를 지정한다.\n",
    "    * ITEM_PIPELINES는 사전에 정의된 키\n",
    "    * MONGO관련 키는 사용자가 정의할 수 있다.\n",
    "```\n",
    "\"ITEM_PIPELINES\":{'ds_web_data_ieee_pipeline.IeeePipeline': 100}\n",
    "\"MONGO_URI\":\"localhost:27017\",\n",
    "\"MONGO_DATABASE\":\"ieee\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile src/tmp.py\n",
    "import os\n",
    "fname=os.path.basename(__file__)\n",
    "print fname\n",
    "print os.path.splitext(fname)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tmp.py\r\n",
      "tmp\r\n"
     ]
    }
   ],
   "source": [
    "!python src/tmp.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Pipeline클래스를 사용해서 파일과 MongoDB에 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/ds_web_data_ieee_pipeline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_ieee_pipeline.py\n",
    "import scrapy\n",
    "import json\n",
    "#import io\n",
    "import os\n",
    "import logging\n",
    "import pymongo\n",
    "\n",
    "class IeeeItem(scrapy.Item):\n",
    "    confName = scrapy.Field()\n",
    "    confDate = scrapy.Field()\n",
    "    confPlace = scrapy.Field()\n",
    "\n",
    "class IeeeSpider(scrapy.Spider):\n",
    "    name = \"ieeeConf\"\n",
    "    start_urls = [\n",
    "        'http://www.ieee.org/conferences_events/conferences/search/index.html'\n",
    "    ]\n",
    "    custom_settings = {\n",
    "        \"ITEM_PIPELINES\":{\n",
    "            os.path.splitext(os.path.basename(__file__))[0]+\".\"+\"IeeeFilePipeline\":100,\n",
    "            os.path.splitext(os.path.basename(__file__))[0]+\".\"+\"IeeeMongoPipeline\":200\n",
    "        },\n",
    "        \"LOG_LEVEL\":\"CRITICAL\",\n",
    "        \"MONGO_URI\":\"localhost:27017\",\n",
    "        \"MONGO_DATABASE\":\"ieee\"\n",
    "    }\n",
    "    def parse(self, response):\n",
    "        nodes=response.css('div.content-r-full table.nogrid-nopad tr')\n",
    "        #nodes=response.css('div.content-r-full table.nogrid-nopad tr.even')\n",
    "        for node in nodes:\n",
    "            ahrefs=node.css('p>a[href]')\n",
    "            print len(ahrefs),\n",
    "            item = IeeeItem()\n",
    "            item['confName'] = ahrefs[0].css('::text').extract()\n",
    "            item['confDate'] = ahrefs[1].css('::text').extract()\n",
    "            item['confPlace'] = ahrefs[2].css('::text').extract()\n",
    "            #print \"crawling \",item['confName']\n",
    "            yield item\n",
    "        next_page = response.css('.pagination > a[href]:nth-child(12)').xpath('@href').extract_first() \n",
    "        if next_page:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            print \"--> visiting\",next_page\n",
    "            yield scrapy.Request(next_page, callback=self.parse)\n",
    "\n",
    "class IeeeFilePipeline(object):\n",
    "    def open_spider(self, spider):\n",
    "        print \"-----> opening file to dump...\"\n",
    "        self.file = open(os.path.join('src','ds_web_data_ieee_pipeline_file.json'), 'w')\n",
    "    def process_item(self, item, spider):\n",
    "        #print \"-----> dumping\"\n",
    "        line = json.dumps(dict(item), ensure_ascii=False) + \"\\n\"\n",
    "        self.file.write(line)\n",
    "        return item\n",
    "    def close_spider(self, spider):\n",
    "        print \"-----> dumped and closing file...\"\n",
    "        self.file.close()\n",
    "\n",
    "class IeeeMongoPipeline(object):\n",
    "    collection_name = 'conf'\n",
    "    def __init__(self, mongo_uri, mongo_db):      \n",
    "        self.mongo_uri = mongo_uri\n",
    "        self.mongo_db = mongo_db\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        return cls(\n",
    "            mongo_uri=crawler.settings.get('MONGO_URI'),\n",
    "            mongo_db=crawler.settings.get('MONGO_DATABASE', 'ieee')\n",
    "        )\n",
    "    def open_spider(self, spider):\n",
    "        print \"-----> connecting to Mongo...\"\n",
    "        self.client = pymongo.MongoClient(self.mongo_uri)\n",
    "        self.db = self.client[self.mongo_db]\n",
    "    def close_spider(self, spider):\n",
    "        print \"-----> closing Mongo...\"\n",
    "        self.client.close()\n",
    "    def process_item(self, item, spider):\n",
    "        self.db[self.collection_name].insert(dict(item))\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> opening file to dump...\n",
      "-----> connecting to Mongo...\n",
      "6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 --> visiting http://www.ieee.org/conferences_events/conferences/search/index.html?RANGE_FROM_DATE=&RANGE_TO_DATE=&KEYWORDS=&COUNTRY=ALL&STATE=ALL&CITY=ALL&REGION=ALL&RECORD_NUM=ALL&SPONSOR=ALL&EXHIBIT=ALL&TUTORIALS=ALL&RowsPerPage=20&PageLinkNum=10&ActivePage=1&SORTORDER=DESC&SORTFIELD=START_DATE&ROWSTART=0&CONF_SRCH_RDO=\n",
      "6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 --> visiting http://www.ieee.org/conferences_events/conferences/search/index.html?RANGE_FROM_DATE=&RANGE_TO_DATE=&KEYWORDS=&COUNTRY=ALL&STATE=ALL&CITY=ALL&REGION=ALL&RECORD_NUM=ALL&SPONSOR=ALL&EXHIBIT=ALL&TUTORIALS=ALL&RowsPerPage=20&PageLinkNum=10&ActivePage=845&SORTORDER=DESC&SORTFIELD=START_DATE&ROWSTART=16880&CONF_SRCH_RDO=\n",
      "6 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 -----> closing Mongo...\n",
      "-----> dumped and closing file...\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_ieee_pipeline.py --logfile src/ds_web_data_ieee_pipeline.logfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 연습 웹데이터-1: 코스피200 데이터 가져오기\n",
    "\n",
    "* Google Finance에서 Scrapy를 사용해서 가져오기\n",
    "\n",
    "* [구글 Finance](https://www.google.com/finance/)로 가서 \n",
    "    * 검색창에 KOSPI 또는 KOSPI200\n",
    "    * 좌측 메뉴에 index > Historical prices\n",
    "\n",
    "* Google Finance KOSPI200 데이터 크롤링\n",
    "    * 구를 개발자도구에서 xpath찾아서 (//*[@id=\"fjfe-click-wrapper\"] 여기를 찾아서 클릭)\n",
    "        * 가격\n",
    "        * 볼륨\n",
    "```\n",
    "scrapy shell https://www.google.com/finance/historical?q=KRX%3AKOSPI200\n",
    ">>> response.xpath('//td[@class=\"rgt\"]/text()')\n",
    ">>> [float(x.extract().strip()) for x in response.xpath('//td[@class=\"rgt\"]/text()')]\n",
    ">>> [int(x.extract().strip().replace(',','')) for x in response.xpath('//td[@class=\"rgt rm\"]/text()')]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 참조 src/ds_web_data_kospi200.py\n",
    "    * parse()\n",
    "        * scrapy가 url에 대해 request하고 그 결과로 response를 돌려받음.\n",
    "        * response를 처리해서 item으로 넘겨주는 역할. urls을 여러 개 있는 경우 처리.\n",
    "    * pipeline\n",
    "        * 별도의 파일에 저장했을 경우 path를 써줌\n",
    "        * 같은 파일에 저장했을 경우 path는 현재 파일의 'class DailyStockPipeline'\n",
    "        * The pipeline function's signature looks like this:\n",
    "            ```\n",
    "            def process_item(self, item, spider):\n",
    "            ```\n",
    "\n",
    "        * item is the input to the pipeline and once you have done processing with the item you have to pass it to the next pipeline by returning item in that function.\n",
    "        * set up settings in the ITEM_PIPELINES array.\n",
    "            ```\n",
    "            custom_settings = {\n",
    "                'ITEM_PIPELINES' : { 'ds_web_data_kospi200.DailyStockPipeline': 300, }\n",
    "            }\n",
    "            ```\n",
    "            \n",
    "    * sqlited에서 결과 확인\n",
    "* 'pipeline' 클래스가 있어서 현재 디렉토리를 사용해서 실습 (src/가 아님)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import iplantuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"240px\" style=\"width:513px;height:240px;\" version=\"1.1\" viewBox=\"0 0 513 240\" width=\"513px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs><filter height=\"300%\" id=\"f1\" width=\"300%\" x=\"-1\" y=\"-1\"><feGaussianBlur result=\"blurOut\" stdDeviation=\"2.0\"/><feColorMatrix in=\"blurOut\" result=\"blurOut2\" type=\"matrix\" values=\"0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 .4 0\"/><feOffset dx=\"4.0\" dy=\"4.0\" in=\"blurOut2\" result=\"blurOut3\"/><feBlend in=\"SourceGraphic\" in2=\"blurOut3\" mode=\"normal\"/></filter></defs><g><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"35\" x2=\"35\" y1=\"38.2969\" y2=\"201.8281\"/><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"139\" x2=\"139\" y1=\"38.2969\" y2=\"201.8281\"/><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"337\" x2=\"337\" y1=\"38.2969\" y2=\"201.8281\"/><line style=\"stroke: #A80036; stroke-width: 1.0; stroke-dasharray: 5.0,5.0;\" x1=\"436\" x2=\"436\" y1=\"38.2969\" y2=\"201.8281\"/><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"51\" x=\"8\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"37\" x=\"15\" y=\"22.9951\">client</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"51\" x=\"8\" y=\"200.8281\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"37\" x=\"15\" y=\"220.8232\">client</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"129\" x=\"73\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"115\" x=\"80\" y=\"22.9951\">DailyStockSpider</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"129\" x=\"73\" y=\"200.8281\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"115\" x=\"80\" y=\"220.8232\">DailyStockSpider</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"31\" x=\"320\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"17\" x=\"327\" y=\"22.9951\">url</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"31\" x=\"320\" y=\"200.8281\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"17\" x=\"327\" y=\"220.8232\">url</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"139\" x=\"365\" y=\"3\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"125\" x=\"372\" y=\"22.9951\">DailyStockPipeline</text><rect fill=\"#FEFECE\" filter=\"url(#f1)\" height=\"30.2969\" style=\"stroke: #A80036; stroke-width: 1.5;\" width=\"139\" x=\"365\" y=\"200.8281\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"14\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"125\" x=\"372\" y=\"220.8232\">DailyStockPipeline</text><polygon fill=\"#A80036\" points=\"127.5,50.2969,137.5,54.2969,127.5,58.2969,131.5,54.2969\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"35.5\" x2=\"133.5\" y1=\"54.2969\" y2=\"54.2969\"/><polygon fill=\"#A80036\" points=\"325.5,79.2969,335.5,83.2969,325.5,87.2969,329.5,83.2969\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"139.5\" x2=\"331.5\" y1=\"83.2969\" y2=\"83.2969\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"174\" x=\"146.5\" y=\"78.3638\">scrapy.Request for each url</text><polygon fill=\"#A80036\" points=\"150.5,108.4297,140.5,112.4297,150.5,116.4297,146.5,112.4297\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"144.5\" x2=\"336.5\" y1=\"112.4297\" y2=\"112.4297\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"59\" x=\"156.5\" y=\"107.4966\">response</text><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"139.5\" x2=\"181.5\" y1=\"141.6953\" y2=\"141.6953\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"181.5\" x2=\"181.5\" y1=\"141.6953\" y2=\"154.6953\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"140.5\" x2=\"181.5\" y1=\"154.6953\" y2=\"154.6953\"/><polygon fill=\"#A80036\" points=\"150.5,150.6953,140.5,154.6953,150.5,158.6953,146.5,154.6953\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"105\" x=\"146.5\" y=\"136.6294\">parse(response)</text><polygon fill=\"#A80036\" points=\"424.5,179.6953,434.5,183.6953,424.5,187.6953,428.5,183.6953\" style=\"stroke: #A80036; stroke-width: 1.0;\"/><line style=\"stroke: #A80036; stroke-width: 1.0;\" x1=\"139.5\" x2=\"430.5\" y1=\"183.6953\" y2=\"183.6953\"/><text fill=\"#000000\" font-family=\"sans-serif\" font-size=\"13\" lengthAdjust=\"spacingAndGlyphs\" textLength=\"96\" x=\"146.5\" y=\"178.7622\">process_item()</text></g></svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%plantuml\n",
    "@startuml\n",
    "client -> DailyStockSpider\n",
    "DailyStockSpider -> url:scrapy.Request for each url\n",
    "url -> DailyStockSpider:scrapy.http.Response\n",
    "DailyStockSpider -> DailyStockSpider:parse(response)\n",
    "DailyStockSpider -> DailyStockPipeline:process_item()\n",
    "@enduml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ds_web_data_kospi200.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds_web_data_kospi200.py\n",
    "import scrapy\n",
    "import dateutil.parser\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "class DailyStockItem(scrapy.Item):\n",
    "    symbol = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    price_open = scrapy.Field()\n",
    "    price_high = scrapy.Field()\n",
    "    price_low = scrapy.Field()\n",
    "    price_close = scrapy.Field()\n",
    "    volume = scrapy.Field()\n",
    "\n",
    "class DailyStockPipeline(object):\n",
    "    #filename = 'dailystock.sqlite'\n",
    "    filename = 'ds_web_data_kospi200.sqlite'\n",
    "\n",
    "    def __init__(self):\n",
    "        self.conn = None\n",
    "        if os.path.exists(self.filename):\n",
    "            self.conn = sqlite3.connect(self.filename)\n",
    "        else:\n",
    "            self.conn = sqlite3.connect(self.filename)\n",
    "            self.conn.execute(\"\"\"create table dailystock\n",
    "                (symbol TEXT NOT NULL,\n",
    "                 date TIMESTAMP NOT NULL,\n",
    "                 price_open REAL,\n",
    "                 price_high REAL,\n",
    "                 price_low REAL,\n",
    "                 price_close REAL,\n",
    "                 volume INTEGER,\n",
    "                 PRIMARY KEY (symbol, date))\"\"\")\n",
    "            self.conn.commit()\n",
    "\n",
    "    def process_item(self, item, domain):\n",
    "        try:\n",
    "            self.conn.execute('insert into dailystock values(?,?,?,?,?,?,?)',\n",
    "                (item['symbol'], item['date'],\n",
    "                 item['price_open'], item['price_high'],\n",
    "                 item['price_low'], item['price_close'],\n",
    "                 item['volume']))\n",
    "            self.conn.commit()\n",
    "        except Exception, e:\n",
    "            print str(e)\n",
    "        return item\n",
    "\n",
    "class DailyStockSpider(scrapy.Spider):\n",
    "    name = \"dailystock\"\n",
    "    start_urls = [\"https://www.google.com/finance/historical?q=KRX%3AKOSPI200\"]\n",
    "    #SPIDER_MODULES = ['tutorial.spiders']\n",
    "    #NEWSPIDER_MODULE = 'tutorial.spiders'\n",
    "    #ITEM_PIPELINES = { 'tutorial.pipelines.DailyStockPipeline': 300, }\n",
    "    custom_settings = {\n",
    "        'BOT_NAME' : 'tutorial',\n",
    "        #'ITEM_PIPELINES' : { 'mykospi200.DailyStockPipeline': 300, },\n",
    "        'ITEM_PIPELINES' : { 'ds_web_data_kospi200.DailyStockPipeline': 300, },\n",
    "        'DOWNLOAD_HANDLERS' : { 's3': None, }\n",
    "    }\n",
    "    def parse(self, response):\n",
    "        dates = [dateutil.parser.parse(x.extract().strip()) for x in response.xpath('//td[@class=\"lm\"]/text()')]\n",
    "        volumes = np.array([int(x.extract().strip().replace(',','')) for x in response.xpath('//td[@class=\"rgt rm\"]/text()')])\n",
    "        prices = np.reshape([float(x.extract().strip()) for x in response.xpath('//td[@class=\"rgt\"]/text()')], (-1, 4))\n",
    "        for d, v, p in zip(dates, volumes, prices):\n",
    "          symbol = \"KOSPI\"\n",
    "          date = d\n",
    "          price_open = p[0]\n",
    "          price_high = p[1]\n",
    "          price_low = p[2]\n",
    "          price_close = p[3]\n",
    "          volume = v\n",
    "          yield {\"symbol\": symbol, \"date\": date, \n",
    "                 \"price_open\": price_open, \"price_high\": price_high, \n",
    "                 \"price_low\": price_low, \"price_close\": price_close, \n",
    "                 \"volume\": volume}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* Scrapy 실행\n",
    "    * json output 없어도 ok?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n",
      "UNIQUE constraint failed: dailystock.symbol, dailystock.date\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider ds_web_data_kospi200.py --output='ds_web_data_kospi200.json' -t json --logfile='ds_web_data_kospi200.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* sqlite 데이터베이스 dailystok 테이블 조회 (위 프로그램 결과)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ds_web_data_kospi200.sql\n"
     ]
    }
   ],
   "source": [
    "%%writefile ds_web_data_kospi200.sql\n",
    "select * from dailystock;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KOSPI|2016-09-13 00:00:00|253.39|253.55|251.66|251.77|74548000\r\n",
      "KOSPI|2016-09-12 00:00:00|252.37|253.43|250.53|250.53|66332000\r\n",
      "KOSPI|2016-09-09 00:00:00|258.68|258.94|256.21|257.31|70053000\r\n",
      "KOSPI|2016-09-08 00:00:00|260.72|261.48|259.41|260.86|79382000\r\n",
      "KOSPI|2016-09-07 00:00:00|261.15|262.1|260.31|260.31|68749000\r\n",
      "KOSPI|2016-09-06 00:00:00|259.39|261.16|259.26|260.93|49969000\r\n",
      "KOSPI|2016-09-05 00:00:00|257.85|259.74|257.63|259.64|54167000\r\n",
      "KOSPI|2016-09-02 00:00:00|256.16|256.73|255.57|256.5|49266000\r\n",
      "KOSPI|2016-09-01 00:00:00|254.95|256.13|253.86|256.03|56098000\r\n",
      "KOSPI|2016-08-31 00:00:00|257.21|257.49|255.9|256.87|66352000\r\n",
      "KOSPI|2016-08-30 00:00:00|257.27|258.93|257.0|257.49|56516000\r\n",
      "KOSPI|2016-08-29 00:00:00|254.93|256.66|254.45|256.49|62755000\r\n",
      "KOSPI|2016-08-26 00:00:00|256.17|256.76|255.02|256.23|68304000\r\n",
      "KOSPI|2016-08-25 00:00:00|256.78|257.76|256.0|257.26|58803000\r\n",
      "KOSPI|2016-08-24 00:00:00|258.4|258.65|256.54|257.3|54461000\r\n",
      "KOSPI|2016-08-23 00:00:00|257.57|258.63|257.09|258.42|65706000\r\n",
      "KOSPI|2016-08-22 00:00:00|258.54|258.63|256.92|257.27|62921000\r\n",
      "KOSPI|2016-08-19 00:00:00|258.42|258.84|257.54|258.69|72796000\r\n",
      "KOSPI|2016-08-18 00:00:00|256.62|258.23|255.8|258.11|64012000\r\n",
      "KOSPI|2016-08-17 00:00:00|255.82|256.32|254.6|256.04|63810000\r\n",
      "KOSPI|2016-08-16 00:00:00|256.95|258.13|256.12|256.15|68324000\r\n",
      "KOSPI|2016-08-12 00:00:00|256.86|257.88|255.7|256.17|74918000\r\n",
      "KOSPI|2016-08-11 00:00:00|255.14|256.18|254.24|256.18|93471000\r\n",
      "KOSPI|2016-08-10 00:00:00|255.68|256.31|255.06|255.43|66935000\r\n",
      "KOSPI|2016-08-09 00:00:00|254.38|256.24|254.19|255.85|60354000\r\n",
      "KOSPI|2016-08-08 00:00:00|253.34|254.18|252.51|254.11|64329000\r\n",
      "KOSPI|2016-08-05 00:00:00|250.21|252.4|249.96|252.36|62003000\r\n",
      "KOSPI|2016-08-04 00:00:00|250.22|250.34|248.65|249.24|54041000\r\n",
      "KOSPI|2016-08-03 00:00:00|250.56|250.56|248.6|248.65|74698000\r\n",
      "KOSPI|2016-08-02 00:00:00|252.8|253.14|251.76|251.93|61749000\r\n"
     ]
    }
   ],
   "source": [
    "!sqlite3 ds_web_data_kospi200.sqlite < ds_web_data_kospi200.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* googlefinance 또는 yahoo_finance 라이브러리를 사용하여 주식데이터를 읽어올 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 연습 웹데이터-2: Scrapy를 사용하여 Tripadvisor 크롤링하기\n",
    "\n",
    "* 미완성 next_link\n",
    "    * onClick() javascript함수가 다음 페이지를 호출. 어떻게 url을 구해서 next_link에 넣어야 할 지 모르겠슴.\n",
    "    * 다음 페이지 링크는 url을 or10, or20...으로 호출한다. urljoin 이렇게 해결??\n",
    "```\n",
    "https://www.tripadvisor.co.kr/Hotel_Review-g294197-d301253-Reviews-The_Shilla_Seoul-Seoul.html\n",
    "https://www.tripadvisor.co.kr/Hotel_Review-g294197-d301253-Reviews-or10-The_Shilla_Seoul-Seoul.html\n",
    "```\n",
    "    \n",
    "    * 참조 https://blog.monkeylearn.com/creating-sentiment-analysis-model-with-scrapy-and-monkeylearn/\n",
    "* css selector\n",
    "    * 공백이 있는 클래스는 실제로는 2개의 클래스, 예를 들어, class=\"foo bar\"는:\n",
    "```\n",
    ".foo\n",
    ".bar\n",
    ".foo.bar\n",
    "```\n",
    "        \n",
    "    * 태그의 text를 출력할 경우, '::text' pseudo-element를 사용한다. 아래 css, xpath는 같은 결과를 얻는다.\n",
    "```\n",
    "css('::text').extract() \n",
    "xpath('.//text()').extract()\n",
    "```\n",
    "        \n",
    "    * unicode - Scrapy는 unicode로 처리함 (ascii가 아니라)\n",
    "        * 문제에 대해서 the best thing to do is to write :\n",
    "```\n",
    "unicode(response.body.decode(response.encoding)).encode('utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* scrapy shell로 확인하기\n",
    "```\n",
    "jsl@jsl-smu:~$ scrapy shell https://www.tripadvisor.co.kr/Hotels-g255060-Sydney_New_South_Wales-Hotels.html\n",
    "In [1]: res=response.css('div.listing_title [dir=\"ltr\"]')\n",
    "In [2]: for each in res:                                 \n",
    "   ...:     print each.extract()\n",
    "...\n",
    "\n",
    "In [21]: response.css('#REVIEWS > .reviewSelector .member_info .username > span::text').extract()\n",
    "Out[21]: \n",
    "[u'semil2015',\n",
    " u'Hyungju K',\n",
    " u'modudada',\n",
    " u'\\uacf5\\uacf5\\ud65c\\ub3d9',\n",
    " u'Gina S',\n",
    " u'Youngkyung L',\n",
    " u'\\ub545\\ucfe0',\n",
    " u'ceytree',\n",
    " u'JWK17',\n",
    " u'\\uc131\\ubc94 \\uae40']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/ds_web_data_tripadvisor.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/ds_web_data_tripadvisor.py\n",
    "import scrapy\n",
    "\n",
    "class HotelSentimentItem(scrapy.item.Item):\n",
    "    user = scrapy.item.Field()\n",
    "    title = scrapy.item.Field()\n",
    "    review = scrapy.item.Field()\n",
    "\n",
    "class TripadvisorSpider(scrapy.spiders.CrawlSpider):\n",
    "    name='tripadvisor'\n",
    "    #allowed_domains=['reddit.com']\n",
    "    #start_urls=['https://www.tripadvisor.com/Hotels-g60763-New_York_City_New_York-Hotels.html']\n",
    "    start_urls=['https://www.tripadvisor.co.kr/Hotel_Review-g294197-d301253-Reviews-The_Shilla_Seoul-Seoul.html']\n",
    "    #custom_settings = {\n",
    "    #    'BOT_NAME': 'reddit-scraper',\n",
    "    #    'DEPTH_LIMIT': 3,\n",
    "    #    'DOWNLOAD_DELAY': 3\n",
    "    #}\n",
    "    def parse(self, response):\n",
    "        #s = scrapy.selector.Selector(response)\n",
    "        https://www.tripadvisor.co.kr/Hotel_Review-g294197-d301253-Reviews-The_Shilla_Seoul-Seoul.html\n",
    "        https://www.tripadvisor.co.kr/Hotel_Review-g294197-d301253-Reviews-or10-The_Shilla_Seoul-Seoul.html#REVIEWS\n",
    "        #next_link = s.css('#REVIEWS .pagination .next::attr(href)').extract()\n",
    "        ##print \"--> visiting \",next_link        \n",
    "        #if len(next_link):\n",
    "            #print \"--> visiting \",next_link[0]\n",
    "            ##yield self.make_requests_from_url(next_link)\n",
    "            ##next_link is a list, so indexing with 0\n",
    "            #yield scrapy.Request(next_link[0], self.parse)\n",
    "        posts = scrapy.selector.Selector(response).css('#REVIEWS > .reviewSelector')\n",
    "        for post in posts:\n",
    "            i = HotelSentimentItem()\n",
    "            i['user'] = post.css('.member_info .username > span::text').extract()\n",
    "            i['title'] = post.css('.quote .noQuotes::text').extract()\n",
    "            i['review'] = post.css('.entry .partial_entry::text').extract()\n",
    "            print 'crawling...',i['user'],i['title'],unicode(i['review'])\n",
    "            yield i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "추\n"
     ]
    }
   ],
   "source": [
    "print u'\\ucd94'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawling... [u'semil2015'] [u'\\ucd94\\ucc9c\\ud638\\ud154'] [u'\\n\\uc11c\\uc6b8\\uc5d0\\uc11c\\ub294 \\ucd5c\\uace0\\uc758 \\ud638\\ud154\\uc778 \\uac83 \\uac19\\uc2b5\\ub2c8\\ub2e4. \\ud30c\\ud06c\\ubdf0 \\ubdd4\\ud398\\ub3c4 \\ucd5c\\uace0\\uace0 \\uc11c\\ube44\\uc2a4\\ub3c4 \\ucd5c\\uace0\\ub77c\\uace0 \\uc0dd\\uac01\\ud569\\ub2c8\\ub2e4. \\ubdf0\\ub3c4 \\ub118\\ub098 \\uc88b\\uc544\\uc694...\\n', u'\\n']\r\n",
      "crawling... [u'Hyungju K'] [u'\\ud55c\\uad6d\\uc5d0\\uc11c \\uac00\\uc7a5 \\ucd94\\ucc9c \\ud558\\ub294 \\ud638\\ud154'] [u'\\n\\uac00\\uaca9\\ub3c4 \\uac00\\uaca9\\uc774\\ub2c8\\ub9cc\\ud07c \\ucd94\\ucc9c\\ud558\\ub294 \\ud638\\ud154 \\uc911\\uc5d0 \\ud558\\ub098. \\ub0b4\\uac00 \\ub300\\uc811\\ubc1b\\uace0 \\uc788\\ub2e4\\ub294\\uac78 \\ub290\\ub07c\\uac8c \\ud574\\uc8fc\\ub294 \\ud638\\ud154\\ub85c \\uc218\\uc601\\uc7a5\\ub3c4 \\uc88b\\uace0 \\ubdf0...\\n', u'\\n']\r\n",
      "crawling... [u'modudada'] [u'\\uc704\\uce58\\ub9cc\\ube7c\\uba74 \\ub2e4\\uc88b\\uc740\\uacf3'] [u'\\n\\ub354\\ud30c\\ud06c\\ubdf0\\uc5d0\\uc120 \\ud06c\\ub77c\\uc544\\uc0c1 \\ubabb\\ubd23\\ub294\\ub370\\n\\ud22c\\uc219\\ud558\\uba74 \\ub77c\\uc6b4\\uc9c0\\uc5d0 \\ud06c\\ub77c\\uc544\\uc0c1\\uc774 \\uc788\\uc5b4\\uc694\\n\\uc9c4\\uc9dc \\uc6b0\\ub9ac\\ub098\\ub77c\\uc5d0\\uc11c \\uba39\\uc740 \\ud06c\\ub77c\\uc544\\uc0c1\\uc911\\uc5d0 \\uc81c...\\n', u'\\n']\r\n",
      "crawling... [u'\\uacf5\\uacf5\\ud65c\\ub3d9'] [u'\\uc2e0\\ub77c \\uad1c\\uce04 \\ud569\\ub2c8\\ub2e4~~~\\uc0ac\\ub78c \\ub9ce\\uc544\\uc694!!!'] [u'\\n\\uc0ac\\ub78c\\ub4e4\\uc774 \\uc5ed\\uc2dc \\ub9ce\\uc2b5\\ub2c8\\ub2e4! \\uc5ec\\uc720\\ub86d\\uae30 \\ubcf4\\ub2e4\\ub294 \\ub108\\ubb34 \\ub85c\\ube44\\ub294 \\uc2dc\\ub04c\\ub7ec\\uc6cc\\uc694~\\ub8f8 \\uc740 \\ud3b8\\uc548\\ud55c \\uce68\\uad6c\\ub958\\uc5d0 \\uc88b\\uc2b5\\ub2c8\\ub2e4! \\uc870\\uc2dd\\ub3c4 \\ubb34...\\n', u'\\n']\r\n",
      "crawling... [u'Gina S'] [u'\\uc774\\uadf8\\uc81c\\ud050\\ud2f0\\ube0c \\ub514\\ub7ed\\uc2a4\\ub8f8 + \\uc5b4\\ubc18\\uc544\\uc77c\\ub79c\\ub4dc \\ud328\\ud0a4\\uc9c0 \\ud6c4\\uae30'] [u'\\n\\uc5ec\\ub984\\ud734\\uac00\\ub85c \\uc368\\uba38\\uc5d0\\ud53c\\uc18c\\ub4dc \\ud328\\ud0a4\\uc9c0\\ub97c \\uc608\\uc57d\\ud558\\uace0 \\ub2e4\\ub140\\uc654\\uc2b5\\ub2c8\\ub2e4.\\n\\ub8f8\\ucee8\\ub514\\uc158\\uc740 \\uace0\\uae09\\ud638\\ud154\\ub2f5\\uac8c \\uc815\\uac08\\ud558\\uace0 \\ubdf0\\ub3c4 \\uad1c\\ucc2e\\uc558\\uc5b4\\uc694. ...\\n', u'\\n']\r\n",
      "crawling... [u'Youngkyung L'] [u'\\uc815\\ub9d0 \\ud3b8\\uc548\\ud558\\uace0 \\uace0\\uae09\\uc2a4\\ub7f0 \\ud638\\ud154'] [u'\\n\\uc9c1\\uc6d0\\ub4e4\\ud0dc\\ub3c4 \\uc2dc\\uc124 \\ub2e4 \\uc88b\\uc558\\uace0 \\ubc29\\uc548\\uc5d0\\uc11c\\ub3c4 \\ub2f5\\ub2f5\\ud568\\uc744 \\ubabb\\ub290\\uaf08\\ub358\\uac70\\uac19\\uc544\\uc694. \\ud2b9\\ud788 \\uc57c\\uc678\\uc790\\ucfe0\\uc9c0\\uac00 \\ub108\\ubb34 \\uc88b\\uc558\\uace0\\uc694. \\uc2e4\\ub0b4\\uc218\\uc601\\uc7a5...\\n', u'\\n']\r\n",
      "crawling... [u'\\ub545\\ucfe0'] [u'\\uce74\\ub4dc\\ud589\\uc0ac\\ub85c \\ub2e4\\ub140\\uc654\\uc2b5\\ub2c8\\ub2e4.'] [u'\\n\\uce74\\ub4dc \\ubc14\\uc6b0\\ucc98\\ub85c \\ub2e4\\ub140\\uc654\\uc2b5\\ub2c8\\ub2e4. \\uc6b0\\uc120 \\ub8f8\\uc740 \\uad49\\uc7a5\\ud788 \\uc88b\\uc558\\uc2b5\\ub2c8\\ub2e4. \\uc5c5\\uadf8\\ub808\\uc774\\ub4dc\\ub3c4 \\ud574\\uc8fc\\uc5c8\\uad6c\\uc694. \\uc5b4\\ubc18 \\uc544\\uc77c\\ub79c\\ub4dc \\uc0ac\\uc6a9\\uae08\\uc561\\uc774...\\n', u'\\n']\r\n",
      "crawling... [u'ceytree'] [u'\\ub098\\ubb34\\ub784 \\ub370 \\uc5c6\\ub294 \\ud638\\ud154'] [u'\\n\\uae00\\ub85c\\ubc8c\\uccb4\\uc778\\uc774 \\uc544\\ub2c8\\uc9c0\\ub9cc \\uba85\\uc2e4\\uc0c1\\ubd80 \\ud55c\\uad6d \\ucd5c\\uace0\\uc758 \\ud638\\ud154\\uc785\\ub2c8\\ub2e4 \\ub0a8\\uc0b0 \\ubdf0\\uc758 \\uae54\\ub054\\ud558\\uace0 \\uae30\\ub2a5\\uc801\\uc778 \\uac1d\\uc2e4\\uacfc \\ud6cc\\ub96d\\ud55c \\ud30c\\ud06c\\ubdf0 \\uc870\\uc2dd...\\n', u'\\n']\r\n",
      "crawling... [u'JWK17'] [u'\\uc11c\\uc6b8\\uc5d0\\uc11c \\uc544\\uae30\\uc640 \\ub180\\uae30\\uc88b\\uc740\\uacf3'] [u'\\n\\uc11c\\uc6b8\\uc5d0\\uc11c \\uc544\\uae30\\uc640 \\uc218\\uc601\\ud558\\uba70 \\ud790\\ub9c1\\ud558\\uae30\\uc5d0 \\ub108\\ubb34 \\uc88b\\uc740\\uacf3\\uc785\\ub2c8\\ub2e4. \\uc774\\ub807\\uac8c \\uce5c\\uc808\\ud558\\uace0 \\uae68\\ub057\\ud558\\uace0 \\ud3b8\\ud55c \\ud638\\ud154\\uc740 \\ucc3e\\uae30 \\ud798\\ub4e0\\uac83\\uac19\\uc544...\\n', u'\\n']\r\n",
      "crawling... [u'\\uc131\\ubc94 \\uae40'] [u'\\ube44\\uc2f8\\ub2e4??? \\uac00\\uc131\\ube44\\uac00 \\uc88b\\ub2e4!!!'] [u'\\n\\ub9dd\\uace0\\ube59\\uc218 4\\ub9cc2\\ucc9c\\uc6d0 \\ube44\\uc2fc\\uac00\\uaca9\\uc774\\uc9c0\\ub9cc \\uadf8 \\ub9db\\uc744 \\ubcf4\\uace0 \\uc2f6\\uc5b4\\uc11c \\uac14\\uc5b4\\uc694 \\uadfc\\ub370 \\ub9db\\uc740 \\uae30\\ub300\\uc774\\uc0c1\\uc740 \\uc544\\ub2c8\\uc9c0\\ub9cc(\\ubb3c\\ub860 \\ub9db\\uc788\\uc5b4\\uc694)...\\n', u'\\n']\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy runspider src/ds_web_data_tripadvisor.py --output='src/ds_web_data_tripadvisor.json' -t json --logfile='src/ds_web_data_tripadvisor.log'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 더 해보기\n",
    "\n",
    "* 폼Form에서 가져오기, login\n",
    "    * scrapy.http.FormRequest\n",
    "    * scrapy -> flume"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
