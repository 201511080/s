{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 회귀분석\n",
    "\n",
    "Last updated 20201106FRI1100 20191213FRI1110\n",
    "\n",
    "## S.1 목차\n",
    "\n",
    "* S.2 문제의 이해\n",
    "* S.3 회귀식\n",
    "* S.3.1 모델\n",
    "* S.3.2 변수\n",
    "* S.3.3 입력변수의 수에 따른 구분\n",
    "* S.3.4 표준화에 따른 구분\n",
    "* S.4 OLS\n",
    "* S.4.1 데이터\n",
    "* S.4.2 그래프\n",
    "* S.4.3 선형 회귀식 풀기\n",
    "* S.4.4 Python으로 미분해서 풀어보자\n",
    "* S.4.5 R2 (S.5.5 오류 계산하고 해서 모두 s.6 gradient 뒤로 이동)\n",
    "* S.5 matrix inverse method\n",
    "* S.5.1 풀기\n",
    "* S.5.2 데이터\n",
    "* S.5.3 identity matrix\n",
    "* S.5.4 $\\beta$\n",
    "* 문제: Matrix Inverse Method 회귀모델\n",
    "* S.5.5 오류 계산\n",
    "* S.5.6 sympy matrix\n",
    "* S.6 gradient for linear regression\n",
    "* 문제: 회귀식 그래프 함수\n",
    "* 문제: 키 몸무게를 matrix inverse method와 gradient descent로 풀기\n",
    "* S.7 정규화 회귀모형\n",
    "* 문제 UCI Abalone\n",
    "* S.8 Local Regression\n",
    "* S.9 Ridge\n",
    "* S.10 Lasso\n",
    "* S.11 forward stagewise\n",
    "* S.11 비선형 모델\n",
    "* 문제: iris데이터를 numpy, statsmodels, sklearn, Pyspark로 회귀분석\n",
    "* 문제: Pyspark로 매출 회귀분석 및 평가\n",
    "* 문제: 주택가격의 sklearn (Lasso, Ridge), Pyspark로 회귀분석\n",
    "* 문제: statsmodel, pyspark로 당뇨의 회귀분석 및 평가\n",
    "\n",
    "* 참조 ISLR 6장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.2 문제의 이해\n",
    "\n",
    "어떤 값을 예측한다고 하고, 그 값에 영향을 미치는 변수들이 있다고 하자.\n",
    "예측 값을 **종속변수 Dependent variables** 또는 **출력변수 Output variables**라고 하고,\n",
    "영향변수를 **독립변수 Input variables**라고 하거나 **입력변수 Input variables**라고 한다.\n",
    "예를 들어, 영화매출을 예측한다고 하자. 매출에 영향을 미칠 수 있는 변수로 평점, 광고비, 투자비, 개봉일 매출액, 조회수 등을 꼽을 수 있다.\n",
    "회귀분석은 입력변수로부터 목표 값을 예측을 하는 문제에 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.3 회귀식\n",
    "\n",
    "### S.3.1 모델\n",
    "\n",
    "$y = \\beta_0 + \\beta_1x_1 + \\ldots + \\beta_nx_n\n",
    " = \\beta_0 + \\sum_{j=1}^n \\beta_j x_j$\n",
    "\n",
    "* $x \\in \\mathbb{R}^{m}_{n}$는 입력변수이고 i개 레코드, j개 속성으로 이루어진 입력데이터\n",
    "* $y$ 는 목표변수로서 1개의 컬럼 값을 가진다.\n",
    "    $$\\mathbf{y} = \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}$$\n",
    "\n",
    "위 식은 $x_0=1$인 경우 $y = \\sum_{j=0}^n \\beta_j x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### 벡터 표기\n",
    "\n",
    "데이터 x는 벡터로 표현할 수 있으며, $x^{m}_{n}$는 m개의 데이터 갯수, n개의 속성으로 구성된 벡터이다.\n",
    "$x_{ij}$는 **i**번째 샘플(행)의 **j**번째 속성(열)이다.\n",
    "또는 위첨자 superscript, 아래첨자 subscript를 이용하여 표기하기도 한다. $x^{(j)}_i$ 영어로 설명하면 이해하기 쉽다. 즉 위첨자 **Up**per는 알파벳 **U**로 시작하니 위에서 아래로 (**up**에서 down), 반면에 아래첨자 **L**ower는 왼쪽에서 오른쪽으로 (**l**eft to right) 구성된다 (Einstein 표기법이라 한다).\n",
    "* 즉, 위첨자 superscript (j)는 j번째 **열**\n",
    "* 아래첨자 subscript (i)는 i번째 **행**을 의미한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### argmin\n",
    "\n",
    "$\\beta$를 w로 표기하기도 하는데, 그러면 회귀식은 y=wx으로 바꿔쓸 수 있다.\n",
    "회귀식은 아래에서 보듯이 $\\hat{w}$를 찾아야 풀리게 된다.\n",
    "$argmin_{w}$은 오류 $(y-wx)^2$를 최소화하는 w를 의미한다.\n",
    "\n",
    "$\\hat{w} = argmin_{w} (y-wx)^2$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.3.2 입력변수의 수에 따른 구분\n",
    "\n",
    "회귀식은 **입력변수에 대해 영향을 미치는 가중치를 주어 출력변수 값을 계산하는 식**이라고 할 수 있다.\n",
    "출력변수는 숫자이고 입력변수는 하나 이상이 될 수 있다.\n",
    "\n",
    "상관관계와 회귀분석은 여러 변수간의 관계를 분석하는 방법이란 공통점이 있다.\n",
    "반면에 **상관관계**는 변수 간 관계의 강도를 -1 ~ 1 값으로 측정하는 것이고, **회귀분석**에서는 종속변수와 독립변수를 구분하여 설명하거나, 예측하는데 쓰인다.\n",
    "\n",
    "입력변수에 따라 이진, 다중회귀분석으로 구분할 수 있다.\n",
    "**이진회귀식 binary regression**은 두 변인 간의 관계를 나타내는 경우에 쓰인다.\n",
    "**다중회귀식 multiple regression**은 변인이 복수인 경우를 말한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### bias와 variance\n",
    "\n",
    "**variance**는 회귀식을 기준으로 퍼져 있는 정도를 의미한다. 통계에서 말하는 분산으로, 오류가 넓게 퍼져 분포될 수록 variance는 크다.\n",
    "반면에 **bias**는 회귀식이 평균적으로 벗어난 정도를 의미한다. 화살이 과녁에 도착하였지만, 여기 저기 퍼져 있는 것이 아니라 잘 모아져 있지만 실제 값에서 벗어난 정도를 말한다. bias가 크고 variance가 적으면 평균만 이동해주면 잘 맞게 된다.\n",
    "\n",
    "변수가 증가할수록 **overfitting**의 위험이 발생할 수 있다. 즉 bias는 감소하지만, variance는 증가하게 된다.\n",
    "예를 들어 변수의 갯수(p)가 데이터 수(n) 보다 많아지게 되면, variance가 무한대로 증가한다.\n",
    "이럴 경우, **shrinkage** 방식으로 모델의 variance를 줄이고, 정확성을 높일 수 있다.\n",
    "\n",
    "모든 변수를 넣어서 모델을 만들 수 있지만, 그 보다는 **변수를 추가하면서 그 조합에서 RSS, Residual Sum of Squares가 가장 적은 모델을 선택**하게 된다.\n",
    "속성이 10개라면 1024개 모델, $2^{10}$의 조합이 가능하다. 속성이 40개 이상이면 $2^{40}$, 계산이 거의 불가능하다.\n",
    "\n",
    "또는 변수를 하나씩 추가하면서 **stepwise**, 또는 순서를 정하지만, 먼저 넣어야할 변수와 아닌 경우로 계층화해서 (고기류는 먼저 넣고, 야채는 나중에) **hierachical** 방식으로 할 수 있다.\n",
    "* forward selection: 후보변수 없이 null model에서 시작하여, 하나씩 후보변수를 추가 \n",
    "* backward selection: 모든 후보변수를 넣고 full model에서 시작하고, 하나씩 제거\n",
    "\n",
    "변수를 선택하는 기법으로 PCA (Principle Component Analysis)를 사용할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.3.4 표준화에 따른 구분\n",
    "\n",
    "비표준화된 회귀식 unstandardized regression equation은 원래 데이터를 그대로 사용하기 때문에 직관적으로 이해할 수 있다.\n",
    "\n",
    "$$ Y = a + b \\times X $$\n",
    "\n",
    "그러나 표준화된 standardized regression equation: 원시값(raw data)를 사용하는 것이 아니라 z값을 사용할 수 있다.\n",
    "\n",
    "$$ z_y = a + b \\times z_x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.3.5 general linear model\n",
    "\n",
    "여러 변수로 구성된 선형적 관계의 모델을 말하는데, 이 때 오류 residuals가 **정규분포**를 따르는 경우를 말한다.\n",
    "즉 general linear model은 결과 값이 정규분포일 경우, ANOVA, MANOVA, t-test, 선형회귀분석 등이 해당된다.\n",
    "에러 $\\epsilon_i \\sim N(0,\\sigma^2)$\n",
    "\n",
    "$$ y_i = \\beta_0 + \\beta_1 x_{1i} + ... + \\beta_p x_{pi} + \\epsilon_i $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> GLM \n",
    "\n",
    "> 일반화선형모델 (GLM: generalized linear model)은 입력변수와 예측변수를 선형관계로 모형화한다.\n",
    "그러나 **오류 Residuals가 정규분포를 따르지 않는 경우**, 연결함수 link function을 통해 비정규화를 줄이려고 한다.\n",
    "정규화가 아닌 분포는 여러 형태가 있을 수 있다. 이항 binomial, 포아손 Poisson, 베타 beta, 감마 gamma 등을 예를 들 수 있다.\n",
    "종속변수가 이항분포인 경우 로지스틱을, 종속변수가 갯수인 경우 poisson으로 모델링한다.\n",
    "generalized라는 단어는 선형, 로지스틱, 포아손을 **일반화 generalized**한 모델이라 그렇게 명명되었다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위키피디아 예제: http://en.wikipedia.org/wiki/Simple_linear_regression\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\hat{a} &= \\frac{nS_{xy}-S_xS_y}{nS_{xx}-S_x^2} = 61.272 \\\\\n",
    "\\hat{b} &= \\frac{1}{n}S_y - \\hat{a} \\frac{1}{n}S_x = -39.062\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.4 OLS 계산\n",
    "\n",
    "회귀식은 입력데이터 간의 관계를 나타내고, 그 관계로 인해 발생하는 오류를 최소화하도록 만들어진다.\n",
    "오류는 **y실제 - $\\hat{y}$예측 차이의 제곱**을 합계낸 값으로, 이를 **최소화**하는 지수를 구하면 최적식을 구할 수 있다.\n",
    "$y-\\hat{y}$은 음수가 나올 수도 있으므로 오류의 합계를 구하려면 제곱을 한다.\n",
    "즉 OLS (Ordinary Least Square)는 오류를 최소화하여, 회귀식을 찾는 방법이다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.1 데이터\n",
    "\n",
    "x,y 데이터를 생성해보자.\n",
    "\n",
    "x | y\n",
    "-----|-----\n",
    "1 | 6\n",
    "2 | 5\n",
    "3 | 7\n",
    "4 | 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.2 그래프\n",
    "\n",
    "```scatter(x,y)```로 각 좌표(x,y)를 나타내보자.\n",
    "```subplot()```에는 3개의 인자가 사용된다 - 행, 열, 순서 (왼쪽 상단 1부터 시작해서 오른쪽으로 이동). 즉 **211은 2행 1열의 첫째**, **212는 2행 1열의 둘째**를 의미한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f441b81beb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "ax.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.4.3 선형 회귀식 풀기\n",
    "\n",
    "x, y 값을 하나씩 입력하면, 아래 식이 성립한다.\n",
    "\n",
    "$6 = \\beta_0 + \\beta_1 \\times 1$\n",
    "\n",
    "$5 = \\beta_0 + \\beta_1 \\times 2$\n",
    "\n",
    "$7 = \\beta_0 + \\beta_1 \\times 3$\n",
    "\n",
    "$10 = \\beta_0 + \\beta_1 \\times 4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "실제와 예측의 차이인 **오류 $(y-yhat)^2$를 최소화**하는 베타 값을 구해보자.\n",
    "\n",
    "$(6 - (\\beta_0 + \\beta_1 \\times 1))^2$\n",
    "\n",
    "$(5 - (\\beta_0 + \\beta_1 \\times 2))^2$\n",
    "\n",
    "$(7 - (\\beta_0 + \\beta_1  \\times 3))^2$\n",
    "\n",
    "$(10 - (\\beta_0 + \\beta_1  \\times 4))^2$\n",
    "\n",
    "위 식을 모두 더해서 총오류를 구하면:\n",
    "\n",
    "$f=210 + 4\\beta_0^2 + 30\\beta_1^2 + 20\\beta_0\\beta_1 - 56\\beta_0 - 154\\beta_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위를 편미분하면:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_0} = 8\\beta_0 + 20\\beta_1 - 56$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_1} = 20\\beta_0 + 60\\beta_1 - 154$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "이를 풀면:\n",
    "\n",
    "$\\beta_0 = 3.5$\n",
    "\n",
    "$\\beta_1 = 1.4$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "예측 값 $\\hat{y}$를 계산해보자.\n",
    "\n",
    "각 x 값에 기울기 1.4를 곱하고, y절편 3.5를 더해서 구할 수 있다.\n",
    "\n",
    "$yhat=3.5 + 1.4 x$\n",
    "\n",
    "x는 벡터 값으로 요소별 '$x \\times 1.4 + 3.5$'와 같이 덧셈, 곱셈이 가능하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat=x*1.4+3.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python3의 출력기능이 있는 f를 사용하여 출력해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predicted: [4.9 6.3 7.7 9.1]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"predicted: {yhat}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "실제값과 예측값의 오류를 구해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'predicted - actual: [-1.1  1.3  0.7 -0.9]'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"predicted - actual: {yhat-y}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오류는 플러스, 마이너스 서로 상쇄될 수 있다. 오류의 제곱, 즉 RSS를 구하면 크기를 올바르게 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'error: 4.199999999999998'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"error: {np.sum(np.power(yhat-y,2))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.4 Python으로 미분해서 풀어보자\n",
    "\n",
    "편미분 partial derivatives은 변수 가운데 하나를 선정하고 나머지는 상수로 보고 미분하는 것이다.\n",
    "예를 들어, \n",
    "\n",
    "$\n",
    "f(x,y) = x^2 + xy + y^2\\\\\n",
    "\\frac{\\partial{f}}{\\partial{x}} = 2x + y\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SymPy는 수식이나 논리명제를 풀기위해 만들어진 Python 라이브러리이다.\n",
    "Anaconda에는 이미 설치가 되어 있다. 설치가 필요하면 ```pip3 install sympy```라고 하면 된다.\n",
    "sympy를 import 해서, sp로 사용해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sympy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```symbols('b0,b1')```는 따옴표 안의 b0,b1을 변수로 선언한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b0,b1=sp.symbols('b0 b1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 구한 총오류 f함수를 b0, b1에 대해 미분을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=210 + 4*b0**2 + 30*b1**2 + 20*b0*b1 - 56*b0 - 154*b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "diff(f,b0)는 f함수를 b0에 대해서 미분하는 명령어이다.\n",
    "diff(f,b1)는 f함수를 b1에 대해서 미분하는 명령어이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fdiff_b0=sp.diff(f,b0)\n",
    "fdiff_b1=sp.diff(f,b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "줄바꿈 ```\\n```을 넣어서 출력하려면 f출력은 조금 복잡해진다.\n",
    "\"\".format()으로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fdiff with b0: 8*b0 + 20*b1 - 56\n",
      "fdiff with b1: 20*b0 + 60*b1 - 154\n"
     ]
    }
   ],
   "source": [
    "print (\"fdiff with b0: {}\\nfdiff with b1: {}\".format(fdiff_b0, fdiff_b1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 식을 Sympy ```solve()```로 풀면 해를 구할 수 있다.\n",
    "이 함수의:\n",
    "* 첫째 인자는 수식이고, 0으로 놓고 풀게 된다.\n",
    "즉 ```fdiff_b0 = 0, fdiff_b1 = 0```으로 하는 것과 동일하다.\n",
    "* 그리고 두번째 인자로 넣은 b0, b1을 구하면 위에서 계산한 값과 동일한 결과가 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp=sp.solve([fdiff_b0, fdiff_b1],[b0,b1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exp={b0: 7/2, b1: 7/5}\n"
     ]
    }
   ],
   "source": [
    "print(\"exp={0}\".format(exp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### S.4.5 $R^2$\n",
    "\n",
    "$R^2$는 **종속변수 dependent variable의 분산을 독립변수 independent variable(s)로 얼마나 예측**할 수 있는지를 나타낸다. 즉, 계산식 **총 오류TSS = 오류SSE +  잔여오류RSS**에서 **잔여오류RSS를 TSS총오류로 나누어** 계산한다.\n",
    "\n",
    "$r^2=\\frac{RSS}{TSS}$\n",
    "\n",
    "* 총오류 TSS (Total Sum of Squared Errors)\n",
    "* 오류의 합계 SSE (Sum of Squared Errors)\n",
    "* 잔여 오류 RSS (Residual Sum of Squared Errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터\n",
    "\n",
    "x는 0~8의 정수로, y는 임의로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.arange(0,9)\n",
    "y=np.array([19, 20, 20.5, 21.5, 22, 23, 23, 25.5, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy에서 해주었던 것과 같이 ```np.ones(9)``` 명령어로 1을 넣어서 절편을 구해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=np.array([x, np.ones(9)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 2. 3. 4. 5. 6. 7. 8.]\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print (A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy에서 행과 열을 읽을 경우 인덱스로, 모든 행 (열)은 ```:```로 표기한다.\n",
    "\n",
    "**행**을 읽을 경우, 즉 ```i```행의 모든 열은 ```A[i,:]``` 또는 ```:```는 생략해서 ```A[i]```로 읽고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5., 6., 7., 8.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**열**을 읽을 경우, 즉 모든 행 ```:```의 ```j```열은  A[:,j]로 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$x_{02}$ 또는 $x_{0}^{2}$**는 1번째 행(레코드), 3번째 컬럼(속성) A[0,2]로 읽는다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numpy listsq\n",
    "\n",
    "numpy의 ```listsq()```를 사용하여, 데이터의 회귀식 계수 w0, w1을 계산해보자.\n",
    "Python2에서는 ```np.linalg.lstsq(A.T, y)```, 그러나 ```np.linalg.lstsq(A.T, y, rcond=None)```라고 해준다.\n",
    "```rcond```는 행렬 A의 특잇값 singular value의 cut-off 비율인데, 기본값을 적용하지 않겠다는 의미이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w0, w1=np.linalg.lstsq(A.T, y, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w0: 0.7166666666666672\n",
      "w1: 19.188888888888897\n"
     ]
    }
   ],
   "source": [
    "print (\"w0: {}\\nw1: {}\".format(w0, w1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측 값을 구해보자. 실제 값과 얼마나 차이가 나는지 비교해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yhat=w0*x+w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측: [19.18888889 19.90555556 20.62222222 21.33888889 22.05555556 22.77222222\n",
      " 23.48888889 24.20555556 24.92222222]\n"
     ]
    }
   ],
   "source": [
    "print (\"예측: {}\".format(yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 그래프\n",
    "\n",
    "예측 yhat과 실제 y의 그래프를 그려보자.\n",
    "예측은 빨간 실선 (r-), 파란 점 (bo, 'b'는 blue, 'o'는 동그라미 점)으로 그린다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x,yhat,'r-',x,y,'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R squared 계산\n",
    "\n",
    "총오류와 잔여오류를 계산해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "RSS = sum((y-yhat)**2)\n",
    "TSS = sum((y-np.mean(y))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r_squared = 1 - (float(RSS))/TSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r_squared: 0.9138385502471171'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"r_squared: {r_squared}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "adjusted R-squared는 독립변수의 개수에 따라 과도한 $R^2$를 수정한 값이다.\n",
    "\n",
    "$R^2\\space adjusted=1 - \\frac{(1-R^2)(n-1)}{n-k-1}$\n",
    "\n",
    "* n은 데이터 개수\n",
    "* k는 독립변수 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-A.shape[0]-1)   # 0.9015 without -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adjusted_r_squared: 0.8851180669961561'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"adjusted_r_squared: {adjusted_r_squared}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### statsmodels R squared\n",
    "statsmodels을 사용해서 회귀분석과 $R^2$를 알아보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = sm.OLS(y, A.T).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "측정 값이 불과 9로 적어서 UserWarning이 뜬다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsl/.local/lib/python3.6/site-packages/scipy/stats/stats.py:1604: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=9\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>            <td>y</td>        <th>  R-squared:         </th> <td>   0.914</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.902</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   74.24</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sat, 07 Nov 2020</td> <th>  Prob (F-statistic):</th> <td>5.66e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>08:19:06</td>     <th>  Log-Likelihood:    </th> <td> -7.6827</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>     9</td>      <th>  AIC:               </th> <td>   19.37</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>     7</td>      <th>  BIC:               </th> <td>   19.76</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>    <td>    0.7167</td> <td>    0.083</td> <td>    8.616</td> <td> 0.000</td> <td>    0.520</td> <td>    0.913</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>   19.1889</td> <td>    0.396</td> <td>   48.458</td> <td> 0.000</td> <td>   18.253</td> <td>   20.125</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 4.624</td> <th>  Durbin-Watson:     </th> <td>   3.078</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.099</td> <th>  Jarque-Bera (JB):  </th> <td>   1.154</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.773</td> <th>  Prob(JB):          </th> <td>   0.561</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.830</td> <th>  Cond. No.          </th> <td>    9.06</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   R-squared:                       0.914\n",
       "Model:                            OLS   Adj. R-squared:                  0.902\n",
       "Method:                 Least Squares   F-statistic:                     74.24\n",
       "Date:                Sat, 07 Nov 2020   Prob (F-statistic):           5.66e-05\n",
       "Time:                        08:19:06   Log-Likelihood:                -7.6827\n",
       "No. Observations:                   9   AIC:                             19.37\n",
       "Df Residuals:                       7   BIC:                             19.76\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.7167      0.083      8.616      0.000       0.520       0.913\n",
       "const         19.1889      0.396     48.458      0.000      18.253      20.125\n",
       "==============================================================================\n",
       "Omnibus:                        4.624   Durbin-Watson:                   3.078\n",
       "Prob(Omnibus):                  0.099   Jarque-Bera (JB):                1.154\n",
       "Skew:                           0.773   Prob(JB):                        0.561\n",
       "Kurtosis:                       3.830   Cond. No.                         9.06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9138385502471168, 0.9015297717109907)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.rsquared, result.rsquared_adj\n",
    "# 0.877643371323 0.863248473832"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.5 matrix inverse method\n",
    "\n",
    "### S.5.1 풀기\n",
    "\n",
    "$\\hat{w}$ 최소해를 구해보자.\n",
    "오류 e는 잔차를 제곱해서 더한 합계 'sum of squared residuals' 이다.\n",
    "따라서,\n",
    "$f = e.T \\times e = (y-wx)^T (y-wx) = y^Ty - 2w(x^Ty)^T+w^Tx^Twx$\n",
    "\n",
    "위를 미분하면 $\\frac{\\partial f}{\\partial w} =\n",
    "    0 -2(x^Ty)+2wx^Tx$\n",
    "\n",
    "위 미분 값을 0으로 놓고 풀면, $-2x^Ty + 2\\hat{w}x^Tx = 0$\n",
    "\n",
    "**위 식을 정리하면** $\\hat{w} = (X^T X) ^{-1} X^T y$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### S.5.2 데이터\n",
    "\n",
    "입력변수 x1, x2로부터 출력변수 y가 있다고 하자.\n",
    "\n",
    "$2x_1 - 4x_2 = 8$\n",
    "\n",
    "$3x_1 + 6x_2 = 9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "y=[8,9]\n",
    "x=[[2,-4],[3,6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x: [[2, -4], [3, 6]] y: [8, 9]'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"x: {x} y: {y}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```set_printoptions(precision=2, suppress=True)```는 출력 자릿수를 2자리로 제한할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x inv:\n",
      "[[ 0.25  0.17]\n",
      " [-0.12  0.08]]\n"
     ]
    }
   ],
   "source": [
    "xI = np.linalg.inv(x) \n",
    "print(\"x inv:\\n{}\".format(xI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.3 identity matrix\n",
    "\n",
    "행열에 역행렬을 서로 곱하면 1, 즉 단위행렬 identity matrix가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity matrix 'x * xI':\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print (\"identity matrix 'x * xI':\\n{}\".format(np.dot(x,xI)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "identity matrix는 ```np.eye()```로 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "행열 x에 역행렬 xI을 서로 곱해서 단위행렬 identity matrix와 비교하면 True가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True],\n",
       "       [ True,  True]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.eye(2) == np.dot(x,xI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### S.5.4 $\\beta$\n",
    "\n",
    "위 식을 행렬로 나타내면,\n",
    "$\\beta \\times x = y$\n",
    "\n",
    "양변에 $x^{-1}$를 곱하면 아래 식이 된다.\n",
    "\n",
    "$x^{-1}\\times x\\times \\beta=x^{-1}\\times y$\n",
    "\n",
    "inverse에 원래의 벡터를 곱하면 identity matrix가 된다.\n",
    "\n",
    "즉 $x^{-1}\\times x=1$이므로\n",
    "\n",
    "아래와 같이 $\\beta$를 구할 수 있다.\n",
    "\n",
    "$\\beta=x^{-1}\\times y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### inverse 계산\n",
    "\n",
    "**$xx^{-1}=I$**인 $x^{-1}$가 존재하면 x는 invertible이라고 한다.\n",
    "\n",
    "\n",
    "```np.array()```는 ```np.linalg.inv()```, ```np.mat()```는 ```x.I```로 inverse를 계산한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# matrix inverse\n",
    "v=np.array([[2,3],[4,5]])\n",
    "vI=np.linalg.inv(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v=\n",
      "[[2 3]\n",
      " [4 5]]\n",
      "v.I=\n",
      "[[-2.5  1.5]\n",
      " [ 2.  -1. ]]\n"
     ]
    }
   ],
   "source": [
    "print (\"v=\\n{0}\\nv.I=\\n{1}\".format(v,vI)) #([[-2.5, 1.5], [ 2. , -1. ]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inverse를 산식으로 계산하면 다음과 같다. 앞서 계산과 동일하다.\n",
    "$\\frac{1}{ad-bc} \\begin{pmatrix}d & -b\\\\-c & a\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[-2.5  1.5  2.  -1. ]'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "left=1./(2*5-3*4)\n",
    "right=np.array([5,-3,-4,2])\n",
    "f\"{left*right}\" #([-2.5,  1.5,  2. , -1. ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### inverse로 $\\beta$ 계산\n",
    "\n",
    "matrix inverse method를 풀어보자.\n",
    "numpy array로 행렬을 선언하고,\n",
    "다음 식을 ```np.dot()```, ```np.linalg.inv()``` 함수를 이용해서 풀면 x1, x2를 구할 수 있다.\n",
    "\n",
    "\n",
    "* $y = \\beta x$인 경우, 양변을 $x^{-1}$로 곱하면\n",
    "* $x^{-1}y = \\beta x^{-1}x$ 여기서 $xx^{-1}=1$이므로 \n",
    "* $\\beta=x^{-1}\\times y$\n",
    "\n",
    "$\n",
    "2 \\times w1 - 4 \\times w2 = 8\\\\\n",
    "3 \\times w2 + 6 \\times w2 = 9\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([[2,-4],[3,6]])\n",
    "y=np.array([8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xI=np.linalg.inv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5 -0.25\n"
     ]
    }
   ],
   "source": [
    "w1,w2=np.dot(xI,y)\n",
    "print(w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위에서 구한 값을 대입해서 식을 풀어보자.\n",
    "주어진 데이터로 풀어서 얻은 w1, w2를 넣어서 구한 값은 8, 9이다.\n",
    "오류가 전혀 없이 실제 값이 구해진다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'8.0'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{2*w1+(-4)*w2}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'9.0'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{3*w1+6*w2}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### numpy solve\n",
    "\n",
    "```numpy.linalg.solve()```는 선형 벡터 식을 풀 수 있다\n",
    "\n",
    "위의 x,y를 넣으면 값을 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.5 , -0.25])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.solve(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 문제: Matrix Inverse Method 회귀모델\n",
    "\n",
    "x: 1,2,3,4\n",
    "y: 6,5,7,10의 경우 회귀식을 도출해 보자.\n",
    "답은 y=3.0 + 1.7x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 데이터\n",
    "\n",
    "주어진 x, y에서 numpy array를 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "절편을 구하기 위해 '1'을 추가한다.\n",
    "즉 x0, x1으로 구성하고, 해당하는 w0, w1을 계산한다.\n",
    "\n",
    "식을 y = wx로 다시 쓸 수 있다.\n",
    "\n",
    "$6 = \\beta_0 + \\beta_1 \\times 1$\n",
    "\n",
    "$5 = \\beta_0 + \\beta_1 \\times 2$\n",
    "\n",
    "$7 = \\beta_0 + \\beta_1 \\times 3$\n",
    "\n",
    "$10 = \\beta_0 + \\beta_1 \\times 4$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "주의할 점은 x를 [x 1]로 변환한다는 점이다. 절편 값을 구하기 위해서 그렇게 해야 한다.\n",
    "\n",
    "아래 데이터는 w1, w0을 각 행으로 구성하고 있다.\n",
    "\n",
    "```python\n",
    "[ 1.  1.]\n",
    "[ 2.  1.]\n",
    "[ 3.  1.]\n",
    "[ 4.  1.]\n",
    "```\n",
    "\n",
    "$6 = \\beta_1 \\times 1 + \\beta_0 \\times 1$\n",
    "\n",
    "$5 = \\beta_1 \\times 2 + \\beta_0 \\times 1$\n",
    "\n",
    "$7 = \\beta_1 \\times 3 + \\beta_0 \\times 1$\n",
    "\n",
    "$10 = \\beta_1 \\times 4 + \\beta_0 \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "원래의 데이터가 x는 행 2, 열 4로 구성되어서, transpose하면 4,2로 변환된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "> 1차원 Transpose\n",
    "\n",
    "> 1차원의 ```x.T*x```를 구해보자.\n",
    "numpy array **1d**는 ```[...,...,...]``` 형식으로 구성된다.\n",
    "**1d는 transpose를 해도 1d로 변환이 되지 않는다**.\n",
    "이럴 경우 ```[[],[]]```로 만들어 trasnpose해야 한다.\n",
    "np.newaxis 명령어로 ```[]```를 추가해서 ```[[],[]]```으로 변환해준다.\n",
    "```python\n",
    "x=np.array([1,2,3,4])\n",
    "x[:, np.newaxis]\n",
    "```\n",
    "그 결과 [1,2,3,4]가 ([[1],[2],[3],[4]])로 변환이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=np.array([x, np.ones(len(x))])\n",
    "x=x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1.]\n",
      " [2. 1.]\n",
      " [3. 1.]\n",
      " [4. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 항목별 계산\n",
    "\n",
    "$\\hat{w} = (X^T X) ^{-1} X^T y$ 식의 우측을 한 항목씩 계산해 나가보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) $X^T X$ 계산\n",
    "\n",
    "이 항목을 계산할 경우, 행렬의 shape에 주의해야 한다.\n",
    "현재 x는 (4,2) x.T는 (2,4)이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'x shape: (4, 2) xT shape:(2, 4)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"x shape: {x.shape} xT shape:{x.T.shape}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  4.,  5.],\n",
       "       [ 3.,  5.,  7.,  9.],\n",
       "       [ 4.,  7., 10., 13.],\n",
       "       [ 5.,  9., 13., 17.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x,x.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위는 대칭선을 기준으로 동일하다. ```np.linalg.inv(singular maxtrix)```는 오류가 난다.\n",
    "자신에게 자신의 Transpose를 곱한 결과는 singular matrx이다.\n",
    "이 의미는 가역행렬이 아니라는 의미이다. 즉 inverse를 할 수 없다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-11876ea0240e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36minv\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 546\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    547\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36m_raise_linalgerror_singular\u001b[0;34m(err, flag)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLinAlgError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Singular matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_raise_linalgerror_nonposdef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "np.linalg.inv(np.dot(x,x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "> **singular matrix**\n",
    "\n",
    "> 위 결과는 singular matrix이다.\n",
    "singlular matrix의 inverse()는 할 수 없다.\n",
    "다음과 같은 singular matrices는 inv() 연산을 할 수 없다.\n",
    "```python\n",
    "np.linalg.inv(np.array([[3,5],[6,10]]))\n",
    "```\n",
    "\n",
    "> 앞서 역행렬은 $\\frac{1}{ad-bc} \\begin{pmatrix}d & -b\\\\-c & a\\end{pmatrix}$ 이렇게 계산했는데, 좌변이 0이면 계산이 가능하지 못하다. 즉 역행렬을 구할 수 없다. 위에서 3 x 10 - 5 x 6 = 0이다. 따라서 위 행렬은 역행렬이다.\n",
    "또한 1d matrix를 자신의 transpose로 곱하면 singular matrix가 된다.\n",
    "**첫째 행의 2배, 3배, 4배로 행이 구성되어 실제 1행만이 독립적인 값을 가지고 있다**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "x (4,2), x.T (2,4)의 dot()을 구하면 (4,4) 행렬이 된다.\n",
    "**순서를 바꾸면 x.T (2,4) x (4,2) -> (2,2)**가 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xTx=', array([[ 30.,  10.],\n",
      "       [ 10.,   4.]]))\n"
     ]
    }
   ],
   "source": [
    "xTx=np.dot(x.T,x)\n",
    "print(\"xTx=\",xTx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) $(X^T X) ^{-1}$ 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xTxI=', array([[ 0.2, -0.5],\n",
      "       [-0.5,  1.5]]))\n"
     ]
    }
   ],
   "source": [
    "xTxI=np.linalg.inv(xTx)\n",
    "print(\"xTxI=\",xTxI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) $X^T y$ 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('xTyT=', array([ 77.,  28.]))\n"
     ]
    }
   ],
   "source": [
    "xTyT=np.dot(x.T,y)\n",
    "print(\"xTyT=\",xTyT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지금까지의 계산을 넣어서 weights를 구할 수 있게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 전체 $(X^T X) ^{-1} X^T y$ 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weights=', array([ 1.4,  3.5]))\n"
     ]
    }
   ],
   "source": [
    "print(\"weights=\",np.dot(xTxI,xTyT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 한 줄로 계산\n",
    "\n",
    "좀 복잡하지만, 한 줄로 줄여서 연산해도 된다.\n",
    "x0이 뒤에 위치해 있으므로, w1, w0으로 저장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w1, w0=np.dot(np.linalg.inv(np.dot(x.T, x)), np.dot(x.T, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.499999999999993, 1.4000000000000021)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w0, w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### numpy 최소자승법\n",
    "\n",
    "데이터는 위 연습에서 사용했던 데이터를 그대로 사용하자.\n",
    "앞서 가역행렬의 문제로 풀 수 없던 문제를 numpy 최소자승법으로 풀어보자.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "```numpy.linalg.lstsq()```에는 컬럼벡터 x를 적어준다.\n",
    "그 결과는 앞서 계산값과 일치한다.\n",
    "\n",
    "함수의 입력을 적어준다.\n",
    "* 독립변수 ```x```\n",
    "* 종속변수 ```y```\n",
    "\n",
    "그 반환 값은 가중치 weights이다. 독립변수 각 각에 대한 가중치이다.\n",
    "$y=w_1\\times x + w_0$ 순서대로 출력한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('numpy lstsq: ', (array([ 1.40,  3.50]), array([ 4.20]), 2, array([ 5.78,  0.77])))\n"
     ]
    }
   ],
   "source": [
    "np.warnings.filterwarnings('ignore')   #suppress numpy warnings\n",
    "print(\"numpy lstsq: \",np.linalg.lstsq(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "w0,w1는 위에서 풀었던 해와 동일하게 계산된다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('w1=', 1.4000000000000006, '\\nw0=', 3.4999999999999978)\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "w1,w0=np.linalg.lstsq(x,y)[0]\n",
    "print(\"w1=\",w1,\"\\nw0=\",w0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정리하면\n",
    "\n",
    "회귀선을 numpy 최소자승법으로 구하고, 실제 데이터와 같이 그래프를 그려보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff9f015a390>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# compute yhat from the model values of b0 and b1\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])\n",
    "_x=np.array([x,np.ones(len(x))])\n",
    "_x=_x.T # shape (4,2)\n",
    "#w1,w0=np.linalg.lstsq(_x,y)[0]\n",
    "w1, w0=np.dot(np.linalg.inv(np.dot(_x.T, _x)), np.dot(_x.T, y)) #1.4 3.5\n",
    "yhat=w0+w1*x\n",
    "fig=plt.figure()\n",
    "ax=fig.add_subplot(111)\n",
    "ax.scatter(x,y)\n",
    "ax.plot(x,yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 문제: Matrix Inverse Method 회귀모델\n",
    "\n",
    "데이터가 다음과 같이 주어졌을 때:\n",
    "* x [1,2],[1,4],[1,6]\n",
    "* y 3,6,7\n",
    "\n",
    "회귀식을 도출해보자.\n",
    "* 1) matrix inverse method로 $\\hat{w}$을 계산,\n",
    "* 2) 예측과 실제의 오류 합계를 계산,\n",
    "* 2) 입력 데이터와 회귀선의 그래프를 작성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## S.6 Gradient 알고리즘\n",
    "\n",
    "\n",
    "### gradient란?\n",
    "기울기 하강법 Gradient decent algorithm은 **오류를 점차 줄여가는 기울기를 선택하가면서 최적해**를 찾는다.\n",
    "\n",
    "gradient는 경사도를 말한다.\n",
    "처음에는 무작위 값에서 출발하여, 오류를 줄여가는 방향으로 경사도를 줄여가며 최적해를 찾아가는 방법이다.\n",
    "경사방법 Gradient algorithm은 **greedy 탐욕알고리즘**으로, 경사도를 선택하여 답을 찾아가지만, 그 답이 최적이라는 보장은 없다.\n",
    "오류함수가 2차함수인 경우 **local optimum**이 곧 global optimum이 된다.\n",
    "탐욕적으로 계속 최적해를 구해나가야 한다.\n",
    "\n",
    "OLS방법을 보편적으로 사용한다. 미분해서 풀 수 있지만, 변인의 갯수만큼 방정식을 풀어야 한다.\n",
    "극대점을 찾기 위해서는 gradient 방향으로 오르는 것을 gradient ascent, 반대는 gradient descent(ascent)로 반복을 하면서 계수를 구한다.\n",
    "(matrix 연산은 $n^{2.373}$이 소요된다는 연구)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### gradient ascent algorithms\n",
    "\n",
    "최적해는 h(x)와 y가 최소화하도록, 가장 근사하게 예측할 수 있는 weights $\\theta$를 추정한다.\n",
    "* hypothesis\n",
    "    * $h_{\\theta}(x)=\\sum_{i=0}^n(\\theta^Tx)$\n",
    "\n",
    "    * $h_{\\theta}(x) = \\theta_0 + \\theta_1x_1 + \\ldots + \\theta_nx_n$ ($x_0=1$ 일 경우)\n",
    "\n",
    "* cost function 오류는 가장 많이 쓰이는 **MSE Mean Squared Error**로 다음과 같이 나타낼수 있다.\n",
    "예측과 실제의 차이를 서로 상쇄하지 않도록 제곱을 하고, 갯수로 나누어 평균\n",
    "??을 구하는 식이다.\n",
    "    * $J(\\theta) = \\frac{1}{2} \\sum_{i=1}^m (h_{\\theta}(x^{(i)}) - y^{(i)})^2$\n",
    "\n",
    "참조: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### gradient 계산\n",
    "\n",
    "gradient는 그 점에서의 **기울기**로서 방향, 크기를 나타낸다.\n",
    "벡터의 gradient는 '각 변수에 대한 f의 편미분 벡터'이다.\n",
    "\n",
    "$\\nabla f(x,y)= \\frac{\\partial{f}}{\\partial{x}},\n",
    "                  \\frac{\\partial{f}}{\\partial{y}}$\n",
    "\n",
    "$x_{n+1}=x_n-\\gamma_n \\nabla F(x_n),\\ n \\ge 0$이면\n",
    "$F(x_0)\\ge F(x_1)\\ge F(x_2)\\ge \\cdots$이므로 결국 **최소값 local minimum**에 도달하게 된다.\n",
    "\n",
    "예를 들어, $f(x,y) = x^2 + y^2$의 gradient를 구하면\n",
    "\n",
    "$\n",
    "\\nabla f= \\frac{\\partial{f}}{\\partial{x}},\\\n",
    "\\frac{\\partial{f}}{\\partial{y}} = (2x,2y)\n",
    "$\n",
    "\n",
    "따라서 (1,1)에서 f값이 최대로 증가하는 방향은 (2,2)\n",
    "그 기울기 (오류)는 $||(2,2)|| = \\sqrt{2^2 + 2^2} = \\sqrt{8}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "* 앞서 정의한\n",
    "    * Hypothesis: $h_\\theta(x) = \\theta^T x$\n",
    "    * Loss: $(h_\\theta(x)-y)$\n",
    "* 위 cost function에 대해 gradient를 구하면\n",
    "\n",
    "$\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial{\\theta_j}}J(\\theta)\n",
    "    &=\\frac{\\partial}{\\partial{\\theta_j}} \\frac{1}{2} (h_{\\theta}(x) - y)^2\\\\\n",
    "    &=2 \\cdot \\frac{1}{2} (h_{\\theta}(x) - y) \\cdot \\frac{\\partial}{\\partial{\\theta_j}} (h_{\\theta}(x) - y)\\\\\n",
    "    &=(h_{\\theta}(x) - y) \\cdot \\frac{\\partial}{\\partial{\\theta_j}} (\\sum_{i=0}^n \\theta_i x_i - y)\\\\\n",
    "    &=(h_{\\theta}(x) - y) x_j\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "위 식을 보면, **gradient는 오류에 x를 dot연산**해서 얻어진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "오류에 상수항을 넣어서 계산해도 마찬가지이다.\n",
    "\n",
    "* $J(\\theta)$ $Error=\\frac{1}{n} \\sum_{i=1}^n(y_i - (ax_i+b))^2$\n",
    "\n",
    "오류를 편미분해서 기울기 gradient를 a,b에 대해 구하면 다음과 같다. **$x_0=1$이면 당연히 위 식 하나로 a만 구하면 된다**.\n",
    "* $\\frac{\\partial}{\\partial{a}}\n",
    "    =\\frac{2}{n} \\sum_{i=1}^n(y_i - (ax_i+b)) (-x_i)$\n",
    "* $\\frac{\\partial}{\\partial{b}}\n",
    "    =\\frac{2}{n} \\sum_{i=1}^n(y_i - (ax_i+b)) (-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 갱신\n",
    "\n",
    "$\\alpha$는 학습비율 Learning Rate이며, 아래 값이 처음에는 큰 값으로 조정하다가, 반복이 계속될수록 적어지면서 0에 가까워질 때까지 현재 $\\theta$를 갱신해 나간다.\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha(h_\\theta(x)-y)x_{j})$\n",
    "\n",
    "$\\theta_j := \\theta_j - \\alpha (y^{(i)}-h(x^{(i)}))x_j^{(i)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### batch, stochastic\n",
    "\n",
    "* **batch gradient**, 학습데이터를 모두 일괄적으로 합계내어, 모델을 갱신함 (deterministic)\n",
    "    * 반복:\n",
    "        * $\\theta$ $\\forall i$\n",
    "* **stochastic gradient descent** (incremental gradient descent) 데이터를 하나씩 사용하여, 모델을 갱신함. 따라서 결과가 batch와 다를 수 있슴.\n",
    "    * 반복\n",
    "        * for i in range(m):\n",
    "            * $\\theta$ 갱신 $\\forall i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 의사코드\n",
    "\n",
    "* 모든 계수 $\\theta = 1$ (또는 무작위)\n",
    "* 반복\n",
    "    * $\\theta$ 갱신\n",
    "        * $\\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial{\\theta_j}}J(\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "실제와 예측의 차이인 **오류 $(y-yhat)^2$를 최소화**하는 베타 값을 구해보자.\n",
    "\n",
    "$(6 - (\\beta_0 + \\beta_1 \\times 1))^2$\n",
    "\n",
    "$(5 - (\\beta_0 + \\beta_1 \\times 2))^2$\n",
    "\n",
    "$(7 - (\\beta_0 + \\beta_1  \\times 3))^2$\n",
    "\n",
    "$(10 - (\\beta_0 + \\beta_1  \\times 4))^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위를 $\\beta_0$에 대해 편미분하면:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_0} = $\n",
    "\n",
    "$2 \\times (6 - \\beta_0 - \\beta_1 \\times 1) \\times (-1)$\n",
    "\n",
    "$2 \\times (5 - \\beta_0 - \\beta_1 \\times 2) \\times (-1)$\n",
    "\n",
    "$2 \\times (7 - \\beta_0 - \\beta_1 \\times 3) \\times (-1)$\n",
    "\n",
    "$2 \\times (10 - \\beta_0 - \\beta_1 \\times 4) \\times (-1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 식을 풀어서 더하면\n",
    "\n",
    "$ -12 + 2\\beta_0 + 2\\beta_1$\n",
    "\n",
    "$- 10 + 2\\beta_0 + 4\\beta_1$\n",
    "\n",
    "$ -14 + 2\\beta_0 + 6\\beta_1$\n",
    "\n",
    "$ -20 + 2\\beta_0 + 8\\beta_1$\n",
    "\n",
    "위 식을 모두 더해서 총오류를 구하면:\n",
    "\n",
    "$8\\beta_0 + 20\\beta_1 - 56$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위를 $\\beta_1$에 대해 편미분하면:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_1} = $\n",
    "\n",
    "$2 \\times (6 - \\beta_0 - \\beta_1 \\times 1) \\times (-1)$\n",
    "\n",
    "$2 \\times (5 - \\beta_0 - \\beta_1 \\times 2) \\times (-2)$\n",
    "\n",
    "$2 \\times (7 - \\beta_0 - \\beta_1 \\times 3) \\times (-3)$\n",
    "\n",
    "$2 \\times (10 - \\beta_0 - \\beta_1 \\times 4) \\times (-4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위 식을 풀어서 더하면\n",
    "\n",
    "$ -12 + 2\\beta_0 + 2\\beta_1$\n",
    "\n",
    "$ -20 + 4\\beta_0 + 8\\beta_1$\n",
    "\n",
    "$ -42 + 6\\beta_0 + 18\\beta_1$\n",
    "\n",
    "$ -80 + 8\\beta_0 + 32\\beta_1$\n",
    "\n",
    "위 식을 모두 더해서 총오류를 구하면:\n",
    "\n",
    "$20\\beta_0 + 60\\beta_1 - 154$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "위에서 구한 2개의 식은:\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_0} = 8\\beta_0 + 20\\beta_1 - 56$\n",
    "\n",
    "$\\frac{\\partial f}{\\partial \\beta_1} = 20\\beta_0 + 60\\beta_1 - 154$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8*b0 + 20*b1 - 56"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])\n",
    "b0,b1=sp.symbols('b0 b1')\n",
    "np.sum(-2 * (y-(b0+x.dot(b1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$\\beta_0 = 1, \\beta_1 = 1$ 이면\n",
    "각각 -28, -74이다.\n",
    "\n",
    "이 값을 대입해서, 새로운 bo, b1을 구하면:\n",
    "\n",
    "* new b0 = 1 - 0.01 * (-28) = 1.28\n",
    "* new b1 = 1 - 0.01 * (-74) = 1.74"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])\n",
    "#x=np.array([0.5,2.3,2.9])\n",
    "#y=np.array([1.4,1.9,3.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 2.  1.]\n",
      " [ 3.  1.]\n",
      " [ 4.  1.]] (4, 2)\n"
     ]
    }
   ],
   "source": [
    "x=np.array([x,np.ones(len(x))])  # w0 * x0 + w1 * x1\n",
    "x=x.T\n",
    "print x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## theta\n",
    "\n",
    "우리가 구하려는 theta는 처음에 1로 설정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.]\n"
     ]
    }
   ],
   "source": [
    "theta=np.array(np.ones([x.shape[1]]))\n",
    "print theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## h(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  1.]\n",
      " [ 2.  1.]\n",
      " [ 3.  1.]\n",
      " [ 4.  1.]]\n",
      "[ 2.  3.  4.  5.]\n"
     ]
    }
   ],
   "source": [
    "print x*theta\n",
    "print np.dot(x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "함수로 만들어보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  3.,  4.,  5.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def h(x,theta):\n",
    "    return np.dot(x,theta)\n",
    "\n",
    "h(x,theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4. -2. -3. -5.]\n"
     ]
    }
   ],
   "source": [
    "error=h(x,theta)-y\n",
    "print error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### gradient\n",
    "\n",
    "```python\n",
    "np.sum(y-h)*(-2)/len(x)의 결과는 -7\n",
    "np.sum((y-h)*x)*(-2)/len(x) 결과는 -18.5\n",
    "```\n",
    "\n",
    "$x_0$은 1이므로 위는 ```np.dot(x.T,error)```로 바꿔쓸 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "아래 결과는 x 길이로 나누어주어서 그렇다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-7.0\n"
     ]
    }
   ],
   "source": [
    "print np.sum(y-h(x,theta))*(-2)/len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-37. -14.]\n"
     ]
    }
   ],
   "source": [
    "gradient=np.dot(x.T,error)\n",
    "print gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### update\n",
    "\n",
    "기울기에 학습률을 곱하여 빼주면 theta를 갱신하게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.37  1.14]\n"
     ]
    }
   ],
   "source": [
    "alpha=0.01\n",
    "theta -= alpha*gradient\n",
    "print theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 | theta [ 1.37  1.14] Cost 6.75000\n",
      "Iteration 100 | theta [ 1.79291203  2.34479071] Cost 0.63779\n",
      "Iteration 200 | theta [ 1.61550957  2.86637557] Cost 0.55893\n",
      "Iteration 300 | theta [ 1.51820553  3.15246131] Cost 0.53521\n",
      "Iteration 400 | theta [ 1.46483493  3.3093774 ] Cost 0.52807\n",
      "Iteration 500 | theta [ 1.43556152  3.3954448 ] Cost 0.52592\n",
      "Iteration 600 | theta [ 1.41950525  3.44265218] Cost 0.52528\n",
      "Iteration 700 | theta [ 1.4106985   3.46854511] Cost 0.52508\n",
      "Iteration 800 | theta [ 1.40586805  3.48274721] Cost 0.52503\n",
      "Iteration 900 | theta [ 1.40321859  3.49053696] Cost 0.52501\n",
      "Iteration 1000 | theta [ 1.40176537  3.49480959] Cost 0.52500\n",
      "Iteration 1100 | theta [ 1.4009683   3.49715309] Cost 0.52500\n",
      "Iteration 1200 | theta [ 1.4005311   3.49843849] Cost 0.52500\n",
      "Iteration 1300 | theta [ 1.40029131  3.49914352] Cost 0.52500\n",
      "Iteration 1400 | theta [ 1.40015978  3.49953023] Cost 0.52500\n",
      "Iteration 1500 | theta [ 1.40008764  3.49974233] Cost 0.52500\n",
      "Iteration 1600 | theta [ 1.40004807  3.49985867] Cost 0.52500\n",
      "Iteration 1700 | theta [ 1.40002637  3.49992248] Cost 0.52500\n",
      "Iteration 1800 | theta [ 1.40001446  3.49995748] Cost 0.52500\n",
      "Iteration 1900 | theta [ 1.40000793  3.49997668] Cost 0.52500\n",
      "Iteration 2000 | theta [ 1.40000435  3.49998721] Cost 0.52500\n",
      "Iteration 2100 | theta [ 1.40000239  3.49999298] Cost 0.52500\n",
      "Iteration 2200 | theta [ 1.40000131  3.49999615] Cost 0.52500\n",
      "Iteration 2300 | theta [ 1.40000072  3.49999789] Cost 0.52500\n",
      "Iteration 2400 | theta [ 1.40000039  3.49999884] Cost 0.52500\n",
      "Iteration 2500 | theta [ 1.40000022  3.49999937] Cost 0.52500\n",
      "Iteration 2600 | theta [ 1.40000012  3.49999965] Cost 0.52500\n",
      "Iteration 2700 | theta [ 1.40000006  3.49999981] Cost 0.52500\n",
      "Iteration 2800 | theta [ 1.40000004  3.4999999 ] Cost 0.52500\n",
      "Iteration 2900 | theta [ 1.40000002  3.49999994] Cost 0.52500\n",
      "Iteration 3000 | theta [ 1.40000001  3.49999997] Cost 0.52500\n",
      "Iteration 3100 | theta [ 1.40000001  3.49999998] Cost 0.52500\n",
      "Iteration 3200 | theta [ 1.4         3.49999999] Cost 0.52500\n",
      "Iteration 3300 | theta [ 1.4         3.49999999] Cost 0.52500\n",
      "Iteration 3400 | theta [ 1.4  3.5] Cost 0.52500\n",
      "Iteration 3500 | theta [ 1.4  3.5] Cost 0.52500\n",
      "Iteration 3600 | theta [ 1.4  3.5] Cost 0.52500\n",
      "Iteration 3700 | theta [ 1.4  3.5] Cost 0.52500\n",
      "Iteration 3800 | theta [ 1.4  3.5] Cost 0.52500\n",
      "Iteration 3900 | theta [ 1.4  3.5] Cost 0.52500\n"
     ]
    }
   ],
   "source": [
    "#def gradientDescent(x, y, theta, alpha, m, numIterations):\n",
    "alpha=0.01\n",
    "numIterations=4000\n",
    "theta=np.ones([x.shape[1]])\n",
    "#theta=np.array([1,1])\n",
    "for i in range(numIterations):\n",
    "    h=np.dot(x,theta)\n",
    "    error=h-y\n",
    "    #gradient=[np.sum(y-h)*(-2), np.sum((y-h)*x)*(-2)]\n",
    "    cost = np.sum((h-y)** 2) / (2 * len(x))\n",
    "    gradient=np.dot(x.T,error) # w0, w1\n",
    "    theta -= alpha*gradient\n",
    "    if i%100 == 0:\n",
    "        print \"Iteration {0} | theta {1} Cost {2:.5f}\".format(i, theta, cost)\n",
    "#    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def computeAvgError(a,b,x,y):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(x)):\n",
    "        totalError += (y[i] - (a + b * x[i])) ** 2\n",
    "    return totalError / float(len(x))\n",
    "\n",
    "a=1\n",
    "b=1\n",
    "alpha=0.01\n",
    "n=len(x)\n",
    "iter=1500\n",
    "for j in range(iter):\n",
    "    aGradient = 0\n",
    "    bGradient = 0\n",
    "    for i in range(n):\n",
    "        #aGradient += (2./n) * (y[i] - ((a * x[i]) + b))*(-1)\n",
    "        #bGradient += (2./n) * (y[i] - ((a * x[i]) + b))*(-x[i])\n",
    "        aGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-1)\n",
    "        bGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-x[i])\n",
    "    a = a - (alpha * aGradient)\n",
    "    b = b - (alpha * bGradient)\n",
    "    if (j%100==0):\n",
    "        print \"iter:{0} a={1:.3f} b={2:.3f} AvgError={3:.3f}\".format(j,a,b,computeAvgError(a,b,x,y))\n",
    "\n",
    "#return a, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "리스트를 사용하면 벡터와 달리 반복문을 사용하는 것이 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def computeAvgError(a,b,x,y):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(x)):\n",
    "        totalError += (y[i] - (a + b* x[i])) ** 2\n",
    "    return totalError / float(len(x))\n",
    "\n",
    "#x: attribute, 1d float array\n",
    "#y: class, 1d int array\n",
    "#alpha: learning rate\n",
    "def GradientDescent(x,y,alpha,iter):\n",
    "    a=random.random()\n",
    "    b=random.random()\n",
    "    alpha=0.01\n",
    "    n=len(x)\n",
    "    for j in range(iter):\n",
    "        aGradient = 0\n",
    "        bGradient = 0\n",
    "        for i in range(n):\n",
    "            #aGradient += (2./n) * (y[i] - ((a * x[i]) + b))*(-1)\n",
    "            #bGradient += (2./n) * (y[i] - ((a * x[i]) + b))*(-x[i])\n",
    "            aGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-1)\n",
    "            bGradient += (2./n) * (y[i] - ((a + b * x[i])))*(-x[i])\n",
    "        a = a - (alpha * aGradient)\n",
    "        b = b - (alpha * bGradient)\n",
    "        if (j%100==0):\n",
    "            print \"iter:{0} a={1:.3f} b={2:.3f} AvgError={3:.3f}\".format(j,a,b,computeAvgError(a,b,x,y))\n",
    "    return a, b\n",
    "\n",
    "x=np.array([1,2,3,4])\n",
    "y=np.array([6,5,7,10])\n",
    "a,b=GradientDescent(x,y,alpha,10000)\n",
    "print \"---> a={0}, b={1} after iterations\".format(a,b)\n",
    "\n",
    "yhat=a + b*x\n",
    "print yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
